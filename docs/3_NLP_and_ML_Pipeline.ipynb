{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "plt.rcParams['font.size'] = 17\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(f'../app/data/processed_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c7eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2618bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c14552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "    sentences = data.split('.')\n",
    "    clean_sentences = []\n",
    "    for i in sentences:\n",
    "        clean_sentence = re.sub(r'[?|!|\\'|\"|#|/|-|,(|)|$|-|' '|:]',r'',i)\n",
    "        clean_sentence = re.sub(r'[?|!|\\'|\"|#|/|-|,(|)|$|(0-9)]',r'',clean_sentence.strip(' '))\n",
    "        if len(clean_sentence.strip()) > 1:\n",
    "            clean_sentences.append(clean_sentence)\n",
    "    done_sent = ''\n",
    "    for i in clean_sentences:\n",
    "        done_sent += (' '+i)\n",
    "    return done_sent.strip()\n",
    "    \n",
    "\n",
    "data['cleaned'] = data.text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043cdcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.cleaned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd3bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "    sentences = data.split('.')\n",
    "    clean_sentences = []\n",
    "    for i in sentences:\n",
    "        clean_sentence = re.sub(r'[?|!|\\'|\"|#|/|-|,(|)|$|-|' '|:]',r'',i)\n",
    "        clean_sentence = re.sub(r'[?|!|\\'|\"|#|/|-|,(|)|$|-|' '|:]',r'',clean_sentence)\n",
    "        clean_sentence = \" \".join(re.findall(\"[(a-zA-Z,&)]+\", clean_sentence))\n",
    "        clean_sentences.append(clean_sentence)\n",
    "    clean_text = ''\n",
    "    for i in clean_sentences:\n",
    "        clean_text += (' '+i)\n",
    "    return clean_text.strip(' ')\n",
    "\n",
    "    \n",
    "\n",
    "data['cleaned'] = data.text.apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96973395",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.cleaned[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd300ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "679e87e7",
   "metadata": {},
   "source": [
    "### 4.1 Target\n",
    "Before I can begin splitting the data I need to set the target for my methodology of training four seperate logistic regression models. I'm doing this because I'd like my classifications to be as accurate as possible, and also, by building my NLP strategy around a particular label, i.e. finding common words for that label as opposed to being generalized through the entire corpus\\\n",
    "\n",
    "I'm going to one-hot-encode the target feature so I can select each of the next columns as my y - one for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['company','job_title','state','city','rating','cleaned','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f30a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, dtype='int')\n",
    "targets = ohe.fit_transform(pd.DataFrame(data.target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.DataFrame(targets,columns=['Q1','Q2','Q3','Q4','unk'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea516b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.join(targets)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['target','unk'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd6058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "\n",
    "data['comment_text'] = data['cleaned'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404132b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.comment_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7348a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def stemming(sentence):\n",
    "    LemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = lemmatizer.lemmatize(word)\n",
    "        LemSentence += stem\n",
    "        LemSentence += \" \"\n",
    "    LemSentence = LemSentence.strip()\n",
    "    return LemSentence\n",
    "\n",
    "\n",
    "data['comment_text_lem'] = data['cleaned'].apply(stemming)\n",
    "data.comment_text_lem[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960258da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['cleaned','comment_text'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb0310",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6235ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292b00b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91b775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_data = data.drop(['Q2','Q3','Q4'], axis=1)\n",
    "q2_data = data.drop(['Q1','Q3','Q4'], axis=1)\n",
    "q3_data = data.drop(['Q1','Q2','Q4'], axis=1)\n",
    "q4_data = data.drop(['Q1','Q2','Q3'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c96ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b2661",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_cols = data.select_dtypes('object').columns\n",
    "scal_cols = ['rating']\n",
    "\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "     transformers=[\n",
    "         ('text', TfidfVectorizer(ngram_range=(1,3), analyzer = 'word',max_features=1000,stop_words='english',decode_error='ignore'), 'comment_text_lem'),\n",
    "         ('category', OneHotEncoder(handle_unknown ='ignore'), le_cols),\n",
    "         ('scaler', StandardScaler(), scal_cols)\n",
    "         \n",
    "         ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipe = Pipeline(\n",
    "     steps=[\n",
    "         ('preprocessor', preprocessor),\n",
    "         ('classifier', LogisticRegression()),\n",
    "     ],\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7f4016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_pipe():\n",
    "    targets = ['Q1','Q2','Q3','Q4']\n",
    "    X = data.drop(targets, axis=1)\n",
    "\n",
    "    evaluations = {}\n",
    "    for i in targets:\n",
    "        y = data[i]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y , test_size=.2, random_state=42)\n",
    "\n",
    "        grid = [\n",
    "        {\n",
    "        'classifier' : [LogisticRegression()],\n",
    "        'classifier__penalty' : ['l1', 'l2'],\n",
    "        #'classifier__C' : np.logspace(-4, 4, 20),\n",
    "        'classifier__solver' : ['liblinear']}\n",
    "]\n",
    "\n",
    "        \n",
    "        grid_search = GridSearchCV(pipe, param_grid=grid, verbose=2, return_train_score=True,n_jobs=-1)\n",
    "        grid_search.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "        if i not in evaluations.keys():\n",
    "            evaluations[i] = {\n",
    "                'grid_search':grid_search,\n",
    "                'x_train':x_train,\n",
    "                'y_train':y_train,\n",
    "                'x_test':x_test,\n",
    "                'y_train':y_train\n",
    "                \n",
    "            }\n",
    "\n",
    "\n",
    "    return evaluations\n",
    "\n",
    "\n",
    "evaluations = multi_pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed88c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf870f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations['Q1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0504aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(evaluations['Q3']['y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf2b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = q1_data.drop(['Q1'], axis=1)\n",
    "y = q1_data['Q1']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y , test_size=.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b0d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_cols = x_train.select_dtypes('object').columns\n",
    "scal_cols = ['rating']\n",
    "\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "     transformers=[\n",
    "         ('text', TfidfVectorizer(ngram_range=(1,3), analyzer = 'word',max_features=1000,stop_words='english',decode_error='ignore'), 'comment_text_lem'),\n",
    "         ('category', OneHotEncoder(handle_unknown ='ignore'), le_cols),\n",
    "         ('scaler', StandardScaler(), scal_cols)\n",
    "         \n",
    "         ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipe = Pipeline(\n",
    "     steps=[\n",
    "         ('preprocessor', preprocessor),\n",
    "         ('classifier', LogisticRegression()),\n",
    "     ],\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [\n",
    "    {'classifier' : [LogisticRegression()],\n",
    "     'classifier__penalty' : ['l1', 'l2'],\n",
    "    'classifier__C' : np.logspace(-4, 4, 20),\n",
    "    'classifier__solver' : ['liblinear']}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90192a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=grid, verbose=2, return_train_score=True,n_jobs=-1)\n",
    "grid_search.fit(x_train,y_train)\n",
    "\n",
    "#1.26.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0ae859",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67e9dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fad4e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd1f6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e56e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameter (CV score=%0.3f):\" % grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f9ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d80af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, hamming_loss,precision_score,recall_score,f1_score,classification_report\n",
    "predictions = pipe.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda20ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy :\",accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a401de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Hamming loss \",hamming_loss(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84cd949",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc63768",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMicro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd7102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d9da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMacro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6265cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClassification Report\")\n",
    "print (classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb52f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a4119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, log_loss, roc_auc_score, roc_curve\n",
    "from confusion import make_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a65660",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf1 = confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2abc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = ['Q1']\n",
    "te_confusions = [cf1]\n",
    "y_t = [y_test]\n",
    "te_probs = [pipe.predict_proba(x_test)]\n",
    "y_pred = [predictions]\n",
    "y_test_preds = predictions\n",
    "y_prob = [pipe.predict_proba(x_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d6f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = pipe.predict_proba(x_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([-.01, 1.])\n",
    "plt.ylim([-.01, 1.05])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, cf, y_t, y_pred, y_prob in zip(Q, te_confusions, y_test, y_test_preds, te_probs):\n",
    "    make_confusion_matrix(cf, title='\\n'+title+' Confusion Matrix\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320418ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b92b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y = pipe.predict(x_test)\n",
    "data_dict = {\"y\":y_test, \n",
    "             \"pred y\": predict_y.tolist(),\n",
    "             \"probabilities\": y_prob.tolist()}\n",
    "pd.DataFrame(data_dict).tail(10)\n",
    "pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c11906",
   "metadata": {},
   "outputs": [],
   "source": [
    "zulu = x_test.join(pd.DataFrame(data_dict))\n",
    "zulu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cc15c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60fc1f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30765298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_tfidf = tfidf.fit_transform(_test).toarray()\n",
    "vocab = q1_vectorizer.vocabulary_\n",
    "reverse_vocab = {v:k for k,v in vocab.items()}\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "df_tfidf = pd.DataFrame(X_tfidf, columns = feature_names)\n",
    "\n",
    "idx = X_tfidf.argsort(axis=1)\n",
    "\n",
    "tfidf_max10 = idx[:,-10:]\n",
    "\n",
    "df_tfidf['top10'] = [[reverse_vocab.get(item) for item in row] for row in tfidf_max10 ]\n",
    "\n",
    "df_tfidf['top10']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f796fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac83f05b37eb5c5c49ba67e50f0047ddfbc4b30205fc79ee5f327b9c0ac37f55"
  },
  "kernelspec": {
   "display_name": "Python [conda env:indeedapp]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
