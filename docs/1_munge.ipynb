{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 17\n",
    "\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"ticks\", color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA\n",
    "# Import/Load\n",
    "data = pd.read_csv('../app/data/total.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The following table is a view of the raw data from the scrape. I'll build the target by reducing the pay column to floating point values; this will require text cleaning which will also be conducted for the rest of the table. Location will be split into new columns for City and State while and because Indeed's search results show dates relative to the date the query was made I'll need to use PostDate and ExtractDate build a DatePosted column. It is important to note that JobUrl and date-related columns will not be converted into features for the logistic regression, instead they will be reattached to the data at the end in order to provide additional information and insights through the EC2 application. The only rows that'l  be using for modelling are JobTitle, Company, Summary, Requirements, and Description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Examine\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.location == 'Remote'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values?\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.isnull().any(axis=1)]\n",
    "# every column has nulls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Value Counts\n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.company.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting = ['company','estimated_salary','job_type_items','location','rating','requirements','salary','sal_guide_items','salary_and_jType']\n",
    "for i in interesting:\n",
    "    print(data[i].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i in np.unique(data[data['company'].notnull()].company):\n",
    "    samples.append(i)\n",
    "print(len(samples))\n",
    "samples[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = sum(data.salary.isnull())\n",
    "nnull = sum(data.salary.notnull())\n",
    "print(data.isnull().sum())\n",
    "print('')\n",
    "print(f'The data contains {data.shape[0]} rows of individual job postings with values for {data.shape[1]} columns based on\\n different sections of each post\\'s web page.')\n",
    "print('')\n",
    "print(f'- Out of {len(data)} job postings {nnull} or {round(nnull/len(data)*100,2)}% include some sort of salary information,\\n- The remaining {null} rowws, or {round(null/len(data)*100,2)}% are missing salary data.\\n- Also, there are no null values in the rest of the table meaning we won\\'t need\\n  to lose any data by dropping rows. While the null values in the Pay column will\\n  simply be dropped after we translate the not nulls into the target.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Here we can observe the sort of values held in the Pay column. Aside from removing special characters and spaces I\\'ll extract\\n the remaing non-numeric chars into a new column that\\'ll allow me to build the annual Salary column once the remaining\\n numeric string are converted to floating point.')\n",
    "\n",
    "samples = []\n",
    "for i in np.unique(data[data['sal_guide_items'].notnull()].sal_guide_items):\n",
    "    samples.append(i)\n",
    "samples[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess Data\n",
    "data = pd.read_csv('../app/data/total.csv')\n",
    "#  drop_dupes\n",
    "data = data.drop_duplicates()\n",
    "data.columns = map(str.lower, data.columns)\n",
    "data = data.rename({'title': 'job_title'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sal_chars(data):\n",
    "    cleaned = re.sub(r'[\\n|,|+|$|[|$|\\'|\"]',r'',data)\n",
    "    #cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',data)\n",
    "    #data[\"Pay\"] = data[\"Pay\"].str.replace(\"\\n\", \"\")\n",
    "    #data[\"Pay\"] = data[\"Pay\"].str.replace(\",\", \"\")\n",
    "    #data[\"Pay\"] = data[\"Pay\"].str.replace(\"+\", \" \")\n",
    "    #data[\"Pay\"] = data[\"Pay\"].str.replace(\"$\", \"\", regex=False)\n",
    "    return cleaned\n",
    "\n",
    "def sal_splitter(data):\n",
    "    x = data.split(',')\n",
    "    return x[2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary = data[data.salary.notnull()]\n",
    "data[['salary',]] = salary['salary'].apply(sal_chars)\n",
    "\n",
    "estimated_salary = data[data.estimated_salary.notnull()]\n",
    "data['estimated_salary'] = estimated_salary['estimated_salary'].apply(sal_chars)\n",
    "\n",
    "salfromsection = data[data.salfromsection.notnull()]\n",
    "data[['salfromsection',]] = salfromsection['salfromsection'].apply(sal_chars)\n",
    "\n",
    "salary_and_jtype = data[data.salfromsection.notnull()]\n",
    "data[['salary_and_jtype',]] = salary_and_jtype['salary_and_jtype'].apply(sal_chars)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sal_guide_arr = data[data.sal_guide_items.notnull()]\n",
    "data[['sal_guide_items']] = sal_guide_arr['sal_guide_items'].apply(sal_splitter)\n",
    "\n",
    "sal_guide_arr = data[data.sal_guide_items.notnull()]\n",
    "data[['sal_guide_items']] = sal_guide_arr['sal_guide_items'].apply(sal_chars)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pay_period(data):\n",
    "    z = ''\n",
    "    substrings = ['hour','day','week','year']\n",
    "    for i in substrings:\n",
    "        if i in data:\n",
    "            return i\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "estimated_salary = data[data['estimated_salary'].notnull()]\n",
    "data['Schedule1'] = estimated_salary['estimated_salary'].apply(Pay_period)\n",
    "\n",
    "sal_guide_items = data[data['sal_guide_items'].notnull()]\n",
    "data['Schedule2'] = sal_guide_items['sal_guide_items'].apply(Pay_period)\n",
    "\n",
    "salary = data[data['salary'].notnull()]\n",
    "data['Schedule3'] = salary['salary'].apply(Pay_period)\n",
    "\n",
    "salary_and_jtype = data[data['salary_and_jtype'].notnull()]\n",
    "data['Schedule4'] = salary_and_jtype['salary_and_jtype'].apply(Pay_period)\n",
    "\n",
    "salfromsection = data[data['salfromsection'].notnull()]\n",
    "data['Schedule5'] = salfromsection['salfromsection'].apply(Pay_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cols = ['Schedule1','Schedule2','Schedule3','Schedule4','Schedule5']\n",
    "data['schedule'] = data[val_cols].bfill(axis=1).iloc[:, 0]\n",
    "data.drop(val_cols,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['year', 'hour', nan], dtype=object)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.schedule.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>description</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>extractdate</th>\n",
       "      <th>job_type_items</th>\n",
       "      <th>location</th>\n",
       "      <th>postdate</th>\n",
       "      <th>rating</th>\n",
       "      <th>raw_desc_soup</th>\n",
       "      <th>requirements</th>\n",
       "      <th>sal_guide_items</th>\n",
       "      <th>salary</th>\n",
       "      <th>salary_and_jtype</th>\n",
       "      <th>salfromsection</th>\n",
       "      <th>summary</th>\n",
       "      <th>job_title</th>\n",
       "      <th>url</th>\n",
       "      <th>schedule</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Online Technical Services</td>\n",
       "      <td>Job descriptionData Scientist, MarketingSAN DI...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-04-13</td>\n",
       "      <td>['Full-time']</td>\n",
       "      <td>Remote</td>\n",
       "      <td>PostedJust posted</td>\n",
       "      <td>3.7</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>Master's (Preferred)Python: 1 year (Preferred)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>145000 - 150000 a year</td>\n",
       "      <td>145000 - 150000 a year -  Full-time</td>\n",
       "      <td>145000 - 150000 a year</td>\n",
       "      <td>Identify relevant data sources and data sets t...</td>\n",
       "      <td>Data Scientist - Marketing</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>West CAP</td>\n",
       "      <td>HUMAN was founded in 2012 in a Brooklyn sci-fi...</td>\n",
       "      <td>Estimated 114K – 144K a year</td>\n",
       "      <td>2022-04-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote in New York, NY+2 locations</td>\n",
       "      <td>PostedJust posted</td>\n",
       "      <td>3.5</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114K to 144K per year is Indeeds estimated sa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You’ve worked as a data scientist solving larg...</td>\n",
       "      <td>Data Scientist, BotGuard</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=58cdde046f643...</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Maya Ai inc.</td>\n",
       "      <td>Our Maya team is expanding and we are looking ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-04-13</td>\n",
       "      <td>['Full-time', 'Part-time']</td>\n",
       "      <td>Remote</td>\n",
       "      <td>PostedJust posted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>Python: 1 year (Preferred)SQL: 1 year (Preferred)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77766 - 183411 a year</td>\n",
       "      <td>77766 - 183411 a year -  Full-time Part-time</td>\n",
       "      <td>77766 - 183411 a year</td>\n",
       "      <td>Our Analyst will be dealing with data coming i...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>https://www.indeed.com/company/Maya-Ai-inc./jo...</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EMERGETECH, INC</td>\n",
       "      <td>Description:Job CategoryData ScienceAbout Emer...</td>\n",
       "      <td>Estimated 94.7K – 120K a year</td>\n",
       "      <td>2022-04-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote</td>\n",
       "      <td>PostedJust posted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.7K to 120K per year is Indeeds estimated s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Design and create the data sources that ”citiz...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=95fb128bb025f...</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Recurrent</td>\n",
       "      <td>What's the opportunity?Recurrent is on a missi...</td>\n",
       "      <td>Estimated 119K – 151K a year</td>\n",
       "      <td>2022-04-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote in Seattle, WA</td>\n",
       "      <td>PostedJust posted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>119K to 151K per year is Indeeds estimated sa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Experienced - you have 2+ years of experience ...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=e9ce610b72deb...</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>Ford Motor Company</td>\n",
       "      <td>DearbornFord Motor CompanyAutonomous VehiclesJ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-04-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote in Dearborn, MI</td>\n",
       "      <td>PostedJust posted</td>\n",
       "      <td>4.1</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Experience with methods and tasks to strengthe...</td>\n",
       "      <td>Geospatial Data Scientist - Remote</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=3eddce4bcdcd9...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>SparkCognition</td>\n",
       "      <td>Voted Best Places to Work in Austin, Best Payi...</td>\n",
       "      <td>Estimated 117K – 148K a year</td>\n",
       "      <td>2022-04-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote in Austin, TX</td>\n",
       "      <td>PostedJust posted</td>\n",
       "      <td>4.4</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117K - 148K a year is Indeeds estimated salar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Partnering with project teams in developing an...</td>\n",
       "      <td>Data Scientist II</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=7f998c4f6357e...</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>SparkCognition</td>\n",
       "      <td>Voted Best Places to Work in Austin, Best Payi...</td>\n",
       "      <td>Estimated 132K – 167K a year</td>\n",
       "      <td>2022-04-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote in Austin, TX</td>\n",
       "      <td>PostedJust posted</td>\n",
       "      <td>4.4</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132K to 167K per year is Indeeds estimated sa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Partnering with and guide other data scientist...</td>\n",
       "      <td>Sr. Data Scientist</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=1e818a9a5849c...</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>Humana</td>\n",
       "      <td>The Data Scientist 2 will work within Corpor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-04-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote in Louisville, KY 40202</td>\n",
       "      <td>PostedJust posted</td>\n",
       "      <td>3.6</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Collaboration with other Humana data scientist...</td>\n",
       "      <td>Data Scientist - Actuarial Valuation</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=25d03e8b0593a...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>CoderPad, Inc.</td>\n",
       "      <td>CoderPad's mission is to improve the technical...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-04-13</td>\n",
       "      <td>['Full-time']</td>\n",
       "      <td>Remote</td>\n",
       "      <td>PostedJust posted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>SQL: 3 years (Required)Data analytics: 5 years...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120000 - 135000 a year</td>\n",
       "      <td>120000 - 135000 a year -  Full-time</td>\n",
       "      <td>120000 - 135000 a year</td>\n",
       "      <td>You’ll spend your time combing and analyzing o...</td>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>https://www.indeed.com/company/CoderPad/jobs/S...</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>330 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       company  \\\n",
       "0    Online Technical Services   \n",
       "1                     West CAP   \n",
       "2                 Maya Ai inc.   \n",
       "3              EMERGETECH, INC   \n",
       "4                    Recurrent   \n",
       "..                         ...   \n",
       "325         Ford Motor Company   \n",
       "326             SparkCognition   \n",
       "327             SparkCognition   \n",
       "328                     Humana   \n",
       "329             CoderPad, Inc.   \n",
       "\n",
       "                                           description  \\\n",
       "0    Job descriptionData Scientist, MarketingSAN DI...   \n",
       "1    HUMAN was founded in 2012 in a Brooklyn sci-fi...   \n",
       "2    Our Maya team is expanding and we are looking ...   \n",
       "3    Description:Job CategoryData ScienceAbout Emer...   \n",
       "4    What's the opportunity?Recurrent is on a missi...   \n",
       "..                                                 ...   \n",
       "325  DearbornFord Motor CompanyAutonomous VehiclesJ...   \n",
       "326  Voted Best Places to Work in Austin, Best Payi...   \n",
       "327  Voted Best Places to Work in Austin, Best Payi...   \n",
       "328    The Data Scientist 2 will work within Corpor...   \n",
       "329  CoderPad's mission is to improve the technical...   \n",
       "\n",
       "                  estimated_salary extractdate              job_type_items  \\\n",
       "0                              NaN  2022-04-13               ['Full-time']   \n",
       "1     Estimated 114K – 144K a year  2022-04-13                         NaN   \n",
       "2                              NaN  2022-04-13  ['Full-time', 'Part-time']   \n",
       "3    Estimated 94.7K – 120K a year  2022-04-13                         NaN   \n",
       "4     Estimated 119K – 151K a year  2022-04-13                         NaN   \n",
       "..                             ...         ...                         ...   \n",
       "325                            NaN  2022-04-13                         NaN   \n",
       "326   Estimated 117K – 148K a year  2022-04-13                         NaN   \n",
       "327   Estimated 132K – 167K a year  2022-04-13                         NaN   \n",
       "328                            NaN  2022-04-13                         NaN   \n",
       "329                            NaN  2022-04-13               ['Full-time']   \n",
       "\n",
       "                               location           postdate  rating  \\\n",
       "0                                Remote  PostedJust posted     3.7   \n",
       "1    Remote in New York, NY+2 locations  PostedJust posted     3.5   \n",
       "2                                Remote  PostedJust posted     NaN   \n",
       "3                                Remote  PostedJust posted     NaN   \n",
       "4                 Remote in Seattle, WA  PostedJust posted     NaN   \n",
       "..                                  ...                ...     ...   \n",
       "325              Remote in Dearborn, MI  PostedJust posted     4.1   \n",
       "326                Remote in Austin, TX  PostedJust posted     4.4   \n",
       "327                Remote in Austin, TX  PostedJust posted     4.4   \n",
       "328      Remote in Louisville, KY 40202  PostedJust posted     3.6   \n",
       "329                              Remote  PostedJust posted     NaN   \n",
       "\n",
       "                                         raw_desc_soup  \\\n",
       "0    <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...   \n",
       "1    <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...   \n",
       "2    <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...   \n",
       "3    <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...   \n",
       "4    <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...   \n",
       "..                                                 ...   \n",
       "325  <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...   \n",
       "326  <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...   \n",
       "327  <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...   \n",
       "328  <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...   \n",
       "329  <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...   \n",
       "\n",
       "                                          requirements  \\\n",
       "0    Master's (Preferred)Python: 1 year (Preferred)...   \n",
       "1                                                  NaN   \n",
       "2    Python: 1 year (Preferred)SQL: 1 year (Preferred)   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "325                                                NaN   \n",
       "326                                                NaN   \n",
       "327                                                NaN   \n",
       "328                                                NaN   \n",
       "329  SQL: 3 years (Required)Data analytics: 5 years...   \n",
       "\n",
       "                                       sal_guide_items  \\\n",
       "0                                                  NaN   \n",
       "1     114K to 144K per year is Indeeds estimated sa...   \n",
       "2                                                  NaN   \n",
       "3     94.7K to 120K per year is Indeeds estimated s...   \n",
       "4     119K to 151K per year is Indeeds estimated sa...   \n",
       "..                                                 ...   \n",
       "325                                                NaN   \n",
       "326   117K - 148K a year is Indeeds estimated salar...   \n",
       "327   132K to 167K per year is Indeeds estimated sa...   \n",
       "328                                                NaN   \n",
       "329                                                NaN   \n",
       "\n",
       "                     salary                              salary_and_jtype  \\\n",
       "0    145000 - 150000 a year           145000 - 150000 a year -  Full-time   \n",
       "1                       NaN                                           NaN   \n",
       "2     77766 - 183411 a year  77766 - 183411 a year -  Full-time Part-time   \n",
       "3                       NaN                                           NaN   \n",
       "4                       NaN                                           NaN   \n",
       "..                      ...                                           ...   \n",
       "325                     NaN                                           NaN   \n",
       "326                     NaN                                           NaN   \n",
       "327                     NaN                                           NaN   \n",
       "328                     NaN                                           NaN   \n",
       "329  120000 - 135000 a year           120000 - 135000 a year -  Full-time   \n",
       "\n",
       "             salfromsection  \\\n",
       "0    145000 - 150000 a year   \n",
       "1                       NaN   \n",
       "2     77766 - 183411 a year   \n",
       "3                       NaN   \n",
       "4                       NaN   \n",
       "..                      ...   \n",
       "325                     NaN   \n",
       "326                     NaN   \n",
       "327                     NaN   \n",
       "328                     NaN   \n",
       "329  120000 - 135000 a year   \n",
       "\n",
       "                                               summary  \\\n",
       "0    Identify relevant data sources and data sets t...   \n",
       "1    You’ve worked as a data scientist solving larg...   \n",
       "2    Our Analyst will be dealing with data coming i...   \n",
       "3    Design and create the data sources that ”citiz...   \n",
       "4    Experienced - you have 2+ years of experience ...   \n",
       "..                                                 ...   \n",
       "325  Experience with methods and tasks to strengthe...   \n",
       "326  Partnering with project teams in developing an...   \n",
       "327  Partnering with and guide other data scientist...   \n",
       "328  Collaboration with other Humana data scientist...   \n",
       "329  You’ll spend your time combing and analyzing o...   \n",
       "\n",
       "                                job_title  \\\n",
       "0              Data Scientist - Marketing   \n",
       "1                Data Scientist, BotGuard   \n",
       "2                          Data Scientist   \n",
       "3                          Data Scientist   \n",
       "4                          Data Scientist   \n",
       "..                                    ...   \n",
       "325    Geospatial Data Scientist - Remote   \n",
       "326                     Data Scientist II   \n",
       "327                    Sr. Data Scientist   \n",
       "328  Data Scientist - Actuarial Valuation   \n",
       "329                   Senior Data Analyst   \n",
       "\n",
       "                                                   url schedule  \n",
       "0    https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...     year  \n",
       "1    https://www.indeed.com/rc/clk?jk=58cdde046f643...     year  \n",
       "2    https://www.indeed.com/company/Maya-Ai-inc./jo...     year  \n",
       "3    https://www.indeed.com/rc/clk?jk=95fb128bb025f...     year  \n",
       "4    https://www.indeed.com/rc/clk?jk=e9ce610b72deb...     year  \n",
       "..                                                 ...      ...  \n",
       "325  https://www.indeed.com/rc/clk?jk=3eddce4bcdcd9...      NaN  \n",
       "326  https://www.indeed.com/rc/clk?jk=7f998c4f6357e...     year  \n",
       "327  https://www.indeed.com/rc/clk?jk=1e818a9a5849c...     year  \n",
       "328  https://www.indeed.com/rc/clk?jk=25d03e8b0593a...      NaN  \n",
       "329  https://www.indeed.com/company/CoderPad/jobs/S...     year  \n",
       "\n",
       "[330 rows x 18 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sal_chars(data)\n",
    "data = Pay_period(data)\n",
    "data = sal_strings(data)\n",
    "    \n",
    "data[\"Pay\"] = data[\"Pay\"].apply(split_sal)\n",
    "data[\"Pay\"] = data[\"Pay\"].apply(split_sal2)\n",
    "data[\"Pay\"] = data[\"Pay\"].apply(from_)\n",
    "data[\"Pay\"] = data[\"Pay\"].apply(from_2)\n",
    "data[\"Pay\"] = data[\"Pay\"].apply(up_to)\n",
    "data[\"Pay\"] = data[\"Pay\"].apply(up_to2)\n",
    "data['Pay'] = pd.to_numeric(data['Pay'])\n",
    "    \n",
    "\n",
    "\n",
    "data['comment_text'] = data['comment_text'].str.lower()\n",
    "data['comment_text'] = data['comment_text'].apply(cleanHtml)\n",
    "data['comment_text'] = data['comment_text'].apply(cleanPunc)\n",
    "data['comment_text'] = data['comment_text'].apply(keepAlpha)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pay_period(data):\n",
    "        \"\"\"[Builds a column for rate of Pay so a yearly salary can be computed]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [in munging]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now with Pay periods]\n",
    "        \"\"\"\n",
    "    data['Schedule'] = np.nan\n",
    "    data['Schedule'] = np.where(data['Pay'].str.contains(\"hour\"),\"hour\",data['Schedule'])\n",
    "    data['Schedule'] = np.where(data['Pay'].str.contains(\"week\"),\"week\",data['Schedule'])\n",
    "        data['Schedule'] = np.where(data['Pay'].str.contains(\"day\"),\"day\",data['Schedule'])\n",
    "        data['Schedule'] = np.where(data['Pay'].str.contains(\"year\"),\"year\",data['Schedule'])\n",
    "        data['Schedule'] = np.where(data['Pay'].str.contains(\"NaN\"),np.nan,data['Schedule'])\n",
    "        return data\n",
    "\n",
    "\n",
    "    def sal_strings(data):\n",
    "        \"\"\"[Reduces Pay values to numeric chars only]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [in munging]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now with only numeric chars in Pay col]\n",
    "        \"\"\"\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\" an hour\", \"\")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\" a day\", \"\")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\" a week\", \"\")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\" a month\", \"\")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\" a year\", \"\")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\" +\", \"\")\n",
    "        return data\n",
    "\n",
    "\n",
    "    def split_sal(i):\n",
    "        \"\"\"[Converts salaries given as a range to the average of their min/max]\n",
    "\n",
    "        Args:\n",
    "            i ([row]): [applied to 'Pay' column]\n",
    "\n",
    "        Returns:\n",
    "            [float]: [If given a range, its mean]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lst = i.split(' - ',1)\n",
    "            x = lst[0]\n",
    "            y = lst[1]\n",
    "            return (float(x)+float(y))//2\n",
    "        except:\n",
    "            return i\n",
    "    def split_sal2(i):\n",
    "        try:\n",
    "            lst = i.split('-',1)\n",
    "            x = lst[0]\n",
    "            y = lst[1]\n",
    "            return (float(x)+float(y))//2\n",
    "        except:\n",
    "            return i\n",
    "\n",
    "    def from_(i):\n",
    "        #TODO I think this is too simple, find a way to include data that\n",
    "        # may fall within a range.\n",
    "        \"\"\"[If salary is given with a base amount, returns that]\n",
    "\n",
    "        Args:\n",
    "            i ([row]): [applied to 'Pay' column]\n",
    "\n",
    "        Returns:\n",
    "            [string]: [Lower limit, if given]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lst = i.split('From ',1)\n",
    "            y = lst[1]\n",
    "            return (y)\n",
    "        except:\n",
    "            return i\n",
    "\n",
    "        \n",
    "    def from_2(i):\n",
    "        #TODO I think this is too simple, find a way to include data that\n",
    "        # may fall within a range.\n",
    "        \"\"\"[If salary is given with a base amount, returns that]\n",
    "\n",
    "        Args:\n",
    "            i ([row]): [applied to 'Pay' column]\n",
    "\n",
    "        Returns:\n",
    "            [string]: [Lower limit, if given]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lst = i.split('From',1)\n",
    "            y = lst[1]\n",
    "            return (y)\n",
    "        except:\n",
    "            return i\n",
    "        \n",
    "\n",
    "    def up_to(i):\n",
    "        # TODO combine this with the above\n",
    "        \"\"\"[If salary is given with a max amount, returns that]\n",
    "\n",
    "        Args:\n",
    "            i ([row]): [applied to 'Pay' column]\n",
    "\n",
    "        Returns:\n",
    "            [string]: [Upper limit, if given]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lst = i.split('Up to ',1)\n",
    "            y = lst[1]\n",
    "            return (y)\n",
    "        except:\n",
    "            return i\n",
    "        \n",
    "    def up_to2(i):\n",
    "        # TODO combine this with the above\n",
    "        \"\"\"[If salary is given with a max amount, returns that]\n",
    "\n",
    "        Args:\n",
    "            i ([row]): [applied to 'Pay' column]\n",
    "\n",
    "        Returns:\n",
    "            [string]: [Upper limit, if given]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lst = i.split('Upto',1)\n",
    "            y = lst[1]\n",
    "            return (y)\n",
    "        except:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sal_chars(data)\n",
    "data = Pay_period(data)\n",
    "data = sal_strings(data)\n",
    "    \n",
    "data[\"Pay\"] = data[\"Pay\"].apply(split_sal)\n",
    "data[\"Pay\"] = data[\"Pay\"].apply(split_sal2)\n",
    "data[\"Pay\"] = data[\"Pay\"].apply(from_)\n",
    "data[\"Pay\"] = data[\"Pay\"].apply(from_2)\n",
    "data[\"Pay\"] = data[\"Pay\"].apply(up_to)\n",
    "data[\"Pay\"] = data[\"Pay\"].apply(up_to2)\n",
    "data['Pay'] = pd.to_numeric(data['Pay'])\n",
    "    \n",
    "\n",
    "\n",
    "data['comment_text'] = data['comment_text'].str.lower()\n",
    "data['comment_text'] = data['comment_text'].apply(cleanHtml)\n",
    "data['comment_text'] = data['comment_text'].apply(cleanPunc)\n",
    "data['comment_text'] = data['comment_text'].apply(keepAlpha)\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    data['State'] = ''\n",
    "    data = states_(data)\n",
    "    data[\"City\"] = data[\"Location\"].apply(cities_)\n",
    "    data['ExtractDate']= pd.to_datetime(data['ExtractDate'])\n",
    "    data = chars(data)\n",
    "    data = postD_int(data)\n",
    "    data['DatePosted'] = data.apply( lambda row : pDate(row), axis = 1)\n",
    "    data = deduper(data)\n",
    "        data = sal_chars(data)\n",
    "    data = Pay_period(data)\n",
    "    data = sal_strings(data)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(split_sal)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(split_sal2)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(from_)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(from_2)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(up_to)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(up_to2)\n",
    "    data['Pay'] = pd.to_numeric(data['Pay'])\n",
    "    data = annual(data)\n",
    "    data = acronyms(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include = 'all')\n",
    "## EDA: Initial Summary Statistics\n",
    "\n",
    "## Visualization: Summary Statistics\n",
    "\n",
    "#  Normailze?\n",
    "## EDA: Feature Importance/Selection\n",
    "# Convariance: Strength of Linear Relationships\n",
    "## Visualization: Feature Relations\n",
    "## Data Wrangling\n",
    "## Modelling\n",
    "# #Fit/Train Model\n",
    "# Evaluate Model\n",
    "# Visualization: Communicating Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def munge():\n",
    "    \"\"\"[Cleaning and Featuring Engineering]\n",
    "\n",
    "    Args:\n",
    "        position ([string]): [identifier for scraped dataset]\n",
    "\n",
    "    Returns:\n",
    "        [csv]: [data prepared for ML]\n",
    "    \"\"\"\n",
    "    # Using the position variable to select/process scraped data based the on the query that\n",
    "    # generated it.\n",
    "    data = pd.read_csv('../app/data/total.csv', index_col=1)\n",
    "    data = data[data.Pay != '30+ days ago']\n",
    "    data = data[data.Pay != '$120 per student']\n",
    "\n",
    "    #TODO just name is salary in scrape.py\n",
    "\n",
    "    \n",
    "\n",
    "    def states_(data):\n",
    "        \"\"\"[Builds 'State' feature by splitting 'Location']\n",
    "\n",
    "        Args:\n",
    "            i ([row of data]): ['Location' value]\n",
    "\n",
    "        Returns:\n",
    "            [string]: [state name abbrieviation]\n",
    "        \"\"\"\n",
    "        states = {\" AL\":\"Alabama\", \" AK\":\"Alaska\", \" AS\":\"American Samoa\", \" AZ\":\"Arizona\", \" AR\":\"Arkansas\",\n",
    "        \" CA\":\"California\", \" CO\":\"Colorado\", \" CT\":\"Connecticut\", \" DE\":\"Delaware\", \" DC\":\"District of Columbia\",\n",
    "        \" FL\":\"Florida\", \" GA\":\"Georgia\", \" GU\":\"Guam \", \" HI\":\"Hawaii\", \" ID\":\"Idaho\", \" IL\":\"Illinois\", \" IN\":\"Indiana\",\n",
    "        \" IA\":\"Iowa\", \" KS\":\"Kansas\", \" KY\":\"Kentucky\", \" LA\":\"Louisiana\", \" ME\":\"Maine\", \" MD\":\"Maryland\", \" MA\":\"Massachusetts\",\n",
    "        \" MI\":\"Michigan\", \" MN\":\"Minnesota\", \" MS\":\"Mississippi\", \" MO\":\"Missouri\", \" MT\":\"Montana\", \" NE\":\"Nebraska\",\n",
    "        \" NV\":\"Nevada\", \" NH\":\"New Hampshire\", \" NJ\":\"New Jersey\", \" NM\":\"New Mexico\", \" NY\":\"New York\", \" NC\":\"North Carolina\",\n",
    "        \" ND\":\"North Dakota\", \" MP\":\"Northern Mariana Islands\", \" OH\":\"Ohio\", \" OK\":\"Oklahoma\", \" OR\":\"Oregon\", \" PA\":\"Pennsylvania\",\n",
    "        \" PR\":\"Puerto Rico\", \" RI\":\"Rhode Island\", \" SC\":\"South Carolina\", \" SD\":\"South Dakota\", \" TN\":\"Tennessee\",\n",
    "        \" TX\":\"Texas\", \" UT\":\"Utah\", \" UM\":\"U.S. Minor Outlying Islands\", \" VT\":\"Vermont\", \" VI\":\"Virgin Islands\", \" VA\":\"Virginia\",\n",
    "        \" WA\":\"Washington\", \" WV\":\"West Virginia\", \" WI\":\"Wisconsin\", \" WY\":\"Wyoming\"}\n",
    "        extras = [\"Alaska\", \"Alabama\", \"Arkansas\", \"American Samoa\", \"Arizona\", \"California\", \"Colorado\", \"Connecticut\", \"District \", \"of Columbia\", \"Delaware\", \"Florida\", \"Georgia\", \"Guam\", \"Hawaii\", \"Iowa\", \"Idaho\", \"Illinois\", \"Indiana\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Massachusetts\", \"Maryland\", \"Maine\", \"Michigan\", \"Minnesota\", \"Missouri\", \"Mississippi\", \"Montana\", \"North Carolina\", \"North Dakota\", \"Nebraska\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"Nevada\", \"New York\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Puerto Rico\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Virginia\", \"Virgin Islands\", \"Vermont\", \"Washington\", \"Wisconsin\", \"West Virginia\", \"Wyoming\"]\n",
    "        for k,v in states.items():\n",
    "            data['State'] = np.where(data['Location'].str.contains(k), k, data['State'])\n",
    "            data['State'] = np.where(data['Location'].str.contains(v), k, data['State'])\n",
    "            data['State'] = np.where(data['Location'].str.contains('Remote'), 'Remote', data['State'])\n",
    "            data['State'] = np.where(data['Location'].str.contains('United States'), 'Remote', data['State'])\n",
    "        return data\n",
    "\n",
    "\n",
    "    def cities_(i):\n",
    "        \"\"\"[Builds 'City' feature by splitting 'Location']\n",
    "\n",
    "        Args:\n",
    "            i ([row of data]): ['Location' value]\n",
    "\n",
    "        Returns:\n",
    "            [string]: [city name from 'Location']\n",
    "        \"\"\"\n",
    "        if ',' in i:\n",
    "            return i.partition(',')[0]\n",
    "        if ',' not in i:\n",
    "            return i\n",
    "        elif 'Remote' in i:\n",
    "            return 'Remote'\n",
    "\n",
    "\n",
    "\n",
    "    def count_dupes(data):\n",
    "        \"\"\"[Small helper for quick data integrity check]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [scrapped data]\n",
    "\n",
    "        Returns:\n",
    "            [tuple]: [number of duplicates (if any), number of unique values]\n",
    "        \"\"\"\n",
    "        dupe = 0\n",
    "        uniq = 0\n",
    "        for i in data:\n",
    "            if i == True:\n",
    "                dupe += 1\n",
    "            else:\n",
    "                uniq +=1\n",
    "        return dupe, uniq\n",
    "\n",
    "\n",
    "    def deduper(data):\n",
    "        \"\"\"[Dropes duplicates]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [in munging]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now without dupes]\n",
    "        \"\"\"\n",
    "        data = data.drop_duplicates()\n",
    "        data = data.reset_index(drop=False, inplace=False)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def sal_chars(data):\n",
    "        \"\"\"[Reduces Pay values to alphanumeric chars only]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [in munging]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now with no special chars]\n",
    "        \"\"\"\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\"\\n\", \"\")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\",\", \"\")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\"+\", \" \")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\"$\", \"\", regex=False)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def Pay_period(data):\n",
    "        \"\"\"[Builds a column for rate of Pay so a yearly salary can be computed]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [in munging]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now with Pay periods]\n",
    "        \"\"\"\n",
    "        data['Schedule'] = np.nan\n",
    "        data['Schedule'] = np.where(data['Pay'].str.contains(\"hour\"),\"hour\",data['Schedule'])\n",
    "        data['Schedule'] = np.where(data['Pay'].str.contains(\"week\"),\"week\",data['Schedule'])\n",
    "        data['Schedule'] = np.where(data['Pay'].str.contains(\"day\"),\"day\",data['Schedule'])\n",
    "        data['Schedule'] = np.where(data['Pay'].str.contains(\"year\"),\"year\",data['Schedule'])\n",
    "        data['Schedule'] = np.where(data['Pay'].str.contains(\"NaN\"),np.nan,data['Schedule'])\n",
    "        return data\n",
    "\n",
    "\n",
    "    def sal_strings(data):\n",
    "        \"\"\"[Reduces Pay values to numeric chars only]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [in munging]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now with only numeric chars in Pay col]\n",
    "        \"\"\"\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\" an hour\", \"\")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\" a day\", \"\")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\" a week\", \"\")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\" a month\", \"\")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\" a year\", \"\")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\" +\", \"\")\n",
    "        return data\n",
    "\n",
    "\n",
    "    def split_sal(i):\n",
    "        \"\"\"[Converts salaries given as a range to the average of their min/max]\n",
    "\n",
    "        Args:\n",
    "            i ([row]): [applied to 'Pay' column]\n",
    "\n",
    "        Returns:\n",
    "            [float]: [If given a range, its mean]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lst = i.split(' - ',1)\n",
    "            x = lst[0]\n",
    "            y = lst[1]\n",
    "            return (float(x)+float(y))//2\n",
    "        except:\n",
    "            return i\n",
    "    def split_sal2(i):\n",
    "        try:\n",
    "            lst = i.split('-',1)\n",
    "            x = lst[0]\n",
    "            y = lst[1]\n",
    "            return (float(x)+float(y))//2\n",
    "        except:\n",
    "            return i\n",
    "\n",
    "    def from_(i):\n",
    "        #TODO I think this is too simple, find a way to include data that\n",
    "        # may fall within a range.\n",
    "        \"\"\"[If salary is given with a base amount, returns that]\n",
    "\n",
    "        Args:\n",
    "            i ([row]): [applied to 'Pay' column]\n",
    "\n",
    "        Returns:\n",
    "            [string]: [Lower limit, if given]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lst = i.split('From ',1)\n",
    "            y = lst[1]\n",
    "            return (y)\n",
    "        except:\n",
    "            return i\n",
    "\n",
    "        \n",
    "    def from_2(i):\n",
    "        #TODO I think this is too simple, find a way to include data that\n",
    "        # may fall within a range.\n",
    "        \"\"\"[If salary is given with a base amount, returns that]\n",
    "\n",
    "        Args:\n",
    "            i ([row]): [applied to 'Pay' column]\n",
    "\n",
    "        Returns:\n",
    "            [string]: [Lower limit, if given]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lst = i.split('From',1)\n",
    "            y = lst[1]\n",
    "            return (y)\n",
    "        except:\n",
    "            return i\n",
    "        \n",
    "\n",
    "    def up_to(i):\n",
    "        # TODO combine this with the above\n",
    "        \"\"\"[If salary is given with a max amount, returns that]\n",
    "\n",
    "        Args:\n",
    "            i ([row]): [applied to 'Pay' column]\n",
    "\n",
    "        Returns:\n",
    "            [string]: [Upper limit, if given]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lst = i.split('Up to ',1)\n",
    "            y = lst[1]\n",
    "            return (y)\n",
    "        except:\n",
    "            return i\n",
    "        \n",
    "    def up_to2(i):\n",
    "        # TODO combine this with the above\n",
    "        \"\"\"[If salary is given with a max amount, returns that]\n",
    "\n",
    "        Args:\n",
    "            i ([row]): [applied to 'Pay' column]\n",
    "\n",
    "        Returns:\n",
    "            [string]: [Upper limit, if given]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lst = i.split('Upto',1)\n",
    "            y = lst[1]\n",
    "            return (y)\n",
    "        except:\n",
    "            return i\n",
    "\n",
    "\n",
    "\n",
    "    def pDate(row):\n",
    "        #TODO 64?\n",
    "        \"\"\"[Builds a column for date posted. since Indeed.com only gives values for\n",
    "        postdate relative to day of query.]\n",
    "\n",
    "        Args:\n",
    "            i ([row]): [applied to 'PostDate' column]\n",
    "\n",
    "        Returns:\n",
    "            [date]: [The actual date the posting was created]\n",
    "        \"\"\"\n",
    "        days_ago = row['PostDate']\n",
    "        delta = timedelta(days_ago)\n",
    "        try:\n",
    "            return row['ExtractDate'] - delta\n",
    "        except:\n",
    "            return row\n",
    "\n",
    "\n",
    "    def annual(data):\n",
    "        \"\"\"[Builds an annual salary feature with values for all data]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [in munging]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now with annual salary values]\n",
    "        \"\"\"\n",
    "        data['Salary'] = np.nan\n",
    "        data['Salary'] = np.where(data['Schedule'].str.contains(\"hour\"), data['Pay']*365/7*40, data['Salary'])\n",
    "        data['Salary'] = np.where(data['Schedule'].str.contains(\"day\"), data['Pay']*365/7*5, data['Salary'])\n",
    "        data['Salary'] = np.where(data['Schedule'].str.contains(\"week\"), data['Pay']*365/7, data['Salary'])\n",
    "        data['Salary'] = np.where(data['Schedule'].str.contains(\"month\"), data['Pay']*365/12, data['Salary'])\n",
    "        data['Salary'] = np.where(data['Schedule'].str.contains(\"year\"), data['Pay'], data['Salary'])\n",
    "        return data\n",
    "\n",
    "\n",
    "    def acronyms(data):\n",
    "        \"\"\"[Spells out some commonly encountered acronyms. Supports accuracy of text analysis.]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [in munging]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now with fewer acronymns]\n",
    "        \"\"\"\n",
    "        data[\"JobTitle\"] = data[\"JobTitle\"].str.replace(\"R&D\", \"research development\")\n",
    "        data[\"Summary\"] = data[\"Summary\"].str.replace(\"R&D\", \"research development\")\n",
    "        data[\"Description\"] = data[\"Description\"].str.replace(\"R&D\", \"research development\")\n",
    "        return data\n",
    "\n",
    "\n",
    "    def chars(data):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [in munging]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now without special chars]\n",
    "        \"\"\"\n",
    "        cleaning_list = [\"+\", \"$\", \"/\", \",\", \"?\", \".\", \";\", \":\", \"-\", \"@\", \"!\", \"&\", \"%\", \"^\", \"*\", \")\", \"(\", \"\\n\"]\n",
    "        for item in cleaning_list:\n",
    "            data['PostDate'] = data['PostDate'].str.replace(item, \" \", regex=False)\n",
    "            data['Summary'] = data['Summary'].str.replace(item, \" \",regex=False)\n",
    "            data['Description'] = data['Description'].str.replace(item, \" \",regex=False)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def postD_int(data):\n",
    "        \"\"\"[Reduces or converts relative post dates to numeric chars]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [in munging]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now with only numeric values for post date]\n",
    "        \"\"\"\n",
    "        data[\"PostDate\"] = data[\"PostDate\"].str.replace(\"Active \", \"\")\n",
    "        data[\"PostDate\"] = data[\"PostDate\"].str.replace(\" day ago\", \"\")\n",
    "        data[\"PostDate\"] = data[\"PostDate\"].str.replace(\"%+ days ago\", \"\")\n",
    "        data[\"PostDate\"] = data[\"PostDate\"].str.replace(\"+\", \"\")\n",
    "        data[\"PostDate\"] = data[\"PostDate\"].str.replace(\" days ago\", \"\")\n",
    "        data[\"PostDate\"] = data[\"PostDate\"].str.replace(\"Just posted\", \"0\")\n",
    "        data[\"PostDate\"] = data[\"PostDate\"].str.replace(\"Today\", \"0\")\n",
    "        data[\"PostDate\"] = data[\"PostDate\"].str.replace(\"today\", \"0\")\n",
    "        data['PostDate'] = data['PostDate'].astype('int')\n",
    "        return data\n",
    "    \n",
    "\n",
    "\n",
    "    def roles(data):\n",
    "        \"\"\"[Supports web app display by providing website view table with information\n",
    "        releavent to the job role.]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [munged]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now with specific jobs and roles for each listing]\n",
    "        \"\"\"\n",
    "        #Primary Role\n",
    "        data['Role'] = ''\n",
    "        analyst = ['anal']\n",
    "        eng = ['big data', 'engin', 'data manag', 'data officer']\n",
    "        ds = ['data scien', 'ml', 'deep', 'model', 'modeler','machine', 'deep', 'ai', 'scientist']\n",
    "\n",
    "\n",
    "        data['Role'] = np.where(data['Role'].str.contains(''), 'Other', data['Role'])\n",
    "        for _ in analyst:\n",
    "            data['Role'] = np.where(data['JobTitle'].str.contains(_), 'data analyst', data['Role'])\n",
    "        for _ in eng:\n",
    "            data['Role'] = np.where(data['JobTitle'].str.contains(_), 'data engineer', data['Role'])\n",
    "        for _ in ds:\n",
    "            data['Role'] = np.where(data['JobTitle'].str.contains(_), 'data scientist', data['Role'])\n",
    "\n",
    "\n",
    "        #Focus\n",
    "        data['Focus'] = ''\n",
    "        ml = ['ml', 'deep', 'model', 'modeler','machine', 'deep', 'ai']\n",
    "        sr = ['sr.', 'lead', 'senior', 'manager']\n",
    "        applied = ['applied']\n",
    "\n",
    "        for _ in analyst:\n",
    "            data['Focus'] = np.where(data['JobTitle'].str.contains(_), 'analysis', data['Focus'])\n",
    "        for _ in ml:\n",
    "            data['Focus'] = np.where(data['JobTitle'].str.contains(_), 'machine learning', data['Focus'])\n",
    "        for _ in sr:\n",
    "            data['Focus'] = np.where(data['JobTitle'].str.contains(_), 'senior', data['Focus'])\n",
    "        return data\n",
    "\n",
    "    # Apply the above functions to the selected DataFrame\n",
    "    # These are in a neceassary order of operation as many functions require some cleaning or\n",
    "    # featurization to have occured prior to their call/application.\n",
    "    #data[\"State\"] = data[\"Location\"].apply(states_)\n",
    "    \n",
    "\n",
    "\n",
    "    data['State'] = ''\n",
    "    data = states_(data)\n",
    "    data[\"City\"] = data[\"Location\"].apply(cities_)\n",
    "    data['ExtractDate']= pd.to_datetime(data['ExtractDate'])\n",
    "    data = chars(data)\n",
    "    data = postD_int(data)\n",
    "    data['DatePosted'] = data.apply( lambda row : pDate(row), axis = 1)\n",
    "    data = deduper(data)\n",
    "    data = sal_chars(data)\n",
    "    data = Pay_period(data)\n",
    "    data = sal_strings(data)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(split_sal)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(split_sal2)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(from_)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(from_2)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(up_to)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(up_to2)\n",
    "    data['Pay'] = pd.to_numeric(data['Pay'])\n",
    "    data = annual(data)\n",
    "    data = acronyms(data)\n",
    "    #data.Location = data.City +' ,' + data.State\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Drop a few cols we no longer need\n",
    "    data.drop(columns=['Pay','ExtractDate', 'PostDate'], inplace=True)\n",
    "\n",
    "    for item in ['JobTitle', 'Company', 'Summary', 'Requirements','Description', 'City']:\n",
    "        data[item] = data[item].str.lower()\n",
    "    data = roles(data)\n",
    "    data.to_csv('../app/data/munged_data.csv', index=False)\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "munged = munge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = len(munged[munged.Salary.isnull()])\n",
    "nnull = len(munged[munged.Salary.notnull()])\n",
    "print(f'Droping {len(data)-len(munged)} duplicates and out of date posts leaves {len(munged)} rows of data for the regressor to be trained and tested on.')\n",
    "print(f'- Of those {len(munged)} job postings {nnull} or {round(nnull/len(munged)*100,2)}% include salary information,\\n- The remaining {null} rows, or {round(null/len(munged)*100,2)}% are missing salary data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Outliers: Using both the Z-Score and IQR methods\n",
    "    Now that preprocessed the data and building the target variable of Salary let's see conduct a bit more EDA to see if there are any outliers that could potentially skew how the regressor will learn from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pairplot(data=munged[munged.Salary.notnull()],\n",
    "                  x_vars=['DatePosted'],\n",
    "                  y_vars=['Salary'], height=8, hue=\"Salary\", palette=\"crest\")\n",
    "\n",
    "ax.fig.set_size_inches(18,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot is helpful in isualizing data, but let's program a few functions to detect and remove outliers based on thresholds.\n",
    "With the Z-Score, we can determine any data outside 3 standard deviations from the mean of our salary data to be an outlier.\n",
    "Wheras with the interquartile (IQR) range, the middle 50% of given salaries, I'll set the conventional cutoff of 1.5+/- the IQR as the cutoff.\n",
    "Values found outside of either range will be collected into a list and that'll allow for some fancy indexing so those particular postings can be examined and removed programatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z-Score\n",
    "Salaries 3 standard deviations away from the mean will be listed as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = []\n",
    "def z_detect(munged):\n",
    "    sample = munged[munged['Salary'].notnull()].Salary\n",
    "    threshold=3\n",
    "    mean = np.mean(sample)\n",
    "    std = np.std(sample)\n",
    "    \n",
    "    for i in sample:\n",
    "        z_score = (i-mean)/std\n",
    "        if np.abs(z_score) > threshold:\n",
    "            outliers.append(i)\n",
    "    if len(outliers) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        return outliers\n",
    "z_detect(munged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IQR\n",
    "Salaries outside 1.5 times the interquartile range boundaries, either above or below will be listed as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_detect(munged):\n",
    "    sample = munged[munged['Salary'].notnull()].Salary\n",
    "    Q1, Q3 = np.percentile(sample,[25,75])\n",
    "    iqr = Q3-Q1\n",
    "    lower_bound = Q1-(1.5*iqr)\n",
    "    upper_bound = Q3+(1.5*iqr)\n",
    "    for i in sample:\n",
    "        if (i < lower_bound)  | (i > upper_bound):\n",
    "            outliers.append(i)\n",
    "    if len(outliers) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        return outliers\n",
    "iqr_detect(munged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1):\n",
    "    list_set = set(list1)\n",
    "    # convert the set to the list\n",
    "    unique_list = (list(list_set))\n",
    "    for x in unique_list:\n",
    "        print(x)\n",
    "        \n",
    "mn = min(outliers)\n",
    "mx = max(outliers)\n",
    "print(f'The {len(set(outliers))} Unique Outliers Are:')\n",
    "unique(outliers)\n",
    "print(f'- With a minumum of ${round(mn)} and a maximum of ${round(mx)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at those tables.\n",
    "\n",
    "outliers = munged[munged['Salary'].isin(outliers)]\n",
    "munged.drop(outliers.index, axis=0,inplace=True)\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = len(munged[munged.Salary.isnull()])\n",
    "nnull = len(munged[munged.Salary.notnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pairplot(data=munged[munged.Salary.notnull()],\n",
    "                  x_vars=['DatePosted'],\n",
    "                  y_vars=['Salary'], height=8, hue=\"Salary\", palette=\"crest\")\n",
    "\n",
    "ax.fig.set_size_inches(18,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Salary', data=munged.notnull(), palette='Set3')\n",
    "print(f'- Dropping {len(outliers)} outliers now leaves {nnull}, or {round(nnull/len(munged)*100,2)}%, of rows with with salary information,\\n- The remaining {null} rows, or {round(null/len(munged)*100,2)}%, are missing salary data.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.to_csv(f'../app/data/munged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac83f05b37eb5c5c49ba67e50f0047ddfbc4b30205fc79ee5f327b9c0ac37f55"
  },
  "kernelspec": {
   "display_name": "Python [conda env:indeedapp]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "46216370387cf43888a1dc9433c5a4546bda9feed2ad4f35c2a851da9960dc54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
