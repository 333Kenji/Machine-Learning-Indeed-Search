{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 17\n",
    "\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"ticks\", color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA\n",
    "# Import/Load\n",
    "data = pd.read_csv('../app/data/total.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The following table is a view of the raw data from the scrape. I'll build the target by reducing the pay column to floating point values; this will require text cleaning which will also be conducted for the rest of the table. Location will be split into new columns for City and State while and because Indeed's search results show dates relative to the date the query was made I'll need to use PostDate and ExtractDate build a DatePosted column. It is important to note that JobUrl and date-related columns will not be converted into features for the logistic regression, instead they will be reattached to the data at the end in order to provide additional information and insights through the EC2 application. The only rows that'l  be using for modelling are JobTitle, Company, Summary, Requirements, and Description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Examine\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.location == 'Remote'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values?\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.isnull().any(axis=1)]\n",
    "# every column has nulls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Value Counts\n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.company.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting = ['company','estimated_salary','job_type_items','location','rating','requirements','salary','sal_guide_items','salary_and_jType']\n",
    "for i in interesting:\n",
    "    print(data[i].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i in np.unique(data[data['company'].notnull()].company):\n",
    "    samples.append(i)\n",
    "print(len(samples))\n",
    "samples[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = sum(data.salary.isnull())\n",
    "nnull = sum(data.salary.notnull())\n",
    "print(data.isnull().sum())\n",
    "print('')\n",
    "print(f'The data contains {data.shape[0]} rows of individual job postings with values for {data.shape[1]} columns based on\\n different sections of each post\\'s web page.')\n",
    "print('')\n",
    "print(f'- Out of {len(data)} job postings {nnull} or {round(nnull/len(data)*100,2)}% include some sort of salary information,\\n- The remaining {null} rowws, or {round(null/len(data)*100,2)}% are missing salary data.\\n- Also, there are no null values in the rest of the table meaning we won\\'t need\\n  to lose any data by dropping rows. While the null values in the Pay column will\\n  simply be dropped after we translate the not nulls into the target.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Here we can observe the sort of values held in the Pay column. Aside from removing special characters and spaces I\\'ll extract\\n the remaing non-numeric chars into a new column that\\'ll allow me to build the annual Salary column once the remaining\\n numeric string are converted to floating point.')\n",
    "\n",
    "samples = []\n",
    "for i in np.unique(data[data['sal_guide_items'].notnull()].sal_guide_items):\n",
    "    samples.append(i)\n",
    "samples[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess Data\n",
    "data = pd.read_csv('../app/data/total.csv')\n",
    "#  drop_dupes\n",
    "data = data.drop_duplicates()\n",
    "data.columns = map(str.lower, data.columns)\n",
    "data = data.rename({'title': 'job_title'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sal_chars(data):\n",
    "    cleaned = re.sub(r'[\\n|,|+|$|[|$|\\'|\"]',r'',data)\n",
    "    #cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',data)\n",
    "    #data[\"Pay\"] = data[\"Pay\"].str.replace(\"\\n\", \"\")\n",
    "    #data[\"Pay\"] = data[\"Pay\"].str.replace(\",\", \"\")\n",
    "    #data[\"Pay\"] = data[\"Pay\"].str.replace(\"+\", \" \")\n",
    "    #data[\"Pay\"] = data[\"Pay\"].str.replace(\"$\", \"\", regex=False)\n",
    "    return cleaned\n",
    "\n",
    "def sal_splitter(data):\n",
    "    x = data.split(',')\n",
    "    return x[2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary = data[data.salary.notnull()]\n",
    "data[['salary',]] = salary['salary'].apply(sal_chars)\n",
    "\n",
    "estimated_salary = data[data.estimated_salary.notnull()]\n",
    "data['estimated_salary'] = estimated_salary['estimated_salary'].apply(sal_chars)\n",
    "\n",
    "salfromsection = data[data.salfromsection.notnull()]\n",
    "data[['salfromsection',]] = salfromsection['salfromsection'].apply(sal_chars)\n",
    "\n",
    "salary_and_jtype = data[data.salfromsection.notnull()]\n",
    "data[['salary_and_jtype',]] = salary_and_jtype['salary_and_jtype'].apply(sal_chars)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sal_guide_arr = data[data.sal_guide_items.notnull()]\n",
    "data[['sal_guide_items']] = sal_guide_arr['sal_guide_items'].apply(sal_splitter)\n",
    "\n",
    "sal_guide_arr = data[data.sal_guide_items.notnull()]\n",
    "data[['sal_guide_items']] = sal_guide_arr['sal_guide_items'].apply(sal_chars)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pay_period(data):\n",
    "    z = ''\n",
    "    substrings = ['hour','day','week','year']\n",
    "    for i in substrings:\n",
    "        if i in data:\n",
    "            return i\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "estimated_salary = data[data['estimated_salary'].notnull()]\n",
    "data['Schedule1'] = estimated_salary['estimated_salary'].apply(Pay_period)\n",
    "\n",
    "sal_guide_items = data[data['sal_guide_items'].notnull()]\n",
    "data['Schedule2'] = sal_guide_items['sal_guide_items'].apply(Pay_period)\n",
    "\n",
    "salary = data[data['salary'].notnull()]\n",
    "data['Schedule3'] = salary['salary'].apply(Pay_period)\n",
    "\n",
    "salary_and_jtype = data[data['salary_and_jtype'].notnull()]\n",
    "data['Schedule4'] = salary_and_jtype['salary_and_jtype'].apply(Pay_period)\n",
    "\n",
    "salfromsection = data[data['salfromsection'].notnull()]\n",
    "data['Schedule5'] = salfromsection['salfromsection'].apply(Pay_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cols = ['Schedule1','Schedule2','Schedule3','Schedule4','Schedule5']\n",
    "data['schedule'] = data[val_cols].bfill(axis=1).iloc[:, 0]\n",
    "data.drop(val_cols,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.schedule.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['estimated_salary','sal_guide_items','salary','salary_and_jtype','salfromsection']\n",
    "for sal in col:\n",
    "    data[sal].replace('[^\\d\\-]','',regex=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sal(i):\n",
    "    try:\n",
    "        lst = i.split('-',1)\n",
    "        x = lst[0]\n",
    "        y = lst[1]\n",
    "        \n",
    "        y = re.sub(r'[-]',r'',y)\n",
    "\n",
    "        return (float(x)+float(y))//2\n",
    "    except:\n",
    "        return i\n",
    "\n",
    "for sal in col:\n",
    "    data[sal] = data[sal].apply(lambda x:split_sal(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'salary': 'sal'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['estimated_salary','sal_guide_items','sal','salary_and_jtype','salfromsection']\n",
    "\n",
    "data['salary'] = data[col].bfill(axis=1).iloc[:, 0]\n",
    "data.drop(col,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.salary.value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = data[data.salary.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['salary'] = pd.to_numeric(data['salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(split_sal2)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(from_)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(from_2)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(up_to)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(up_to2)\n",
    "    data['Pay'] = pd.to_numeric(data['Pay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Pay'] = pd.to_numeric(data['Pay'])\n",
    "data['comment_text'] = data['comment_text'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include = 'all')\n",
    "## EDA: Initial Summary Statistics\n",
    "\n",
    "## Visualization: Summary Statistics\n",
    "\n",
    "#  Normailze?\n",
    "## EDA: Feature Importance/Selection\n",
    "# Convariance: Strength of Linear Relationships\n",
    "## Visualization: Feature Relations\n",
    "## Data Wrangling\n",
    "## Modelling\n",
    "# #Fit/Train Model\n",
    "# Evaluate Model\n",
    "# Visualization: Communicating Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def munge():\n",
    "    \"\"\"[Cleaning and Featuring Engineering]\n",
    "\n",
    "    Args:\n",
    "        position ([string]): [identifier for scraped dataset]\n",
    "\n",
    "    Returns:\n",
    "        [csv]: [data prepared for ML]\n",
    "    \"\"\"\n",
    "    # Using the position variable to select/process scraped data based the on the query that\n",
    "    # generated it.\n",
    "    data = pd.read_csv('../app/data/total.csv', index_col=1)\n",
    "    data = data[data.Pay != '30+ days ago']\n",
    "    data = data[data.Pay != '$120 per student']\n",
    "\n",
    "    #TODO just name is salary in scrape.py\n",
    "\n",
    "    \n",
    "\n",
    "    def states_(data):\n",
    "        \"\"\"[Builds 'State' feature by splitting 'Location']\n",
    "\n",
    "        Args:\n",
    "            i ([row of data]): ['Location' value]\n",
    "\n",
    "        Returns:\n",
    "            [string]: [state name abbrieviation]\n",
    "        \"\"\"\n",
    "        states = {\" AL\":\"Alabama\", \" AK\":\"Alaska\", \" AS\":\"American Samoa\", \" AZ\":\"Arizona\", \" AR\":\"Arkansas\",\n",
    "        \" CA\":\"California\", \" CO\":\"Colorado\", \" CT\":\"Connecticut\", \" DE\":\"Delaware\", \" DC\":\"District of Columbia\",\n",
    "        \" FL\":\"Florida\", \" GA\":\"Georgia\", \" GU\":\"Guam \", \" HI\":\"Hawaii\", \" ID\":\"Idaho\", \" IL\":\"Illinois\", \" IN\":\"Indiana\",\n",
    "        \" IA\":\"Iowa\", \" KS\":\"Kansas\", \" KY\":\"Kentucky\", \" LA\":\"Louisiana\", \" ME\":\"Maine\", \" MD\":\"Maryland\", \" MA\":\"Massachusetts\",\n",
    "        \" MI\":\"Michigan\", \" MN\":\"Minnesota\", \" MS\":\"Mississippi\", \" MO\":\"Missouri\", \" MT\":\"Montana\", \" NE\":\"Nebraska\",\n",
    "        \" NV\":\"Nevada\", \" NH\":\"New Hampshire\", \" NJ\":\"New Jersey\", \" NM\":\"New Mexico\", \" NY\":\"New York\", \" NC\":\"North Carolina\",\n",
    "        \" ND\":\"North Dakota\", \" MP\":\"Northern Mariana Islands\", \" OH\":\"Ohio\", \" OK\":\"Oklahoma\", \" OR\":\"Oregon\", \" PA\":\"Pennsylvania\",\n",
    "        \" PR\":\"Puerto Rico\", \" RI\":\"Rhode Island\", \" SC\":\"South Carolina\", \" SD\":\"South Dakota\", \" TN\":\"Tennessee\",\n",
    "        \" TX\":\"Texas\", \" UT\":\"Utah\", \" UM\":\"U.S. Minor Outlying Islands\", \" VT\":\"Vermont\", \" VI\":\"Virgin Islands\", \" VA\":\"Virginia\",\n",
    "        \" WA\":\"Washington\", \" WV\":\"West Virginia\", \" WI\":\"Wisconsin\", \" WY\":\"Wyoming\"}\n",
    "        extras = [\"Alaska\", \"Alabama\", \"Arkansas\", \"American Samoa\", \"Arizona\", \"California\", \"Colorado\", \"Connecticut\", \"District \", \"of Columbia\", \"Delaware\", \"Florida\", \"Georgia\", \"Guam\", \"Hawaii\", \"Iowa\", \"Idaho\", \"Illinois\", \"Indiana\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Massachusetts\", \"Maryland\", \"Maine\", \"Michigan\", \"Minnesota\", \"Missouri\", \"Mississippi\", \"Montana\", \"North Carolina\", \"North Dakota\", \"Nebraska\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"Nevada\", \"New York\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Puerto Rico\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Virginia\", \"Virgin Islands\", \"Vermont\", \"Washington\", \"Wisconsin\", \"West Virginia\", \"Wyoming\"]\n",
    "        for k,v in states.items():\n",
    "            data['State'] = np.where(data['Location'].str.contains(k), k, data['State'])\n",
    "            data['State'] = np.where(data['Location'].str.contains(v), k, data['State'])\n",
    "            data['State'] = np.where(data['Location'].str.contains('Remote'), 'Remote', data['State'])\n",
    "            data['State'] = np.where(data['Location'].str.contains('United States'), 'Remote', data['State'])\n",
    "        return data\n",
    "\n",
    "\n",
    "    def cities_(i):\n",
    "        \"\"\"[Builds 'City' feature by splitting 'Location']\n",
    "\n",
    "        Args:\n",
    "            i ([row of data]): ['Location' value]\n",
    "\n",
    "        Returns:\n",
    "            [string]: [city name from 'Location']\n",
    "        \"\"\"\n",
    "        if ',' in i:\n",
    "            return i.partition(',')[0]\n",
    "        if ',' not in i:\n",
    "            return i\n",
    "        elif 'Remote' in i:\n",
    "            return 'Remote'\n",
    "\n",
    "\n",
    "\n",
    "    def count_dupes(data):\n",
    "        \"\"\"[Small helper for quick data integrity check]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [scrapped data]\n",
    "\n",
    "        Returns:\n",
    "            [tuple]: [number of duplicates (if any), number of unique values]\n",
    "        \"\"\"\n",
    "        dupe = 0\n",
    "        uniq = 0\n",
    "        for i in data:\n",
    "            if i == True:\n",
    "                dupe += 1\n",
    "            else:\n",
    "                uniq +=1\n",
    "        return dupe, uniq\n",
    "\n",
    "\n",
    "    def deduper(data):\n",
    "        \"\"\"[Dropes duplicates]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [in munging]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now without dupes]\n",
    "        \"\"\"\n",
    "        data = data.drop_duplicates()\n",
    "        data = data.reset_index(drop=False, inplace=False)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def sal_chars(data):\n",
    "        \"\"\"[Reduces Pay values to alphanumeric chars only]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [in munging]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now with no special chars]\n",
    "        \"\"\"\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\"\\n\", \"\")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\",\", \"\")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\"+\", \" \")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\"$\", \"\", regex=False)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def Pay_period(data):\n",
    "        \"\"\"[Builds a column for rate of Pay so a yearly salary can be computed]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [in munging]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now with Pay periods]\n",
    "        \"\"\"\n",
    "        data['Schedule'] = np.nan\n",
    "        data['Schedule'] = np.where(data['Pay'].str.contains(\"hour\"),\"hour\",data['Schedule'])\n",
    "        data['Schedule'] = np.where(data['Pay'].str.contains(\"week\"),\"week\",data['Schedule'])\n",
    "        data['Schedule'] = np.where(data['Pay'].str.contains(\"day\"),\"day\",data['Schedule'])\n",
    "        data['Schedule'] = np.where(data['Pay'].str.contains(\"year\"),\"year\",data['Schedule'])\n",
    "        data['Schedule'] = np.where(data['Pay'].str.contains(\"NaN\"),np.nan,data['Schedule'])\n",
    "        return data\n",
    "\n",
    "\n",
    "    def sal_strings(data):\n",
    "        \"\"\"[Reduces Pay values to numeric chars only]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [in munging]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now with only numeric chars in Pay col]\n",
    "        \"\"\"\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\" an hour\", \"\")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\" a day\", \"\")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\" a week\", \"\")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\" a month\", \"\")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\" a year\", \"\")\n",
    "        data[\"Pay\"] = data[\"Pay\"].str.replace(\" +\", \"\")\n",
    "        return data\n",
    "\n",
    "\n",
    "    def split_sal(i):\n",
    "        \"\"\"[Converts salaries given as a range to the average of their min/max]\n",
    "\n",
    "        Args:\n",
    "            i ([row]): [applied to 'Pay' column]\n",
    "\n",
    "        Returns:\n",
    "            [float]: [If given a range, its mean]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lst = i.split(' - ',1)\n",
    "            x = lst[0]\n",
    "            y = lst[1]\n",
    "            return (float(x)+float(y))//2\n",
    "        except:\n",
    "            return i\n",
    "    def split_sal2(i):\n",
    "        try:\n",
    "            lst = i.split('-',1)\n",
    "            x = lst[0]\n",
    "            y = lst[1]\n",
    "            return (float(x)+float(y))//2\n",
    "        except:\n",
    "            return i\n",
    "\n",
    "    def from_(i):\n",
    "        #TODO I think this is too simple, find a way to include data that\n",
    "        # may fall within a range.\n",
    "        \"\"\"[If salary is given with a base amount, returns that]\n",
    "\n",
    "        Args:\n",
    "            i ([row]): [applied to 'Pay' column]\n",
    "\n",
    "        Returns:\n",
    "            [string]: [Lower limit, if given]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lst = i.split('From ',1)\n",
    "            y = lst[1]\n",
    "            return (y)\n",
    "        except:\n",
    "            return i\n",
    "\n",
    "        \n",
    "    def from_2(i):\n",
    "        #TODO I think this is too simple, find a way to include data that\n",
    "        # may fall within a range.\n",
    "        \"\"\"[If salary is given with a base amount, returns that]\n",
    "\n",
    "        Args:\n",
    "            i ([row]): [applied to 'Pay' column]\n",
    "\n",
    "        Returns:\n",
    "            [string]: [Lower limit, if given]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lst = i.split('From',1)\n",
    "            y = lst[1]\n",
    "            return (y)\n",
    "        except:\n",
    "            return i\n",
    "        \n",
    "\n",
    "    def up_to(i):\n",
    "        # TODO combine this with the above\n",
    "        \"\"\"[If salary is given with a max amount, returns that]\n",
    "\n",
    "        Args:\n",
    "            i ([row]): [applied to 'Pay' column]\n",
    "\n",
    "        Returns:\n",
    "            [string]: [Upper limit, if given]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lst = i.split('Up to ',1)\n",
    "            y = lst[1]\n",
    "            return (y)\n",
    "        except:\n",
    "            return i\n",
    "        \n",
    "    def up_to2(i):\n",
    "        # TODO combine this with the above\n",
    "        \"\"\"[If salary is given with a max amount, returns that]\n",
    "\n",
    "        Args:\n",
    "            i ([row]): [applied to 'Pay' column]\n",
    "\n",
    "        Returns:\n",
    "            [string]: [Upper limit, if given]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lst = i.split('Upto',1)\n",
    "            y = lst[1]\n",
    "            return (y)\n",
    "        except:\n",
    "            return i\n",
    "\n",
    "\n",
    "\n",
    "    def pDate(row):\n",
    "        #TODO 64?\n",
    "        \"\"\"[Builds a column for date posted. since Indeed.com only gives values for\n",
    "        postdate relative to day of query.]\n",
    "\n",
    "        Args:\n",
    "            i ([row]): [applied to 'PostDate' column]\n",
    "\n",
    "        Returns:\n",
    "            [date]: [The actual date the posting was created]\n",
    "        \"\"\"\n",
    "        days_ago = row['PostDate']\n",
    "        delta = timedelta(days_ago)\n",
    "        try:\n",
    "            return row['ExtractDate'] - delta\n",
    "        except:\n",
    "            return row\n",
    "\n",
    "\n",
    "    def annual(data):\n",
    "        \"\"\"[Builds an annual salary feature with values for all data]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [in munging]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now with annual salary values]\n",
    "        \"\"\"\n",
    "        data['Salary'] = np.nan\n",
    "        data['Salary'] = np.where(data['Schedule'].str.contains(\"hour\"), data['Pay']*365/7*40, data['Salary'])\n",
    "        data['Salary'] = np.where(data['Schedule'].str.contains(\"day\"), data['Pay']*365/7*5, data['Salary'])\n",
    "        data['Salary'] = np.where(data['Schedule'].str.contains(\"week\"), data['Pay']*365/7, data['Salary'])\n",
    "        data['Salary'] = np.where(data['Schedule'].str.contains(\"month\"), data['Pay']*365/12, data['Salary'])\n",
    "        data['Salary'] = np.where(data['Schedule'].str.contains(\"year\"), data['Pay'], data['Salary'])\n",
    "        return data\n",
    "\n",
    "\n",
    "    def acronyms(data):\n",
    "        \"\"\"[Spells out some commonly encountered acronyms. Supports accuracy of text analysis.]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [in munging]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now with fewer acronymns]\n",
    "        \"\"\"\n",
    "        data[\"JobTitle\"] = data[\"JobTitle\"].str.replace(\"R&D\", \"research development\")\n",
    "        data[\"Summary\"] = data[\"Summary\"].str.replace(\"R&D\", \"research development\")\n",
    "        data[\"Description\"] = data[\"Description\"].str.replace(\"R&D\", \"research development\")\n",
    "        return data\n",
    "\n",
    "\n",
    "    def chars(data):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [in munging]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now without special chars]\n",
    "        \"\"\"\n",
    "        cleaning_list = [\"+\", \"$\", \"/\", \",\", \"?\", \".\", \";\", \":\", \"-\", \"@\", \"!\", \"&\", \"%\", \"^\", \"*\", \")\", \"(\", \"\\n\"]\n",
    "        for item in cleaning_list:\n",
    "            data['PostDate'] = data['PostDate'].str.replace(item, \" \", regex=False)\n",
    "            data['Summary'] = data['Summary'].str.replace(item, \" \",regex=False)\n",
    "            data['Description'] = data['Description'].str.replace(item, \" \",regex=False)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def postD_int(data):\n",
    "        \"\"\"[Reduces or converts relative post dates to numeric chars]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [in munging]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now with only numeric values for post date]\n",
    "        \"\"\"\n",
    "        data[\"PostDate\"] = data[\"PostDate\"].str.replace(\"Active \", \"\")\n",
    "        data[\"PostDate\"] = data[\"PostDate\"].str.replace(\" day ago\", \"\")\n",
    "        data[\"PostDate\"] = data[\"PostDate\"].str.replace(\"%+ days ago\", \"\")\n",
    "        data[\"PostDate\"] = data[\"PostDate\"].str.replace(\"+\", \"\")\n",
    "        data[\"PostDate\"] = data[\"PostDate\"].str.replace(\" days ago\", \"\")\n",
    "        data[\"PostDate\"] = data[\"PostDate\"].str.replace(\"Just posted\", \"0\")\n",
    "        data[\"PostDate\"] = data[\"PostDate\"].str.replace(\"Today\", \"0\")\n",
    "        data[\"PostDate\"] = data[\"PostDate\"].str.replace(\"today\", \"0\")\n",
    "        data['PostDate'] = data['PostDate'].astype('int')\n",
    "        return data\n",
    "    \n",
    "\n",
    "\n",
    "    def roles(data):\n",
    "        \"\"\"[Supports web app display by providing website view table with information\n",
    "        releavent to the job role.]\n",
    "\n",
    "        Args:\n",
    "            data ([DataFrame]): [munged]\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: [Now with specific jobs and roles for each listing]\n",
    "        \"\"\"\n",
    "        #Primary Role\n",
    "        data['Role'] = ''\n",
    "        analyst = ['anal']\n",
    "        eng = ['big data', 'engin', 'data manag', 'data officer']\n",
    "        ds = ['data scien', 'ml', 'deep', 'model', 'modeler','machine', 'deep', 'ai', 'scientist']\n",
    "\n",
    "\n",
    "        data['Role'] = np.where(data['Role'].str.contains(''), 'Other', data['Role'])\n",
    "        for _ in analyst:\n",
    "            data['Role'] = np.where(data['JobTitle'].str.contains(_), 'data analyst', data['Role'])\n",
    "        for _ in eng:\n",
    "            data['Role'] = np.where(data['JobTitle'].str.contains(_), 'data engineer', data['Role'])\n",
    "        for _ in ds:\n",
    "            data['Role'] = np.where(data['JobTitle'].str.contains(_), 'data scientist', data['Role'])\n",
    "\n",
    "\n",
    "        #Focus\n",
    "        data['Focus'] = ''\n",
    "        ml = ['ml', 'deep', 'model', 'modeler','machine', 'deep', 'ai']\n",
    "        sr = ['sr.', 'lead', 'senior', 'manager']\n",
    "        applied = ['applied']\n",
    "\n",
    "        for _ in analyst:\n",
    "            data['Focus'] = np.where(data['JobTitle'].str.contains(_), 'analysis', data['Focus'])\n",
    "        for _ in ml:\n",
    "            data['Focus'] = np.where(data['JobTitle'].str.contains(_), 'machine learning', data['Focus'])\n",
    "        for _ in sr:\n",
    "            data['Focus'] = np.where(data['JobTitle'].str.contains(_), 'senior', data['Focus'])\n",
    "        return data\n",
    "\n",
    "    # Apply the above functions to the selected DataFrame\n",
    "    # These are in a neceassary order of operation as many functions require some cleaning or\n",
    "    # featurization to have occured prior to their call/application.\n",
    "    #data[\"State\"] = data[\"Location\"].apply(states_)\n",
    "    \n",
    "\n",
    "\n",
    "    data['State'] = ''\n",
    "    data = states_(data)\n",
    "    data[\"City\"] = data[\"Location\"].apply(cities_)\n",
    "    data['ExtractDate']= pd.to_datetime(data['ExtractDate'])\n",
    "    data = chars(data)\n",
    "    data = postD_int(data)\n",
    "    data['DatePosted'] = data.apply( lambda row : pDate(row), axis = 1)\n",
    "    data = deduper(data)\n",
    "    data = sal_chars(data)\n",
    "    data = Pay_period(data)\n",
    "    data = sal_strings(data)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(split_sal)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(split_sal2)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(from_)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(from_2)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(up_to)\n",
    "    data[\"Pay\"] = data[\"Pay\"].apply(up_to2)\n",
    "    data['Pay'] = pd.to_numeric(data['Pay'])\n",
    "    data = annual(data)\n",
    "    data = acronyms(data)\n",
    "    #data.Location = data.City +' ,' + data.State\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Drop a few cols we no longer need\n",
    "    data.drop(columns=['Pay','ExtractDate', 'PostDate'], inplace=True)\n",
    "\n",
    "    for item in ['JobTitle', 'Company', 'Summary', 'Requirements','Description', 'City']:\n",
    "        data[item] = data[item].str.lower()\n",
    "    data = roles(data)\n",
    "    data.to_csv('../app/data/munged_data.csv', index=False)\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "munged = munge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = len(munged[munged.Salary.isnull()])\n",
    "nnull = len(munged[munged.Salary.notnull()])\n",
    "print(f'Droping {len(data)-len(munged)} duplicates and out of date posts leaves {len(munged)} rows of data for the regressor to be trained and tested on.')\n",
    "print(f'- Of those {len(munged)} job postings {nnull} or {round(nnull/len(munged)*100,2)}% include salary information,\\n- The remaining {null} rows, or {round(null/len(munged)*100,2)}% are missing salary data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Outliers: Using both the Z-Score and IQR methods\n",
    "    Now that preprocessed the data and building the target variable of Salary let's see conduct a bit more EDA to see if there are any outliers that could potentially skew how the regressor will learn from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pairplot(data=munged[munged.Salary.notnull()],\n",
    "                  x_vars=['DatePosted'],\n",
    "                  y_vars=['Salary'], height=8, hue=\"Salary\", palette=\"crest\")\n",
    "\n",
    "ax.fig.set_size_inches(18,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot is helpful in isualizing data, but let's program a few functions to detect and remove outliers based on thresholds.\n",
    "With the Z-Score, we can determine any data outside 3 standard deviations from the mean of our salary data to be an outlier.\n",
    "Wheras with the interquartile (IQR) range, the middle 50% of given salaries, I'll set the conventional cutoff of 1.5+/- the IQR as the cutoff.\n",
    "Values found outside of either range will be collected into a list and that'll allow for some fancy indexing so those particular postings can be examined and removed programatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z-Score\n",
    "Salaries 3 standard deviations away from the mean will be listed as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = []\n",
    "def z_detect(munged):\n",
    "    sample = munged[munged['Salary'].notnull()].Salary\n",
    "    threshold=3\n",
    "    mean = np.mean(sample)\n",
    "    std = np.std(sample)\n",
    "    \n",
    "    for i in sample:\n",
    "        z_score = (i-mean)/std\n",
    "        if np.abs(z_score) > threshold:\n",
    "            outliers.append(i)\n",
    "    if len(outliers) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        return outliers\n",
    "z_detect(munged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IQR\n",
    "Salaries outside 1.5 times the interquartile range boundaries, either above or below will be listed as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_detect(munged):\n",
    "    sample = munged[munged['Salary'].notnull()].Salary\n",
    "    Q1, Q3 = np.percentile(sample,[25,75])\n",
    "    iqr = Q3-Q1\n",
    "    lower_bound = Q1-(1.5*iqr)\n",
    "    upper_bound = Q3+(1.5*iqr)\n",
    "    for i in sample:\n",
    "        if (i < lower_bound)  | (i > upper_bound):\n",
    "            outliers.append(i)\n",
    "    if len(outliers) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        return outliers\n",
    "iqr_detect(munged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1):\n",
    "    list_set = set(list1)\n",
    "    # convert the set to the list\n",
    "    unique_list = (list(list_set))\n",
    "    for x in unique_list:\n",
    "        print(x)\n",
    "        \n",
    "mn = min(outliers)\n",
    "mx = max(outliers)\n",
    "print(f'The {len(set(outliers))} Unique Outliers Are:')\n",
    "unique(outliers)\n",
    "print(f'- With a minumum of ${round(mn)} and a maximum of ${round(mx)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at those tables.\n",
    "\n",
    "outliers = munged[munged['Salary'].isin(outliers)]\n",
    "munged.drop(outliers.index, axis=0,inplace=True)\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = len(munged[munged.Salary.isnull()])\n",
    "nnull = len(munged[munged.Salary.notnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pairplot(data=munged[munged.Salary.notnull()],\n",
    "                  x_vars=['DatePosted'],\n",
    "                  y_vars=['Salary'], height=8, hue=\"Salary\", palette=\"crest\")\n",
    "\n",
    "ax.fig.set_size_inches(18,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Salary', data=munged.notnull(), palette='Set3')\n",
    "print(f'- Dropping {len(outliers)} outliers now leaves {nnull}, or {round(nnull/len(munged)*100,2)}%, of rows with with salary information,\\n- The remaining {null} rows, or {round(null/len(munged)*100,2)}%, are missing salary data.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.to_csv(f'../app/data/munged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac83f05b37eb5c5c49ba67e50f0047ddfbc4b30205fc79ee5f327b9c0ac37f55"
  },
  "kernelspec": {
   "display_name": "Python [conda env:indeedapp]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "46216370387cf43888a1dc9433c5a4546bda9feed2ad4f35c2a851da9960dc54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
