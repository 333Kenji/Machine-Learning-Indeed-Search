{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "import re\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from to_img import to_img\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 17\n",
    "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, log_loss, roc_auc_score, roc_curve, hamming_loss,precision_score,recall_score,f1_score\n",
    "\n",
    "from confusion import make_confusion_matrix\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "plt.rcParams['font.size'] = 17\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data\n",
    "\n",
    "The data consists of text scraped from every search result for 'data science/scientist' on Indeed.com using the [Requests](https://docs.python-requests.org/en/master/ \"Requests Library\"), Tor, and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/ \"BeautifulSoup\") libraries.\n",
    "\n",
    "### 1.1 Load and Inspect Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# note the change in memory usage.\n",
    "data = pd.read_csv('../app/data/total.csv')\n",
    "print(len(data))\n",
    "data.info(memory_usage='deep')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table is a view of the raw data from the scrape. I'll build the target by reducing the pay column to floating point values; this will require text cleaning which will also be conducted for the rest of the table. Location will be split into new columns for City and State while and because Indeed's search results show dates relative to the date the query was made I'll need to use PostDate and ExtractDate build a DatePosted column. It is important to note that JobUrl and date-related columns will not be converted into features for the logistic regression, instead they will be reattached to the data at the end in order to provide additional information and insights through the EC2 application. The only rows that'l  be using for modelling are JobTitle, Company, Summary, Requirements, and Description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.extractDate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = map(str.lower, data.columns)\n",
    "data = data.rename({'title': 'job_title'}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Dictionary\n",
    "- company<br>\n",
    "&nbsp;\n",
    "&nbsp;name of company\n",
    "- description<br>\n",
    "&nbsp;full text description of job\n",
    "- estimated_salary<br>\n",
    "&nbsp;Indeed's salary estimation\n",
    "- extractDate<br>\n",
    "&nbsp;String(?), date record was webscrapped\n",
    "- job_type_items<br>\n",
    "&nbsp;formatted list of 'full-time', 'part-time', etc\n",
    "- location<br>\n",
    "&nbsp;state and city/remote\n",
    "- postDate<br>\n",
    "&nbsp;date job posting was created\n",
    "- rating<br>\n",
    "&nbsp;Indeed.com ascribed company ratings\n",
    "- raw_desc_soup<br>\n",
    "&nbsp;raw full description, retained for possible future use/analysis\n",
    "- requirements<br>\n",
    "&nbsp;employer-listed educational, aptitude, and experience requirements\n",
    "- sal_guide_items<br>\n",
    "&nbsp;formatted list containing redundant characters and the Indeed salary estimate\n",
    "- salary<br>\n",
    "&nbsp;if given by employer\n",
    "- salary_and_jType<br>\n",
    "&nbsp;formatted list combining salary from salfromsection and job_type_items\n",
    "- salfromsection<br>\n",
    "&nbsp;salary, if given, extracted from job description\n",
    "- summary<br>\n",
    "&nbsp;brief summaries of each job extracted from sach results\n",
    "- title<br>\n",
    "&nbsp;job position title\n",
    "- url<br>\n",
    "&nbsp;job posting/description address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplcates.\n",
    "print(f'Number of dupes: {sum(data.duplicated())}')\n",
    "data[data.duplicated() == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of seeing duplicates coming from the raw scrapping is very unlikely, but this is still a good check to do in case something changes with Indeed.com's front-end code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values are another story. It seems that Indeed.com doesn't require employers to provide values for every feature, this certainly seems to be the case with requirements, not to mention everywhere salary information coud be held.\\\n",
    "Others, like estimated salary and ratings are generated by Indeed.com but don't exist for every posting.\\\n",
    "job_type_items and salary_and_jType will likely be collapsed into a new column for job types, I'll also do the same with all of the salary related features to build the model's target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Diagnose Data\n",
    "In order to proceed with any statistical or exploratory data analysis I will need to wrangle the data quite a bit.\n",
    "Here, I'll list each of the current feature columns and briefly describe what I'll need to do with them.\n",
    "\n",
    "Column Groups and Needs\n",
    "Simple\n",
    "Salary\n",
    "Text\n",
    "etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing\n",
    "print(data.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"The {data.shape[0]} rows of data have {data.shape[1]} features. Since this is a classification problem I'll be selecting one to be the target, leaving me with {data.shape[1]-1} dependent variables to work with.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2a Rating\n",
    "\n",
    "As the only feature coming in with numeric (float) values I should just need to scale it. I'll address missing values by imputing zero. The rationale behind this is that an unrated company should simply be rated as zero.\n",
    "print(sorted(data.rating.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.rating.isnull()].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing(data,feature):\n",
    "    data_len = len(data)\n",
    "    missing_len = len(data[data[feature].isnull()])\n",
    "    try:\n",
    "        print(f'Percent Missing: {round(data_len/missing_len,1)}%')\n",
    "    except:\n",
    "        print('nothing missing')\n",
    "\n",
    "    notna = data[feature].notna()\n",
    "\n",
    "    print(f'Number Missing: {missing_len}')\n",
    "\n",
    "    print(np.mean(data[notna]['rating']))\n",
    "\n",
    "    return data[feature].describe()\n",
    "        \n",
    "missing(data, feature = 'rating')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now I'll drop these. I'm considering what and how to impute. What I'd like to do is run clustering on the data, or perhaps logistic regression. One thing I know is that the picture will become clearer as I gain more data. For now though, the amount of missing values is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['rating'].notnull()]\n",
    "print(len(data))\n",
    "\n",
    "ax = sns.distplot(data.rating, fit=norm, bins=data.rating.nunique(), kde=True)\n",
    "plt.title('raw')\n",
    "#ax.set_xticks(data.rating.unique())\n",
    "plt.axvline(x=np.mean(data.rating), label='mean', c = 'r')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = []\n",
    "def z_detect(data, feature):\n",
    "    sample = data[feature]\n",
    "    threshold=3\n",
    "    mean = np.mean(sample)\n",
    "    std = np.std(sample)\n",
    "    \n",
    "    for i in sample:\n",
    "        z_score = (i-mean)/std\n",
    "        if np.abs(z_score) > threshold:\n",
    "            outliers.append(i)\n",
    "    if len(outliers) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        return sorted(outliers)\n",
    "z_detect(data, feature = 'rating')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_detect(data):\n",
    "    q1 = np.quantile(data.rating, 0.25)\n",
    "    q2 = np.quantile(data.rating, 0.5)\n",
    "    q3 = np.quantile(data.rating, 0.75)\n",
    "\n",
    "    # calc iqr\n",
    "    iqr = (q3 - q1)\n",
    "    \n",
    "    # expand iqr to discern outliers\n",
    "    iqr_x = iqr*1.5\n",
    "\n",
    "    # setting the lower and upper limits\n",
    "    iqr_lower = q1-iqr_x\n",
    "    iqr_upper = q3+iqr_x\n",
    "\n",
    "    # report\n",
    "    print(f'There are {len(data[data.rating > iqr_upper])} records below and {len(data[data.rating < iqr_lower])} above.')\n",
    "\n",
    "\n",
    "\n",
    "    # plotting\n",
    "    sns.displot(data.rating)\n",
    "    plt.axvline(x=q1, label=\"Q1\", c = 'g')\n",
    "    plt.axvline(x=q2, label=\"Q2\", c = 'b')\n",
    "    plt.axvline(x=q3, label=\"Q3\", c = 'r')\n",
    "\n",
    "    plt.axvline(x=iqr_lower, label = 'IQR Lower', c = 'black')\n",
    "    plt.axvline(x=iqr_upper, label = 'IQR Upper', c = 'black')\n",
    "    plt.axvline(x=np.mean(data.rating), label='mean', c = 'y')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    sns.boxplot(data=data, x='rating')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # trimming outliers\n",
    "    trimmed = data[data.rating > iqr_upper]\n",
    "    trimmed = data[data.rating < iqr_lower]\n",
    "    print(len(data))\n",
    "    data = data[data.rating <= iqr_upper]\n",
    "    data = data[data.rating >= iqr_lower]\n",
    "    print(len(data))\n",
    "\n",
    "\n",
    "\n",
    "    # plotting\n",
    "    sns.displot(data.rating)\n",
    "    plt.axvline(x=q1, label=\"Q1\", c = 'g')\n",
    "    plt.axvline(x=q2, label=\"Q2\", c = 'b')\n",
    "    plt.axvline(x=q3, label=\"Q3\", c = 'r')\n",
    "\n",
    "    plt.axvline(x=iqr_lower, label = 'IQR Lower', c = 'black')\n",
    "    plt.axvline(x=iqr_upper, label = 'IQR Upper', c = 'black')\n",
    "    plt.axvline(x=np.mean(data.rating), label='mean', c = 'y')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    sns.boxplot(data=data, x='rating')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.show()\n",
    "    return data, trimmed\n",
    "\n",
    "data, trimmed = iqr_detect(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(data.rating, fit=norm, bins=data.rating.nunique(), kde=True)\n",
    "plt.title('raw')\n",
    "#ax.set_xticks(data.rating.unique())\n",
    "plt.axvline(x=np.mean(data.rating), label='mean', c = 'r')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rating.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scalar = StandardScaler()\n",
    "scaled_ratings = pd.DataFrame(scalar.fit_transform(pd.DataFrame(data.rating)), columns=['rating'])\n",
    "\n",
    "scalar = StandardScaler()\n",
    "scaled_ratings = pd.DataFrame(scalar.fit_transform(pd.DataFrame(data.rating)), columns=['rating'])\n",
    "ax = sns.distplot(scaled_ratings, fit=norm, bins=data.rating.nunique(), kde=True)\n",
    "plt.title('scaled')\n",
    "#ax.set_xticks(data.rating.unique())\n",
    "plt.axvline(x=np.mean(scaled_ratings.rating), label='mean', c = 'r')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "scaled_ratings.rating.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2a Dates\n",
    "\n",
    "'extractdate' & 'postdate'\\\n",
    "extract_date is a string showing the day the record was webscrapped.\n",
    "post_date is a string description of the relative age of each post from the date it was posted to the day it was webscrapped.\n",
    "Converting post_date to a number will give me the the posts ae, which I'll then subtract from it's extract date in order to build a new feature carrying values for the DATE each job posting has been submitted to Indeed.com.\n",
    "\n",
    "The extractdate is the value given by my parser representing the date the data was pulled from Indeed.com.\\\n",
    "The postdate is how many days have passed since the post was made and when the search was conducted (by my webscraper).\\\n",
    "The task is to find the delta (difference), the intervening days since the post was made, and subtract it from the extract date so I can get the actual date publishd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['extractdate', 'postdate']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing(data, feature = 'extractdate'),'\\n')\n",
    "print(missing(data, feature = 'postdate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.postdate.unique(),'\\n')\n",
    "print(data.extractdate.unique(),'\\n')\n",
    "print(data.postdate.value_counts(),'\\n')\n",
    "print(data.extractdate.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postD(row):\n",
    "    \"\"\"Critical function that converts postdate values into relative (in days) distance from the extract date.\n",
    "    Args:\n",
    "        data (string): text descrption of how old the post is.\n",
    "    Returns:\n",
    "        npnan or int: numeric representation of post age\n",
    "    \"\"\"\n",
    "    if row['postdate'] != np.nan:\n",
    "        if row['postdate'] in ['PostedToday', 'PostedJust posted', 'Hiring ongoing']:\n",
    "            delta = timedelta(0)\n",
    "        elif row['postdate'] in ['PostedPosted 1 day ago', 'Posted1 day ago']:\n",
    "            delta = timedelta(1)\n",
    "\n",
    "         \n",
    "        value = pd.to_datetime(row['extractdate'])-delta\n",
    "    return value\n",
    "\n",
    "\n",
    "\n",
    "data['dateposted'] = data.apply( lambda row : postD(row), axis = 1)\n",
    "\n",
    "data[['extractdate', 'postdate','dateposted']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2b Salary\n",
    "\n",
    "'salary'\\\n",
    "Several different approaches will need to be used to extract salary information from this feature because employers provide this information themselves.\n",
    "For one example, the pay periods of hour, day, week etc will need to be scaled to an anuual scale so I can have a standard scale to start wiith.\n",
    "The salary ranges also appear here, so I'll be extracting numeric string characters, converting them into floats representing the lower and upper bounds of each range, and taking their medaian value.\n",
    "\n",
    "'estimated_salary'\\\n",
    "\n",
    "\n",
    "\n",
    "'sal_guide_items'\\\n",
    "\n",
    "\n",
    "'salary_and_jtype' & 'salfromsection'\\\n",
    "These will be wrangled in much the same way. The only other pertinent information in these features would be the job_type data already found in job_type_items,\n",
    "\n",
    "\n",
    "The 'salary' feature is whatever information the employer provided. There are several subgroups within these values:\\\n",
    "&nbsp;work week\\\n",
    "While I could convert these to lists, the only information I need (salary) can be parsed out.\n",
    "\n",
    "\n",
    "The 'estimated_salary' feature \n",
    "Indeed.com provides its own salary estimates in what appear to be bins.\n",
    "These and all other salary ranges will be collapsed to their median values once I've extracted and converted their numeric string reresentations.\n",
    "\n",
    "\n",
    "\n",
    "The task is to find the delta (difference), the intervening days since the post was made, and subtract it from the extract date so I can get the actual date publishd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.salary.unique()[:])\n",
    "print(data.estimated_salary.unique()[:3],'\\n')\n",
    "data[['salary','estimated_salary']].head(3)\n",
    "x = data[['salary','estimated_salary']]\n",
    "for i in x:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def justNumbers(data):\n",
    "    if type(data) == str:\n",
    "        # for estimatedsalary col\n",
    "        if bool(re.search(r'Not provided by employer', data)):\n",
    "            est = data.split(',')\n",
    "            est = est[2].split('a year')[0]\n",
    "            est = re.sub(r'[(a-zA-Z,&,$)]+' '|:]',r'',est)\n",
    "            est = est.replace(\"-\",\"\")\n",
    "            est = est.replace('\"',\"\")\n",
    "            est1 = est.split()[0]\n",
    "            est2 = est.split()[1]\n",
    "  \n",
    "\n",
    "\n",
    "            return (float(est1)+float(est2))/2\n",
    "        # for salary col\n",
    "        else:\n",
    "            if bool(re.search(r'$', data)):\n",
    "                if bool(re.search(r'\\d', data)):\n",
    "                    if bool(re.search(r' - ', data)):\n",
    "                        z = data.split(' - ')\n",
    "                        sal = z[0]+' '+z[1]\n",
    "                    else:\n",
    "\n",
    "                        sal = data\n",
    "\n",
    "\n",
    "                    sal = re.sub(r'[?|!|\\'|\"|#|/|-|(|)|$|' '|:]',r'',sal)\n",
    "                    sal = re.sub(r'[(a-zA-Z,&)]+' '|:]',r'',sal)\n",
    "                    sal = sal.replace(\"-\",\"\")\n",
    "                    sal = sal.strip()\n",
    "                    if bool(re.search(r' ', sal)):\n",
    "                        sal1 = sal.split(' ')[0]\n",
    "                        sal2 = sal.split(' ')[1]\n",
    "                        # Assuming a forty hour workweek.\n",
    "                        if bool(re.search(r'hour', data)):\n",
    "                            return ((float(sal1)+float(sal2))/2)*40*(365/7)\n",
    "                        else:\n",
    "                            return (float(sal1)+float(sal2))/2\n",
    "                    else:\n",
    "                        return float(sal)\n",
    "            \n",
    "        return None\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "data['cleanedsal'] = data.salary.apply(justNumbers)\n",
    "data['cleanedest'] = data.estimated_salary.apply(justNumbers)\n",
    "\n",
    "data['cleanedsal'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salaries(row):\n",
    "    \"\"\"Critical function that converts postdate values into relative (in days) distance from the extract date.\n",
    "    Args:\n",
    "        data (string): text descrption of how old the post is.\n",
    "    Returns:\n",
    "        npnan or int: numeric representation of post age\n",
    "    \"\"\"\n",
    "    if (row['cleanedest'] >0) or (row['cleanedsal'] >0):\n",
    "        if (row['cleanedest'] >0):\n",
    "            return round(float(row['cleanedest']) ,2)\n",
    "        else:\n",
    "            return round(float(row['cleanedsal']) ,2)\n",
    "\n",
    "data['salaries'] = data.apply( lambda row : salaries(row), axis = 1).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO There's no overlap here, which is anticipated. owever, I need to come back and see what's being cleaned out/lost.\n",
    "cols = ['salary','cleanedsal','estimated_salary','cleanedest','salaries']\n",
    "for i in cols:\n",
    "    print(i)\n",
    "    print(len(data[i].unique()),'\\n')\n",
    "\n",
    "data[cols].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.salaries.notna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str_features = data[['company','job_title','location','summary','description']]\n",
    "str_features = data[['company','job_title']]\n",
    "str_features.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText_comp_jtitle(sentence):\n",
    "    \n",
    "    #function to clean the word of any punctuation or special characters\n",
    "\n",
    "    cleaned = sentence.replace('AI/ML','artificial intelligence machine learning')\n",
    "    cleaned = cleaned.replace('AI','artificial intelligence')\n",
    "    cleaned = cleaned.replace('ML','machine learning')\n",
    "    cleaned = cleaned.replace('Sr.','senior')\n",
    "    cleaned = cleaned.lower()\n",
    "    cleaned = re.sub(r'[^a-zA-Z0-9 \\n\\.|.]+', r'', cleaned)\n",
    "    cleaned = cleaned.replace('  ',' ')\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "\n",
    "str_features = data[['company','job_title']]\n",
    "for i in str_features:\n",
    "    data[i] = data[i].apply(cleanText_comp_jtitle)\n",
    "\n",
    "\n",
    "\n",
    "#before = [data.job_title]\n",
    "#after = [data2.job_title]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for i,j in zip(before[0],after[0]):\n",
    "    #print(i)\n",
    "    print(j)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleanText_loc(row):\n",
    "    \n",
    "    #function to clean the word of any punctuation or special characters\n",
    "    states = [', AK', 'Alaska', ', AL', 'Alabama', ', AR', 'Arkansas', ', AZ', 'Arizona', ', CA', 'California', ', CO', 'Colorado', ', CT', 'Connecticut', ', DC', 'District of Columbia', ', DE', 'Delaware', ', FL', 'Florida', ', GA', 'Georgia', ', HI', 'Hawaii', ', IA', 'Iowa', ', ID', 'Idaho', ', IL', 'Illinois', ', IN', 'Indiana', ', KS', 'Kansas',\n",
    " ', KY', 'Kentucky', ', LA', 'Louisiana', ', MA', 'Massachusetts', ', MD', 'Maryland', ', ME', 'Maine', ', MI', 'Michigan', ', MN', 'Minnesota', ', MO', 'Missouri', ', MS', 'Mississippi', ', MT', 'Montana', ', NC', 'North Carolina', ', ND', 'North Dakota', ', NE', 'Nebraska', ', NH', 'New Hampshire', ', NJ', 'New Jersey', ', NM', 'New Mexico',\n",
    " ', NV', 'Nevada', ', NY', 'New York', ', OH', 'Ohio', ', OK', 'Oklahoma', ', OR', 'Oregon', ', PA', 'Pennsylvania', ', RI', 'Rhode Island', ', SC', 'South Carolina', ', SD',\n",
    " 'South Dakota', ', TN', 'Tennessee', ', TX', 'Texas', ', UT', 'Utah', ', VA', 'Virginia', ', VT', 'Vermont', ', WA', 'Washington', ', WI', 'Wisconsin', ', WV', 'West Virginia',\n",
    " ', WY', 'Wyoming']\n",
    "\n",
    "    for i in states:\n",
    "        if i in row['location']:\n",
    "            row['state'] = i.strip(',')\n",
    "        if ',' in row['location']:\n",
    "            row['city'] = row['location'].split(',')[0].replace('Remote in ','')\n",
    "            row['city'] = row['city'].replace('Hybrid','')\n",
    "            row['city'] = row['city'].replace('remote in ','').strip()\n",
    "    \n",
    "    if 'remote in ' in row['location']:\n",
    "        row['remote'] = 'remote in'\n",
    "    elif row['location'] == 'Remote':\n",
    "        row['remote'] = 'remote'\n",
    "\n",
    "\n",
    "    return row\n",
    "\n",
    "data = data.copy()\n",
    "data['remote'] = ''\n",
    "data['state'] = ''\n",
    "data['city'] = ''\n",
    "data = data.apply( lambda row : cleanText_loc(row), axis = 1)\n",
    "data[['location','state','city','remote']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)\n",
    "data.dropna(subset=['description'], inplace=True)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleadText_descriptive(row):\n",
    "    text = row['summary']+' '+row['description']\n",
    "\n",
    "    \n",
    "    sentences = text.split('.')\n",
    "    clean_sentences = []\n",
    "    for i in sentences:\n",
    "        clean_sentence = re.sub(r'[?|!|\\'|\"|#|/|-|,(|)|$|-|' '|:]',r'',i)\n",
    "        clean_sentence = re.sub(r'[?|!|\\'|\"|#|/|-|,(|)|$|-|' '|:]',r'',clean_sentence)\n",
    "        clean_sentence = \" \".join(re.findall(\"[(a-zA-Z,&)]+\", clean_sentence))\n",
    "        clean_sentences.append(clean_sentence)\n",
    "    clean_text = ''\n",
    "    for i in clean_sentences:\n",
    "        clean_text+=' '\n",
    "        clean_text+=i\n",
    "        clean_text = clean_text.strip().lower()\n",
    "    row['text'] =  clean_text\n",
    "    return row\n",
    "\n",
    "\n",
    "data = data.copy()\n",
    "data['text'] = ''\n",
    "data = data.apply( lambda row : cleadText_descriptive(row), axis = 1)\n",
    "data[['summary','description','text']].head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "\n",
    "data2['text'] = data['text'].apply(stemming)\n",
    "data2.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemming(sentence):\n",
    "    LemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = lemmatizer.lemmatize(word)\n",
    "        LemSentence += stem\n",
    "        LemSentence += \" \"\n",
    "    LemSentence = LemSentence.strip()\n",
    "    return LemSentence\n",
    "\n",
    "\n",
    "data['text'] = data['text'].apply(lemming)\n",
    "data.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.salaries.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_data = data[data.salaries.notna()]\n",
    "sal_data.salaries.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = np.quantile(sal_data.salaries, 0.25)\n",
    "q2 = np.quantile(sal_data.salaries, 0.5)\n",
    "q3 = np.quantile(sal_data.salaries, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Q(row):\n",
    "    \"\"\"Subtracts the distance (in days) provided by the postD function.\n",
    "    Args:\n",
    "        row (record in the data): record\n",
    "\n",
    "    Returns:\n",
    "        datetime object: extract date - relative age = actual post date\n",
    "    \"\"\"\n",
    "\n",
    "    salary = row['salaries']\n",
    "    if salary < q1:\n",
    "        return 1\n",
    "    if (salary >= q1) & (salary < q2):\n",
    "        return 2\n",
    "    if (salary >= q2) & (salary < q3):\n",
    "        return 3\n",
    "    if salary >= q3:\n",
    "        return 4\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "data['target'] = data.apply( lambda row : Q(row), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, dtype='int')\n",
    "targets = ohe.fit_transform(pd.DataFrame(data.target))\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "targets = pd.DataFrame(targets,columns=['Q1','Q2','Q3','Q4','unk'])\n",
    "data = data.join(targets)\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['unk'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['company','job_title','state','city','rating','text',,'url', 'Q1', 'Q2', 'Q3', 'Q4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['url'].isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['Q1','Q2','Q3','Q4']\n",
    "X = data.drop(targets, axis=1)\n",
    "X = X.drop(['summary','dateposted','summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_pipe():\n",
    "    targets = ['Q1','Q2','Q3','Q4']\n",
    "    X = data.drop(targets, axis=1)\n",
    "\n",
    "    le_cols = ['company', 'job_title', 'state', 'city']\n",
    "    scal_cols = ['rating']\n",
    "\n",
    "    evaluations = {}\n",
    "\n",
    "    \n",
    "    for i in targets:\n",
    "    # test/train split\n",
    "        y = data[i]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y , test_size=.2, random_state=42)\n",
    "\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('text', TfidfVectorizer(ngram_range=(1,3), analyzer = 'word',max_features=1000,stop_words='english',decode_error='ignore'), 'text'),\n",
    "                ('category', OneHotEncoder(handle_unknown ='ignore'), le_cols),\n",
    "                ('scaler', StandardScaler(), scal_cols)\n",
    "                \n",
    "                ],remainder='drop')\n",
    "        #   populating parameter grid to search\n",
    "        grid = [\n",
    "            {\n",
    "            'classifier' : [LogisticRegression()],\n",
    "            #'classifier__penalty' : ['l1', 'l2'],\n",
    "            #'classifier__C' : np.logspace(-4, 4, 20),\n",
    "            'classifier__solver' : ['liblinear']}\n",
    "            ]\n",
    "\n",
    "        pipe = Pipeline(\n",
    "            steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', LogisticRegression(grid)),\n",
    "            ],\n",
    "            )\n",
    "\n",
    "\n",
    "        \n",
    "        grid_search = GridSearchCV(pipe, param_grid=grid, verbose=2, return_train_score=True)\n",
    "        grid_search.fit(x_train,y_train)\n",
    "\n",
    "        print(\"Best parameter (CV score=%0.3f):\" % grid_search.best_score_)\n",
    "        print(grid_search.best_params_)\n",
    "\n",
    "\n",
    "        if i not in evaluations.keys():\n",
    "            evaluations[i] = {\n",
    "                'grid_search':grid_search,\n",
    "                'x_train':x_train,\n",
    "                'y_train':y_train,\n",
    "                'x_test':x_test,\n",
    "                'y_test':y_test\n",
    "                \n",
    "            }\n",
    "\n",
    "\n",
    "    return evaluations\n",
    "\n",
    "\n",
    "evaluations = grid_search_pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def grid_search_pipe():\n",
    "    targets = ['Q1','Q2','Q3','Q4']\n",
    "    X = data.drop(targets, axis=1)\n",
    "\n",
    "    le_cols = ['company', 'job_title', 'state', 'city']\n",
    "    scal_cols = ['rating']\n",
    "\n",
    "    evaluations = {}\n",
    "\n",
    "    \n",
    "    for i in targets:\n",
    "    # test/train split\n",
    "        y = data[i]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y , test_size=.2, random_state=42)\n",
    "\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('text', TfidfVectorizer(ngram_range=(1,3), analyzer = 'word',max_features=1000,stop_words='english',decode_error='ignore'), 'text'),\n",
    "                ('category', OneHotEncoder(handle_unknown ='ignore'), le_cols),\n",
    "                ('scaler', StandardScaler(), scal_cols)\n",
    "                \n",
    "                ],remainder='drop')\n",
    "        #   populating parameter grid to search\n",
    "        grid = [\n",
    "            {\n",
    "            'classifier' : [LogisticRegression()],\n",
    "            #'classifier__penalty' : ['l1', 'l2'],\n",
    "            #'classifier__C' : np.logspace(-4, 4, 20),\n",
    "            'classifier__solver' : ['liblinear']}\n",
    "            ]\n",
    "\n",
    "        pipe = Pipeline(\n",
    "            steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', LogisticRegression(grid)),\n",
    "            ],\n",
    "            )\n",
    "\n",
    "\n",
    "        \n",
    "        grid_search = GridSearchCV(pipe, param_grid=grid, verbose=2, return_train_score=True)\n",
    "        grid_search.fit(x_train,y_train)\n",
    "\n",
    "        print(\"Best parameter (CV score=%0.3f):\" % grid_search.best_score_)\n",
    "        print(grid_search.best_params_)\n",
    "\n",
    "\n",
    "        if i not in evaluations.keys():\n",
    "            evaluations[i] = {\n",
    "                'grid_search':grid_search,\n",
    "                'x_train':x_train,\n",
    "                'y_train':y_train,\n",
    "                'x_test':x_test,\n",
    "                'y_test':y_test\n",
    "                \n",
    "            }\n",
    "\n",
    "\n",
    "    return evaluations\n",
    "\n",
    "\n",
    "evaluations = grid_search_pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = sum(data.salaries.isnull())\n",
    "nnull = sum(data.salaries.notnull())\n",
    "print('')\n",
    "print(f'The data contains {data.shape[0]} rows of individual job postings with values for {data.shape[1]} columns based on\\n different sections of each post\\'s web page.')\n",
    "print('')\n",
    "print(f'- Out of {len(data)} job postings {nnull} or {round(nnull/len(data)*100,2)}% include some sort of salary information,\\n- The remaining {null} rowws, or {round(null/len(data)*100,2)}% are missing salary data.')\n",
    "#\\n- Also, there are no null values in the rest of the table meaning we won\\'t need\\n  to lose any data by dropping rows. While the null values in the Pay column will\\n  simply be dropped after we translate the not nulls into the target.')\n",
    "\n",
    "\n",
    "data[data['salaries'].notna()].salaries\n",
    "print(missing(data, feature = 'salaries'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'job_type_items'\\\n",
    "    I'm going to treat these as likert values, converrting each of these string formatted lists into an ordinal values for a new column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing(data, feature = 'job_type_items'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "TODO\n",
    "\n",
    "Fri - Sat - Sun\n",
    "- make sure shit has zeros\n",
    "- check if there is missing salary data from dropped columns\n",
    "------ clean: company, description, location, summary, job_title\n",
    "------ split location\n",
    "------ join main text\n",
    "- prepare main text for nlp\n",
    "\n",
    "- drop cols and do eda\n",
    "\n",
    "- gridsearch\n",
    "- model\n",
    "- evaluation\n",
    "- collate all data\n",
    "- find class characteristics, i.e. top employers/locations/words-phrases\n",
    "\n",
    "- Code review and organizing\n",
    "- readme\n",
    "\n",
    "\n",
    "Monday\n",
    "- plots\n",
    "- review kmeans\n",
    "- review indeed\n",
    "- review the best 2-3 other projects\n",
    "- categorical features\n",
    "\n",
    "\n",
    "Tuesday\n",
    "- review remaining projects\n",
    "- categorical features (wrap up)\n",
    "- numeric features\n",
    "- 2hrs linear alg\n",
    "- 2hrs stats\n",
    "- 2hrs proba\n",
    "\n",
    "\n",
    "Wednesday\n",
    "- review all projects, particularly text/readmes\n",
    "- review resume\n",
    "- 2hrs Git: commits and prettify\n",
    "- LinkedIn\n",
    "- Job Sites\n",
    "- 1hr linear alg\n",
    "- 1hr stats\n",
    "- 1hr proba\n",
    "- look at jobs/need for CVs\n",
    "\n",
    "\n",
    "Thurs\n",
    "- 2hr SQL\n",
    "- 1hr Numpy\n",
    "- 1hr Pandas\n",
    "- 1hr SciPyStats\n",
    "- 2hr quiz\n",
    "- Apply\n",
    "\n",
    "\n",
    "Days\n",
    "- 1hr SQL\n",
    "- 1hr Quiz and Practice Questions\n",
    "- 1hr Projects\n",
    "- 1hr Reading/Watching Knoweledge\n",
    "\n",
    "\n",
    "Off\n",
    "- 2hr SQL\n",
    "- 2hr Apply\n",
    "- 3hr Projects\n",
    "- 1hr Reading/Watching Knoweledge\n",
    "- 2hr Quiz and Practice Questions\n",
    "- 1hr linear alg or stats or proba\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def jobtype(data):\n",
    "    \"\"\"Used for creating a new feature containing nominal values for the job type.\n",
    "\n",
    "    Args:\n",
    "        data (string): Values from the 'job_type_items' column.\n",
    "\n",
    "    Returns:\n",
    "        string: Extracted and simplified values for each category.\n",
    "    \"\"\"\n",
    "    if type(data) == str:\n",
    "        if re.search('full', data):\n",
    "            out = 'full'\n",
    "        if re.search('part', data):\n",
    "            out = 'part'\n",
    "        if re.search('contract', data):\n",
    "            out = 'contract'\n",
    "        if re.search('temp', data):\n",
    "            out = 'temp'\n",
    "        return out\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "cols = ['company','description', 'job_type_items','location','postdate','requirements',\t'summary','job_title']\n",
    "for txt_col in cols:\n",
    "    data[txt_col] = data[txt_col].str.lower()\n",
    "\n",
    "\n",
    "\n",
    "data['jobtype'] = data.job_type_items.apply(jobtype)\n",
    "\n",
    "data['text'] = data.description+' '+data.summary\n",
    "#TODO add requirements\n",
    "\n",
    "cols = ['job_type_items','raw_desc_soup','url','description','summary','requirements']\n",
    "\n",
    "\n",
    "data.text = data.text.replace(')',' ')\n",
    "\n",
    "\n",
    "\n",
    "data = annual(data)\n",
    "#data.schedule.value_counts() keep and eye on this, may have to deal with more than just hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'location'\\\n",
    "This feature holds a wealth of information: state, city/remote, and in some cases a zip code.\n",
    "Zip codes seem to be a recent addition to the information I can get from Indeed.com but since its presence is rather sparse I'll just extract city/remote and state values, splitting them into their respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.job_type_items.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'job_title'\\\n",
    "Although the webscrapper searches for 'data scientist' the results contain over 80 unique job titles. This is largely due to a prefix or suffix that provides the role's specialization at that particular company for that particular job posting. These include items like 'Marketing', 'Senior', 'Geospatial...'.\n",
    "I'll start by cleaning and simplifying these values, i.e. appearences of 'Sr' can be converted to 'Senior'. This will significantly reduce the number of unique values, hopefully increasing model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data.job_title.unique()))\n",
    "data.job_title.unique()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'requirements'\\\n",
    "I will most likely merge this in with the description prior to conducting NLP. The rationale is that these explicit requirements may be echoed in the description or summary, so by adding them to the corpus I'll raise the scores derived from their increaseed frequency. Eventually I wiuld like to build a set of requirements that I can use in with a clustering algorithm, like KNN, to find/define subgroups within the sample population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.requirements[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'summary'\\\n",
    "Much like the description, these are unique. COming from the search results page itself, these are brief descriptions of each job. In most cases this is not merely an abridgement of the summary. Like 'requirement' I'll start by lumping these texts in with description so analysis of the corpus can be boosted by the added descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.summary[:3]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'description'\\\n",
    "Each value is a unique string of the full job description. I'll lower and remove special characters but this feature will be dealt with seperately in its own NLP process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'raw_desc_soup'\\\n",
    "I'm only saving this in case I refine or change my parsing. Below is a truncated slice of one of the raw records.\n",
    "\n",
    "'url'\\\n",
    "Like raw_doc_soup, I'm simply keeping these in case I need to go back and re-parse the raw data. In fact, this URL points directly to the page where that raw doc soup is found.\n",
    "I may also be able to detect job post updates and duplicates by using these unique addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.raw_desc_soup[1][:1000]\n",
    "data.url[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Wrangling & Feature Engineering\n",
    "Below are the implementations of the data wrangling steps I described above for each feature. Using Numpy, Regex, and Pandas I'll simutaneaously clean the strings, extract and convert numeric information, and create new feature columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the data looks like at this point. It looks like the 17 original features have been reduced to 11. Most of the columns have been collapsed and cleaned, too. Just a bit more work till this is ready for EDA, NLP, and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data.rating = data.rating.fillna(0)\n",
    "\n",
    "cols = ['extractdate','postdate','schedule','jobtype','salary']\n",
    "data.drop(cols,inplace=True,axis=1)\n",
    "data = data.rename({'annual_sal': 'salary'}, axis=1)\n",
    "\n",
    "data['salary'] = data['salary'].apply(sal_fixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns\n",
    "cols = ['salary','estimated_salary', 'job_type_items','raw_desc_soup', 'sal_guide_items','salary_and_jtype', 'salfromsection']\n",
    "data.drop(cols,inplace=True,axis=1)\n",
    "data.rename(columns={'final_sal': 'salary'},inplace=True)\n",
    "\n",
    "data = data[data.description != 'NaN']\n",
    "\n",
    "len(data)\n",
    "\n",
    "data.description.value_counts()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(memory_usage='deep')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = data.shape\n",
    "print(f\"The {shape[0]} rows of data now have {shape[1]} variables whose values are now cleaned and typed properly.\\nThe only missing values are in Salary which is my target.\\nNext, I'll conduct analysis, encoding, and scaling of each feature, starting with Salary which .\")\n",
    "data.describe(include = 'all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='salary', data=data.notnull(), palette='Set3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = len(data[data.salary.isnull()])\n",
    "nnull = len(data[data.salary.notnull()])\n",
    "print(f'{round(nnull/len(data)*100,2)}% of the data has salary information, should be an easy target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(f'../app/data/wrangled_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged = data\n",
    "munged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = len(munged[munged.salary.isnull()])\n",
    "nnull = len(munged[munged.salary.notnull()])\n",
    "print(f'Droping {len(data)-len(munged)} duplicates and out of date posts leaves {len(munged)} rows of data for the regressor to be trained and tested on.')\n",
    "print(f'- Of those {len(munged)} job postings {nnull} or {round(nnull/len(munged)*100,2)}% include salary information,\\n- The remaining {null} rows, or {round(null/len(munged)*100,2)}% are missing salary data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy.stats import pearsonr, chi2_contingency\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "plt.rcParams['font.size'] = 17\n",
    "\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target\n",
    "\n",
    "### Numeric Features\n",
    "### Categorical Features\n",
    "### Feature Associations\n",
    "#### Feature Selection and Hyperparameter Tuning\n",
    "### Pipeline GridSearch\n",
    "## Modelling\n",
    "## Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Outliers: Using both the Z-Score and IQR methods\n",
    "    Now that preprocessed the data and building the target variable of Salary let's see conduct a bit more EDA to see if there are any outliers that could potentially skew how the regressor will learn from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pairplot(data=munged[munged.salary.notnull()],\n",
    "                  x_vars=['dateposted'],\n",
    "                  y_vars=['salary'], height=8, hue=\"salary\", palette=\"crest\")\n",
    "\n",
    "ax.fig.set_size_inches(18,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot is helpful in isualizing data, but let's program a few functions to detect and remove outliers based on thresholds.\n",
    "With the Z-Score, we can determine any data outside 3 standard deviations from the mean of our salary data to be an outlier.\n",
    "Wheras with the interquartile (IQR) range, the middle 50% of given salaries, I'll set the conventional cutoff of 1.5+/- the IQR as the cutoff.\n",
    "Values found outside of either range will be collected into a list and that'll allow for some fancy indexing so those particular postings can be examined and removed programatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z-Score\n",
    "Salaries 3 standard deviations away from the mean will be listed as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = []\n",
    "def z_detect(munged):\n",
    "    sample = munged[munged['salary'].notnull()].salary\n",
    "    threshold=3\n",
    "    mean = np.mean(sample)\n",
    "    std = np.std(sample)\n",
    "    \n",
    "    for i in sample:\n",
    "        z_score = (i-mean)/std\n",
    "        if np.abs(z_score) > threshold:\n",
    "            outliers.append(i)\n",
    "    if len(outliers) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        return outliers\n",
    "z_detect(munged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IQR\n",
    "Salaries outside 1.5 times the interquartile range boundaries, either above or below will be listed as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_detect(munged):\n",
    "    sample = munged[munged['salary'].notnull()].salary\n",
    "    Q1, Q3 = np.percentile(sample,[25,75])\n",
    "    iqr = Q3-Q1\n",
    "    lower_bound = Q1-(1.5*iqr)\n",
    "    upper_bound = Q3+(1.5*iqr)\n",
    "    for i in sample:\n",
    "        if (i < lower_bound)  | (i > upper_bound):\n",
    "            outliers.append(i)\n",
    "    if len(outliers) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        return outliers\n",
    "set(iqr_detect(munged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1):\n",
    "    list_set = set(list1)\n",
    "    # convert the set to the list\n",
    "    unique_list = (list(list_set))\n",
    "    for x in unique_list:\n",
    "        print(x)\n",
    "        \n",
    "mn = min(outliers)\n",
    "mx = max(outliers)\n",
    "print(f'The {len(set(outliers))} Unique Outliers Are:')\n",
    "unique(outliers)\n",
    "print(f'- With a minumum of ${round(mn)} and a maximum of ${round(mx)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at those tables.\n",
    "\n",
    "outliers = munged[munged['salary'].isin(outliers)]\n",
    "munged.drop(outliers.index, axis=0,inplace=True)\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = len(munged[munged.salary.isnull()])\n",
    "\n",
    "nnull = len(munged[munged.salary.notnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pairplot(data=munged[munged.salary.notnull()],\n",
    "                  x_vars=['dateposted'],\n",
    "                  y_vars=['salary'], height=8, hue=\"salary\", palette=\"crest\")\n",
    "\n",
    "ax.fig.set_size_inches(18,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='salary', data=munged.notnull(), palette='Set3')\n",
    "print(f'- Dropping {len(outliers)} outliers now leaves {nnull}, or {round(nnull/len(munged)*100,2)}%, of rows with with salary information,\\n- The remaining {null} rows, or {round(null/len(munged)*100,2)}%, are missing salary data.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.to_csv(f'../app/data/wrangled_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.drop('location',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac83f05b37eb5c5c49ba67e50f0047ddfbc4b30205fc79ee5f327b9c0ac37f55"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('indeedapp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "46216370387cf43888a1dc9433c5a4546bda9feed2ad4f35c2a851da9960dc54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
