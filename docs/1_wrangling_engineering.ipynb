{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "import re\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from to_img import to_img\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 17\n",
    "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, log_loss, roc_auc_score, roc_curve, hamming_loss,precision_score,recall_score,f1_score\n",
    "\n",
    "from confusion import make_confusion_matrix\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "plt.rcParams['font.size'] = 17\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy.stats import pearsonr, chi2_contingency\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "plt.rcParams['font.size'] = 17\n",
    "\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data\n",
    "\n",
    "The data consists of text scraped from every search result for 'data science/scientist' on Indeed.com using the [Requests](https://docs.python-requests.org/en/master/ \"Requests Library\"), Tor, and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/ \"BeautifulSoup\") libraries.\n",
    "\n",
    "### 1.1 Load and Inspect Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the change in memory usage.\n",
    "data = pd.read_csv('../app/data/total.csv')\n",
    "print(len(data))\n",
    "data.info(memory_usage='deep')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the info table I can see that the only missing values are for the Salary target feature. However, for duplicates it would 'seem' that almost all of my data is duplicated.\\\n",
    "It appears this way because an absolutely distinguishing feature has been removed, the url. That's what these duplicates are, individual records for multiple openings (records/observations,singular data). How this affects NLP and ML will be part of the ongoing discovery process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table is a view of the raw data from the scrape. I'll build the target by reducing the pay column to floating point values; this will require text cleaning which will also be conducted for the rest of the table. Location will be split into new columns for City and State while and because Indeed's search results show dates relative to the date the query was made I'll need to use PostDate and ExtractDate build a DatePosted column. It is important to note that JobUrl and date-related columns will not be converted into features for the logistic regression, instead they will be reattached to the data at the end in order to provide additional information and insights through the EC2 application. The only rows that'l  be using for modelling are JobTitle, Company, Summary, Requirements, and Description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.extractDate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = map(str.lower, data.columns)\n",
    "data = data.rename({'title': 'job_title'}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Dictionary\n",
    "- company<br>\n",
    "&nbsp;\n",
    "&nbsp;name of company\n",
    "- description<br>\n",
    "&nbsp;full text description of job\n",
    "- estimated_salary<br>\n",
    "&nbsp;Indeed's salary estimation\n",
    "- extractDate<br>\n",
    "&nbsp;String(?), date record was webscrapped\n",
    "- job_type_items<br>\n",
    "&nbsp;formatted list of 'full-time', 'part-time', etc\n",
    "- location<br>\n",
    "&nbsp;state and city/remote\n",
    "- postDate<br>\n",
    "&nbsp;date job posting was created\n",
    "- rating<br>\n",
    "&nbsp;Indeed.com ascribed company ratings\n",
    "- raw_desc_soup<br>\n",
    "&nbsp;raw full description, retained for possible future use/analysis\n",
    "- requirements<br>\n",
    "&nbsp;employer-listed educational, aptitude, and experience requirements\n",
    "- sal_guide_items<br>\n",
    "&nbsp;formatted list containing redundant characters and the Indeed salary estimate\n",
    "- salary<br>\n",
    "&nbsp;if given by employer\n",
    "- salary_and_jType<br>\n",
    "&nbsp;formatted list combining salary from salfromsection and job_type_items\n",
    "- salfromsection<br>\n",
    "&nbsp;salary, if given, extracted from job description\n",
    "- summary<br>\n",
    "&nbsp;brief summaries of each job extracted from sach results\n",
    "- title<br>\n",
    "&nbsp;job position title\n",
    "- url<br>\n",
    "&nbsp;job posting/description address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplcates.\n",
    "print(f'Number of dupes: {sum(data.duplicated())}')\n",
    "data[data.duplicated() == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of seeing duplicates coming from the raw scrapping is very unlikely, but this is still a good check to do in case something changes with Indeed.com's front-end code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values are another story. It seems that Indeed.com doesn't require employers to provide values for every feature, this certainly seems to be the case with requirements, not to mention everywhere salary information coud be held.\\\n",
    "Others, like estimated salary and ratings are generated by Indeed.com but don't exist for every posting.\\\n",
    "job_type_items and salary_and_jType will likely be collapsed into a new column for job types, I'll also do the same with all of the salary related features to build the model's target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Diagnose Data\n",
    "In order to proceed with any statistical or exploratory data analysis I will need to wrangle the data quite a bit.\n",
    "Here, I'll list each of the current feature columns and briefly describe what I'll need to do with them.\n",
    "\n",
    "Column Groups and Needs\n",
    "Simple\n",
    "Salary\n",
    "Text\n",
    "etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing\n",
    "print(data.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"The {data.shape[0]} rows of data have {data.shape[1]} features. Since this is a classification problem I'll be selecting one to be the target, leaving me with {data.shape[1]-1} dependent variables to work with.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2a Rating\n",
    "\n",
    "As the only feature coming in with numeric (float) values I should just need to scale it. I'll address missing values by imputing zero. The rationale behind this is that an unrated company should simply be rated as zero.\n",
    "print(sorted(data.rating.unique()))\n",
    "\n",
    "'rating'\\\n",
    "Roughly a third of the data is missing values for the company rating feature. At first I used np.fillna(0) to imput 0 for all the missing values but that sharply right-skewed the distribution towards zero.\\\n",
    "Then I checked is scaling the data would fix the bi-modality but I think I have two options here, drop all those records or drop a potentially valuable feature.\\\n",
    "For now, I'll drop the records, I can always get more, and in the future I could conduct linear regression to imput the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.rating.isnull()].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing(data,feature):\n",
    "    data_len = len(data)\n",
    "    missing_len = len(data[data[feature].isnull()])\n",
    "    try:\n",
    "        print(f'Percent Missing: {round(data_len/missing_len,1)}%')\n",
    "    except:\n",
    "        print('nothing missing')\n",
    "\n",
    "    notna = data[feature].notna()\n",
    "\n",
    "    print(f'Number Missing: {missing_len}')\n",
    "\n",
    "    print(np.mean(data[notna]['rating']))\n",
    "\n",
    "    return data[feature].describe()\n",
    "        \n",
    "missing(data, feature = 'rating')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO imput the rating with a regressor.\n",
    "sns.histplot(data=data[data.target ==1], x='rating')\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()\n",
    "\n",
    "data.primarytitle.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = MinMaxScaler()\n",
    "scale = scale.fit_transform(pd.DataFrame(data['rating']))\n",
    "sns.distplot(scale)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=data[data.target ==1], x='rating')\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now I'll drop these. I'm considering what and how to impute. What I'd like to do is run clustering on the data, or perhaps logistic regression. One thing I know is that the picture will become clearer as I gain more data. For now though, the amount of missing values is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['rating'].notnull()]\n",
    "print(len(data))\n",
    "\n",
    "ax = sns.distplot(data.rating, fit=norm, bins=data.rating.nunique(), kde=True)\n",
    "plt.title('raw')\n",
    "#ax.set_xticks(data.rating.unique())\n",
    "plt.axvline(x=np.mean(data.rating), label='mean', c = 'r')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = []\n",
    "def z_detect(data, feature):\n",
    "    sample = data[feature]\n",
    "    threshold=3\n",
    "    mean = np.mean(sample)\n",
    "    std = np.std(sample)\n",
    "    \n",
    "    for i in sample:\n",
    "        z_score = (i-mean)/std\n",
    "        if np.abs(z_score) > threshold:\n",
    "            outliers.append(i)\n",
    "    if len(outliers) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        return sorted(outliers)\n",
    "z_detect(data, feature = 'rating')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_detect(data):\n",
    "    q1 = np.quantile(data.rating, 0.25)\n",
    "    q2 = np.quantile(data.rating, 0.5)\n",
    "    q3 = np.quantile(data.rating, 0.75)\n",
    "\n",
    "    # calc iqr\n",
    "    iqr = (q3 - q1)\n",
    "    \n",
    "    # expand iqr to discern outliers\n",
    "    iqr_x = iqr*1.5\n",
    "\n",
    "    # setting the lower and upper limits\n",
    "    iqr_lower = q1-iqr_x\n",
    "    iqr_upper = q3+iqr_x\n",
    "\n",
    "    # report\n",
    "    print(f'There are {len(data[data.rating > iqr_upper])} records below and {len(data[data.rating < iqr_lower])} above.')\n",
    "\n",
    "\n",
    "\n",
    "    # plotting\n",
    "    sns.displot(data.rating)\n",
    "    plt.axvline(x=q1, label=\"Q1\", c = 'g')\n",
    "    plt.axvline(x=q2, label=\"Q2\", c = 'b')\n",
    "    plt.axvline(x=q3, label=\"Q3\", c = 'r')\n",
    "\n",
    "    plt.axvline(x=iqr_lower, label = 'IQR Lower', c = 'black')\n",
    "    plt.axvline(x=iqr_upper, label = 'IQR Upper', c = 'black')\n",
    "    plt.axvline(x=np.mean(data.rating), label='mean', c = 'y')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    sns.boxplot(data=data, x='rating')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # trimming outliers\n",
    "    trimmed = data[data.rating > iqr_upper]\n",
    "    trimmed = data[data.rating < iqr_lower]\n",
    "    print(len(data))\n",
    "    data = data[data.rating <= iqr_upper]\n",
    "    data = data[data.rating >= iqr_lower]\n",
    "    print(len(data))\n",
    "\n",
    "\n",
    "\n",
    "    # plotting\n",
    "    sns.displot(data.rating)\n",
    "    plt.axvline(x=q1, label=\"Q1\", c = 'g')\n",
    "    plt.axvline(x=q2, label=\"Q2\", c = 'b')\n",
    "    plt.axvline(x=q3, label=\"Q3\", c = 'r')\n",
    "\n",
    "    plt.axvline(x=iqr_lower, label = 'IQR Lower', c = 'black')\n",
    "    plt.axvline(x=iqr_upper, label = 'IQR Upper', c = 'black')\n",
    "    plt.axvline(x=np.mean(data.rating), label='mean', c = 'y')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    sns.boxplot(data=data, x='rating')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.show()\n",
    "    return data, trimmed\n",
    "\n",
    "data, trimmed = iqr_detect(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(data.rating, fit=norm, bins=data.rating.nunique(), kde=True)\n",
    "plt.title('raw')\n",
    "#ax.set_xticks(data.rating.unique())\n",
    "plt.axvline(x=np.mean(data.rating), label='mean', c = 'r')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rating.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scalar = StandardScaler()\n",
    "scaled_ratings = pd.DataFrame(scalar.fit_transform(pd.DataFrame(data.rating)), columns=['rating'])\n",
    "\n",
    "scalar = StandardScaler()\n",
    "scaled_ratings = pd.DataFrame(scalar.fit_transform(pd.DataFrame(data.rating)), columns=['rating'])\n",
    "ax = sns.distplot(scaled_ratings, fit=norm, bins=data.rating.nunique(), kde=True)\n",
    "plt.title('scaled')\n",
    "#ax.set_xticks(data.rating.unique())\n",
    "plt.axvline(x=np.mean(scaled_ratings.rating), label='mean', c = 'r')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "scaled_ratings.rating.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2a Dates\n",
    "\n",
    "'extractdate' & 'postdate'\\\n",
    "extract_date is a string showing the day the record was webscrapped.\n",
    "post_date is a string description of the relative age of each post from the date it was posted to the day it was webscrapped.\n",
    "Converting post_date to a number will give me the the posts ae, which I'll then subtract from it's extract date in order to build a new feature carrying values for the DATE each job posting has been submitted to Indeed.com.\n",
    "\n",
    "The extractdate is the value given by my parser representing the date the data was pulled from Indeed.com.\\\n",
    "The postdate is how many days have passed since the post was made and when the search was conducted (by my webscraper).\\\n",
    "The task is to find the delta (difference), the intervening days since the post was made, and subtract it from the extract date so I can get the actual date publishd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['extractdate', 'postdate']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing(data, feature = 'extractdate'),'\\n')\n",
    "print(missing(data, feature = 'postdate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.postdate.unique(),'\\n')\n",
    "print(data.extractdate.unique(),'\\n')\n",
    "print(data.postdate.value_counts(),'\\n')\n",
    "print(data.extractdate.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postD(row):\n",
    "    \"\"\"Critical function that converts postdate values into relative (in days) distance from the extract date.\n",
    "    Args:\n",
    "        data (string): text descrption of how old the post is.\n",
    "    Returns:\n",
    "        npnan or int: numeric representation of post age\n",
    "    \"\"\"\n",
    "    if row['postdate'] != np.nan:\n",
    "        if row['postdate'] in ['PostedToday', 'PostedJust posted', 'Hiring ongoing']:\n",
    "            delta = timedelta(0)\n",
    "        elif row['postdate'] in ['PostedPosted 1 day ago', 'Posted1 day ago']:\n",
    "            delta = timedelta(1)\n",
    "\n",
    "         \n",
    "        value = pd.to_datetime(row['extractdate'])-delta\n",
    "    return value\n",
    "\n",
    "\n",
    "\n",
    "data['dateposted'] = data.apply( lambda row : postD(row), axis = 1)\n",
    "\n",
    "data[['extractdate', 'postdate','dateposted']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2b Salary\n",
    "\n",
    "'salary'\\\n",
    "Several different approaches will need to be used to extract salary information from this feature because employers provide this information themselves.\n",
    "For one example, the pay periods of hour, day, week etc will need to be scaled to an anuual scale so I can have a standard scale to start wiith.\n",
    "The salary ranges also appear here, so I'll be extracting numeric string characters, converting them into floats representing the lower and upper bounds of each range, and taking their medaian value.\n",
    "\n",
    "'estimated_salary'\\\n",
    "\n",
    "\n",
    "\n",
    "'sal_guide_items'\\\n",
    "\n",
    "\n",
    "'salary_and_jtype' & 'salfromsection'\\\n",
    "These will be wrangled in much the same way. The only other pertinent information in these features would be the job_type data already found in job_type_items,\n",
    "\n",
    "\n",
    "The 'salary' feature is whatever information the employer provided. There are several subgroups within these values:\\\n",
    "&nbsp;work week\\\n",
    "While I could convert these to lists, the only information I need (salary) can be parsed out.\n",
    "\n",
    "\n",
    "The 'estimated_salary' feature \n",
    "Indeed.com provides its own salary estimates in what appear to be bins.\n",
    "These and all other salary ranges will be collapsed to their median values once I've extracted and converted their numeric string reresentations.\n",
    "\n",
    "\n",
    "\n",
    "The task is to find the delta (difference), the intervening days since the post was made, and subtract it from the extract date so I can get the actual date publishd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.salary.unique()[:])\n",
    "print(data.estimated_salary.unique()[:3],'\\n')\n",
    "data[['salary','estimated_salary']].head(3)\n",
    "x = data[['salary','estimated_salary']]\n",
    "for i in x:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def justNumbers(data):\n",
    "    if type(data) == str:\n",
    "        # for estimatedsalary col\n",
    "        if bool(re.search(r'Not provided by employer', data)):\n",
    "            est = data.split(',')\n",
    "            est = est[2].split('a year')[0]\n",
    "            est = re.sub(r'[(a-zA-Z,&,$)]+' '|:]',r'',est)\n",
    "            est = est.replace(\"-\",\"\")\n",
    "            est = est.replace('\"',\"\")\n",
    "            est1 = est.split()[0]\n",
    "            est2 = est.split()[1]\n",
    "  \n",
    "\n",
    "\n",
    "            return (float(est1)+float(est2))/2\n",
    "        # for salary col\n",
    "        else:\n",
    "            if bool(re.search(r'$', data)):\n",
    "                if bool(re.search(r'\\d', data)):\n",
    "                    if bool(re.search(r' - ', data)):\n",
    "                        z = data.split(' - ')\n",
    "                        sal = z[0]+' '+z[1]\n",
    "                    else:\n",
    "\n",
    "                        sal = data\n",
    "\n",
    "\n",
    "                    sal = re.sub(r'[?|!|\\'|\"|#|/|-|(|)|$|' '|:]',r'',sal)\n",
    "                    sal = re.sub(r'[(a-zA-Z,&)]+' '|:]',r'',sal)\n",
    "                    sal = sal.replace(\"-\",\"\")\n",
    "                    sal = sal.strip()\n",
    "                    if bool(re.search(r' ', sal)):\n",
    "                        sal1 = sal.split(' ')[0]\n",
    "                        sal2 = sal.split(' ')[1]\n",
    "                        # Assuming a forty hour workweek.\n",
    "                        if bool(re.search(r'hour', data)):\n",
    "                            return ((float(sal1)+float(sal2))/2)*40*(365/7)\n",
    "                        else:\n",
    "                            return (float(sal1)+float(sal2))/2\n",
    "                    else:\n",
    "                        return float(sal)\n",
    "            \n",
    "        return None\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "data['cleanedsal'] = data.salary.apply(justNumbers)\n",
    "data['cleanedest'] = data.estimated_salary.apply(justNumbers)\n",
    "\n",
    "data['cleanedsal'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salaries(row):\n",
    "    \"\"\"Critical function that converts postdate values into relative (in days) distance from the extract date.\n",
    "    Args:\n",
    "        data (string): text descrption of how old the post is.\n",
    "    Returns:\n",
    "        npnan or int: numeric representation of post age\n",
    "    \"\"\"\n",
    "    if (row['cleanedest'] >0) or (row['cleanedsal'] >0):\n",
    "        if (row['cleanedest'] >0):\n",
    "            return round(float(row['cleanedest']) ,2)\n",
    "        else:\n",
    "            return round(float(row['cleanedsal']) ,2)\n",
    "\n",
    "data['salaries'] = data.apply( lambda row : salaries(row), axis = 1).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO There's no overlap here, which is anticipated. owever, I need to come back and see what's being cleaned out/lost.\n",
    "cols = ['salary','cleanedsal','estimated_salary','cleanedest','salaries']\n",
    "for i in cols:\n",
    "    print(i)\n",
    "    print(len(data[i].unique()),'\\n')\n",
    "\n",
    "data[cols].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.salaries.notna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str_features = data[['company','job_title','location','summary','description']]\n",
    "str_features = data[['company','job_title']]\n",
    "str_features.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText_comp_jtitle(sentence):\n",
    "    \n",
    "    #function to clean the word of any punctuation or special characters\n",
    "\n",
    "    cleaned = sentence.replace('AI/ML','artificial intelligence machine learning')\n",
    "    cleaned = cleaned.replace('AI','artificial intelligence')\n",
    "    cleaned = cleaned.replace('ML','machine learning')\n",
    "    cleaned = cleaned.replace('Sr.','senior')\n",
    "    cleaned = cleaned.lower()\n",
    "    cleaned = re.sub(r'[^a-zA-Z0-9 \\n\\.|.]+', r'', cleaned)\n",
    "    cleaned = cleaned.replace('  ',' ')\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "\n",
    "str_features = data[['company','job_title']]\n",
    "for i in str_features:\n",
    "    data[i] = data[i].apply(cleanText_comp_jtitle)\n",
    "\n",
    "\n",
    "\n",
    "#before = [data.job_title]\n",
    "#after = [data2.job_title]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for i,j in zip(before[0],after[0]):\n",
    "    #print(i)\n",
    "    print(j)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleanText_loc(row):\n",
    "    \n",
    "    #function to clean the word of any punctuation or special characters\n",
    "    states = [', AK', 'Alaska', ', AL', 'Alabama', ', AR', 'Arkansas', ', AZ', 'Arizona', ', CA', 'California', ', CO', 'Colorado', ', CT', 'Connecticut', ', DC', 'District of Columbia', ', DE', 'Delaware', ', FL', 'Florida', ', GA', 'Georgia', ', HI', 'Hawaii', ', IA', 'Iowa', ', ID', 'Idaho', ', IL', 'Illinois', ', IN', 'Indiana', ', KS', 'Kansas',\n",
    " ', KY', 'Kentucky', ', LA', 'Louisiana', ', MA', 'Massachusetts', ', MD', 'Maryland', ', ME', 'Maine', ', MI', 'Michigan', ', MN', 'Minnesota', ', MO', 'Missouri', ', MS', 'Mississippi', ', MT', 'Montana', ', NC', 'North Carolina', ', ND', 'North Dakota', ', NE', 'Nebraska', ', NH', 'New Hampshire', ', NJ', 'New Jersey', ', NM', 'New Mexico',\n",
    " ', NV', 'Nevada', ', NY', 'New York', ', OH', 'Ohio', ', OK', 'Oklahoma', ', OR', 'Oregon', ', PA', 'Pennsylvania', ', RI', 'Rhode Island', ', SC', 'South Carolina', ', SD',\n",
    " 'South Dakota', ', TN', 'Tennessee', ', TX', 'Texas', ', UT', 'Utah', ', VA', 'Virginia', ', VT', 'Vermont', ', WA', 'Washington', ', WI', 'Wisconsin', ', WV', 'West Virginia',\n",
    " ', WY', 'Wyoming']\n",
    "\n",
    "    for i in states:\n",
    "        if i in row['location']:\n",
    "            row['state'] = i.strip(',')\n",
    "        if ',' in row['location']:\n",
    "            row['city'] = row['location'].split(',')[0].replace('Remote in ','')\n",
    "            row['city'] = row['city'].replace('Hybrid','')\n",
    "            row['city'] = row['city'].replace('remote in ','').strip()\n",
    "    \n",
    "    if 'remote in ' in row['location']:\n",
    "        row['remote'] = 'remote in'\n",
    "    elif row['location'] == 'Remote':\n",
    "        row['remote'] = 'remote'\n",
    "\n",
    "\n",
    "    return row\n",
    "\n",
    "data = data.copy()\n",
    "data['remote'] = ''\n",
    "data['state'] = ''\n",
    "data['city'] = ''\n",
    "data = data.apply( lambda row : cleanText_loc(row), axis = 1)\n",
    "data[['location','state','city','remote']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)\n",
    "data.dropna(subset=['description'], inplace=True)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO save text column in its raw form\n",
    "#data.drop(['cleaned','comment_text'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleadText_descriptive(row):\n",
    "    text = row['summary']+' '+row['description']\n",
    "\n",
    "    \n",
    "    sentences = text.split('.')\n",
    "    clean_sentences = []\n",
    "    for i in sentences:\n",
    "        clean_sentence = re.sub(r'[?|!|\\'|\"|#|/|-|,(|)|$|-|' '|:]',r'',i)\n",
    "        clean_sentence = re.sub(r'[?|!|\\'|\"|#|/|-|,(|)|$|-|' '|:]',r'',clean_sentence)\n",
    "        clean_sentence = \" \".join(re.findall(\"[(a-zA-Z,&)]+\", clean_sentence))\n",
    "        clean_sentences.append(clean_sentence)\n",
    "    clean_text = ''\n",
    "    for i in clean_sentences:\n",
    "        clean_text+=' '\n",
    "        clean_text+=i\n",
    "        clean_text = clean_text.strip().lower()\n",
    "    row['text'] =  clean_text\n",
    "    return row\n",
    "\n",
    "\n",
    "data = data.copy()\n",
    "data['text'] = ''\n",
    "data = data.apply( lambda row : cleadText_descriptive(row), axis = 1)\n",
    "data[['summary','description','text']].head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemming(sentence):\n",
    "    LemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = lemmatizer.lemmatize(word)\n",
    "        LemSentence += stem\n",
    "        LemSentence += \" \"\n",
    "    LemSentence = LemSentence.strip()\n",
    "    return LemSentence\n",
    "\n",
    "\n",
    "data['text'] = data['text'].apply(lemming)\n",
    "data.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "### 2.2 Univariate Feature Analysis & Centering\n",
    "I'm going to employ visual and statistical methods to analyze each feature, starting with the target, and outside of scaling, prepare them for logistic regression which will require that each feature's vaues are normally distributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.salaries.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_data = data[data.salaries.notna()]\n",
    "sal_data.salaries.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = np.quantile(sal_data.salaries, 0.25)\n",
    "q2 = np.quantile(sal_data.salaries, 0.5)\n",
    "q3 = np.quantile(sal_data.salaries, 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Target\n",
    "Before I can begin splitting the data I need to set the target for my methodology of training four seperate logistic regression models. I'm doing this because I'd like my classifications to be as accurate as possible, and also, by building my NLP strategy around a particular label, i.e. finding common words for that label as opposed to being generalized through the entire corpus\\\n",
    "\n",
    "I'm going to one-hot-encode the target feature so I can select each of the next columns as my y - one for each model.\n",
    "\n",
    "\n",
    "\n",
    "##\n",
    "I need to make one model for each target. Before doing so, I'll need to use gridsearch to find the best hyperperamters.\n",
    "Looping through each target, I set y to that specific single-column binary '(for target in targets target == 0 or 1)'\n",
    "Then do a test train split before setting up the pipeline. The grid being use in this case is the selection of hyperperamters I want to check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Q(row):\n",
    "    \"\"\"Subtracts the distance (in days) provided by the postD function.\n",
    "    Args:\n",
    "        row (record in the data): record\n",
    "\n",
    "    Returns:\n",
    "        datetime object: extract date - relative age = actual post date\n",
    "    \"\"\"\n",
    "\n",
    "    salary = row['salaries']\n",
    "    if salary < q1:\n",
    "        row['Q1'] = 1\n",
    "    else:\n",
    "        row['Q1'] = 0\n",
    "    if (salary >= q1) & (salary < q2):\n",
    "        row['Q2'] = 1\n",
    "    else:\n",
    "        row['Q2'] = 0\n",
    "    if (salary >= q2) & (salary < q3):\n",
    "        row['Q3'] = 1\n",
    "    else:\n",
    "        row['Q3'] = 0\n",
    "    if salary >= q3:\n",
    "        row['Q4'] = 1\n",
    "    else:\n",
    "        row['Q4'] = 0\n",
    "    return row\n",
    "\n",
    "\n",
    "data['Q1'] = ''\n",
    "data['Q2'] = ''\n",
    "data['Q3'] = ''\n",
    "data['Q4'] = ''\n",
    "data = data.apply( lambda row : Q(row), axis = 1)\n",
    "data.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['company','job_title','text']] = data[['company','job_title','text']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"target\", y=\"salary\", kind=\"violin\", inner=None, data=data)\n",
    "sns.swarmplot(x=\"target\", y=\"salary\", color=\"k\", size=2, data=data, ax=g.ax)\n",
    "\n",
    "plt.xticks(rotation=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Associations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.drop('salary', axis=1)\n",
    "data2.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'target'\n",
    "\n",
    "associated = []\n",
    "\n",
    "for i in data2.select_dtypes('number').columns:\n",
    "    if i == target:\n",
    "        continue\n",
    "    else:\n",
    "        pearson_cor, pval = pearsonr(data2[i],data2[target])\n",
    "\n",
    "    if pearson_cor > .3:\n",
    "        associated.append([i,pearson_cor])\n",
    "\n",
    "# Extracting column names of the dependent variables that passed the pearson correlation test of being at least .3\n",
    "num_cols = []\n",
    "for i in associated:\n",
    "    num_cols.append(i[0])\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO complete this by collecting the most significant colinear features and possibl removing them.\n",
    "\n",
    "score_dictionary = {}\n",
    "for i in data.select_dtypes('object').columns:\n",
    "    feature = pd.crosstab(data[i], data[target])\n",
    "    # calculate the chi squared statistic and save it as chi2, then print it:\n",
    "    chi2, pval, dof, expected = chi2_contingency(feature)\n",
    "    score_dictionary[i] = round(chi2,2)\n",
    "score_df = pd.DataFrame(score_dictionary.items(), columns=['feature','chi2 score']).sort_values('chi2 score', ascending=False)\n",
    "score_df[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['company','job_title','text']] = data[['company','job_title','text']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['company','job_title','state','city','rating','text', 'Q1', 'Q2', 'Q3', 'Q4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_pipe():\n",
    "    targets = ['Q1','Q2','Q3','Q4']\n",
    "    X = data.drop(targets, axis=1)\n",
    "\n",
    "    le_cols = ['company', 'job_title', 'state', 'city']\n",
    "    scal_cols = ['rating']\n",
    "\n",
    "    evaluations = {}\n",
    "\n",
    "    \n",
    "    for i in targets:\n",
    "    # test/train split\n",
    "        y = data[i]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y , test_size=.2, random_state=42)\n",
    "\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('text', TfidfVectorizer(ngram_range=(1,3), analyzer = 'word',max_features=1000,stop_words='english',decode_error='ignore'), 'text'),\n",
    "                ('category', OneHotEncoder(handle_unknown ='ignore'), le_cols),\n",
    "                ('scaler', StandardScaler(), scal_cols)\n",
    "                \n",
    "                ],remainder='drop')\n",
    "        #   populating parameter grid to search\n",
    "        grid = [\n",
    "            {\n",
    "            'classifier' : [LogisticRegression()],\n",
    "            #'classifier__penalty' : ['l1', 'l2'],\n",
    "            #'classifier__C' : np.logspace(-4, 4, 20),\n",
    "            'classifier__solver' : ['liblinear']}\n",
    "            ]\n",
    "\n",
    "        pipe = Pipeline(\n",
    "            steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', LogisticRegression(grid)),\n",
    "            ],\n",
    "            )\n",
    "\n",
    "\n",
    "        \n",
    "        grid_search = GridSearchCV(pipe, param_grid=grid, verbose=2, return_train_score=True)\n",
    "        grid_search.fit(x_train,y_train)\n",
    "\n",
    "        print(\"Best parameter (CV score=%0.3f):\" % grid_search.best_score_)\n",
    "        print(grid_search.best_params_)\n",
    "\n",
    "\n",
    "        if i not in evaluations.keys():\n",
    "            evaluations[i] = {\n",
    "                'grid_search':grid_search,\n",
    "                'x_train':x_train,\n",
    "                'y_train':y_train,\n",
    "                'x_test':x_test,\n",
    "                'y_test':y_test\n",
    "                \n",
    "            }\n",
    "\n",
    "\n",
    "    return evaluations\n",
    "\n",
    "\n",
    "evaluations = grid_search_pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(evaluations):\n",
    "\n",
    "\n",
    "    targets = ['Q1','Q2','Q3','Q4']\n",
    "    X = data.drop(targets, axis=1)\n",
    "\n",
    "    test_results = {}\n",
    "\n",
    "    for i in targets:\n",
    "        evaluation = evaluations[i]['grid_search']\n",
    "        params = evaluation.best_params_\n",
    "\n",
    "\n",
    "        y = data[i]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y , test_size=.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "        le_cols = ['company', 'job_title', 'state', 'city']\n",
    "        scal_cols = ['rating']\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('text', TfidfVectorizer(ngram_range=(1,3), analyzer = 'word',max_features=1000,stop_words='english',decode_error='ignore'), 'text'),\n",
    "                ('category', OneHotEncoder(handle_unknown ='ignore'), le_cols),\n",
    "                ('scaler', MinMaxScaler(), scal_cols)\n",
    "                \n",
    "                ],remainder='drop')\n",
    "\n",
    "\n",
    "        pipe = Pipeline(\n",
    "            steps=[\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('classifier', params['classifier']),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "\n",
    "        pipe.fit(x_train,y_train)\n",
    "\n",
    "        predictions = pipe.predict(x_test)\n",
    "        score = pipe.score(x_train, y_train)\n",
    "\n",
    "\n",
    "        ### check\n",
    "        check = pd.DataFrame(preprocessor.fit_transform(x_train,y_train).toarray())\n",
    "\n",
    "\n",
    "        if i not in test_results.keys():\n",
    "            test_results[i] = pipe, x_train, y_train, x_test, y_test, predictions, score\n",
    "                \n",
    "    return test_results, check\n",
    "        \n",
    "test_results, check = make_model(evaluations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinator(test_results):\n",
    "    targets = ['Q1','Q2','Q3','Q4']\n",
    "\n",
    "    resultsDB = {}\n",
    "    lst = []\n",
    "\n",
    "    \n",
    "    for label in targets:\n",
    "        data_dict = {}\n",
    "        pipe = test_results[label][0]\n",
    "        x_test = test_results[label][3]\n",
    "        y_test = test_results[label][4]\n",
    "        y_prob = pipe.predict_proba(x_test)\n",
    "        predict_y = pipe.predict(x_test)\n",
    "        data_dict = {\n",
    "            f'{label}_y_test':y_test, \n",
    "             f'{label}_pred y': predict_y.tolist(),\n",
    "             f'{label}_probabilities': y_prob.tolist()\n",
    "             }\n",
    "        zulu = x_test.join(pd.DataFrame(data_dict))\n",
    "        if label not in resultsDB.keys():\n",
    "            resultsDB[label] = zulu\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "        out = pd.DataFrame(data_dict)\n",
    "        lst.append(out)\n",
    "\n",
    "    results_table = x_test\n",
    "    for i, label in zip(lst, targets):\n",
    "        results_table = results_table.join(i, lsuffix=label)\n",
    "    #results_table = results_table.join(lst[1], lsuffix='__')\n",
    "    #for label, results in zip(targets,lst):\n",
    "    #    results_table = results_table.join(results, lsuffix=label)\n",
    "    return resultsDB, results_table, lst\n",
    "   \n",
    "\n",
    "data_dict, results_table, lst = combinator(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_results['Q1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = test_results['Q1'][0]\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = test_results['Q1'][1]\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = test_results['Q1'][2]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_results['Q1'][3]\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_results['Q1'][4]\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  test_results['Q1'][5]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score =  test_results['Q1'][6]\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = pipe.score(x_train, y_train)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = pipe.predict(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy :\",accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Hamming loss \",hamming_loss(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMicro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMacro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClassification Report\")\n",
    "print (classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf1 = confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = ['Q1']\n",
    "te_confusions = [cf1]\n",
    "y_t = [y_test]\n",
    "te_probs = [pipe.predict_proba(x_test)]\n",
    "y_pred = [predictions]\n",
    "y_test_preds = predictions\n",
    "y_prob = [pipe.predict_proba(x_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = pipe.predict_proba(x_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([-.01, 1.])\n",
    "plt.ylim([-.01, 1.05])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, cf, y_t, y_pred, y_prob in zip(Q, te_confusions, y_test, y_test_preds, te_probs):\n",
    "    make_confusion_matrix(cf, title='\\n'+title+' Confusion Matrix\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q(row):\n",
    "    \"\"\"Subtracts the distance (in days) provided by the postD function.\n",
    "    Args:\n",
    "        row (record in the data): record\n",
    "\n",
    "    Returns:\n",
    "        datetime object: extract date - relative age = actual post date\n",
    "    \"\"\"\n",
    "    #np.argmax(row[['Q1_Prob', 'Q2_Prob','Q3_Prob','Q4_Prob']].values)\n",
    "    Q1 = row['Q1_probabilities'][1]\n",
    "    Q2 = row['Q2_probabilities'][1]\n",
    "    Q3 = row['Q3_probabilities'][1]\n",
    "    Q4 = row['Q4_probabilities'][1]\n",
    "    lst = [Q1,Q2,Q3,Q4]\n",
    "    # adding 1 so these correspond to the target names rather than 0 indexed.\n",
    "    return int(lst.index(max(lst))+1)\n",
    "    #next, check if that \n",
    "\n",
    "\n",
    "results_table['Q'] = results_table.apply( lambda row : Q(row), axis = 1)\n",
    "results_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "#results_table = results_table.join(pd.DataFrame(ohe.fit_transform(results_table[['Q']]), columns=['Q1','Q2','Q3','Q4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = ohe.fit_transform(results_table[['Q']]).T\n",
    "\n",
    "results_table['Q1'] = ohe[0]\n",
    "results_table['Q2'] = ohe[1]\n",
    "results_table['Q3'] = ohe[2]\n",
    "results_table['Q4'] = ohe[3]\n",
    "\n",
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table.to_csv(f'../app/data/tableau_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table=results_table[results_table.salary.notna()]\n",
    "results_table[results_table.Q != results_table.target]\n",
    "\n",
    "results_table.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = np.quantile(results_table.salary, 0.25)\n",
    "q2 = np.quantile(results_table.salary, 0.5)\n",
    "q3 = np.quantile(results_table.salary, 0.75)\n",
    "\n",
    "# calc iqr\n",
    "iqr = (q3 - q1)\n",
    "# expand iqr to discern outliers\n",
    "iqr_x = iqr*1.5\n",
    "\n",
    "# setting the lower and upper limits\n",
    "iqr_lower = q1-iqr_x\n",
    "iqr_upper = q3+iqr_x\n",
    "\n",
    "\n",
    "sns.displot(results_table.salary)\n",
    "plt.axvline(x=q1, label=\"Q1\", c = 'g')\n",
    "plt.axvline(x=q2, label=\"Q2\", c = '#fd4d3f')\n",
    "plt.axvline(x=q3, label=\"Q3\", c = 'r')\n",
    "\n",
    "plt.axvline(x=iqr_lower, label = 'IQR Lower', c = 'black')\n",
    "plt.axvline(x=iqr_upper, label = 'IQR Upper', c = 'black')\n",
    "plt.xticks(rotation=30)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table.Q.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"Q\", y=\"salary\", kind=\"violin\", inner=None, data=results_table)\n",
    "sns.swarmplot(x=\"Q\", y=\"salary\", color=\"k\", size=2, data=results_table, ax=g.ax)\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_tfidf = tfidf.fit_transform(_test).toarray()\n",
    "vocab = q1_vectorizer.vocabulary_\n",
    "reverse_vocab = {v:k for k,v in vocab.items()}\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "df_tfidf = pd.DataFrame(X_tfidf, columns = feature_names)\n",
    "\n",
    "idx = X_tfidf.argsort(axis=1)\n",
    "\n",
    "tfidf_max10 = idx[:,-10:]\n",
    "\n",
    "df_tfidf['top10'] = [[reverse_vocab.get(item) for item in row] for row in tfidf_max10 ]\n",
    "\n",
    "df_tfidf['top10']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = sum(data.salaries.isnull())\n",
    "nnull = sum(data.salaries.notnull())\n",
    "print('')\n",
    "print(f'The data contains {data.shape[0]} rows of individual job postings with values for {data.shape[1]} columns based on\\n different sections of each post\\'s web page.')\n",
    "print('')\n",
    "print(f'- Out of {len(data)} job postings {nnull} or {round(nnull/len(data)*100,2)}% include some sort of salary information,\\n- The remaining {null} rowws, or {round(null/len(data)*100,2)}% are missing salary data.')\n",
    "#\\n- Also, there are no null values in the rest of the table meaning we won\\'t need\\n  to lose any data by dropping rows. While the null values in the Pay column will\\n  simply be dropped after we translate the not nulls into the target.')\n",
    "\n",
    "\n",
    "data[data['salaries'].notna()].salaries\n",
    "print(missing(data, feature = 'salaries'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'job_type_items'\\\n",
    "    I'm going to treat these as likert values, converrting each of these string formatted lists into an ordinal values for a new column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing(data, feature = 'job_type_items'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "TODO\n",
    "\n",
    "Fri - Sat - Sun\n",
    "- make sure shit has zeros\n",
    "- check if there is missing salary data from dropped columns\n",
    "------ clean: company, description, location, summary, job_title\n",
    "------ split location\n",
    "------ join main text\n",
    "- prepare main text for nlp\n",
    "\n",
    "- drop cols and do eda\n",
    "\n",
    "- gridsearch\n",
    "- model\n",
    "- evaluation\n",
    "- collate all data\n",
    "- find class characteristics, i.e. top employers/locations/words-phrases\n",
    "\n",
    "- Code review and organizing\n",
    "- readme\n",
    "\n",
    "\n",
    "Monday\n",
    "- plots\n",
    "- review kmeans\n",
    "- review indeed\n",
    "- review the best 2-3 other projects\n",
    "- categorical features\n",
    "\n",
    "\n",
    "Tuesday\n",
    "- review remaining projects\n",
    "- categorical features (wrap up)\n",
    "- numeric features\n",
    "- 2hrs linear alg\n",
    "- 2hrs stats\n",
    "- 2hrs proba\n",
    "\n",
    "\n",
    "Wednesday\n",
    "- review all projects, particularly text/readmes\n",
    "- review resume\n",
    "- 2hrs Git: commits and prettify\n",
    "- LinkedIn\n",
    "- Job Sites\n",
    "- 1hr linear alg\n",
    "- 1hr stats\n",
    "- 1hr proba\n",
    "- look at jobs/need for CVs\n",
    "\n",
    "\n",
    "Thurs\n",
    "- 2hr SQL\n",
    "- 1hr Numpy\n",
    "- 1hr Pandas\n",
    "- 1hr SciPyStats\n",
    "- 2hr quiz\n",
    "- Apply\n",
    "\n",
    "\n",
    "Days\n",
    "- 1hr SQL\n",
    "- 1hr Quiz and Practice Questions\n",
    "- 1hr Projects\n",
    "- 1hr Reading/Watching Knoweledge\n",
    "\n",
    "\n",
    "Off\n",
    "- 2hr SQL\n",
    "- 2hr Apply\n",
    "- 3hr Projects\n",
    "- 1hr Reading/Watching Knoweledge\n",
    "- 2hr Quiz and Practice Questions\n",
    "- 1hr linear alg or stats or proba\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def jobtype(data):\n",
    "    \"\"\"Used for creating a new feature containing nominal values for the job type.\n",
    "\n",
    "    Args:\n",
    "        data (string): Values from the 'job_type_items' column.\n",
    "\n",
    "    Returns:\n",
    "        string: Extracted and simplified values for each category.\n",
    "    \"\"\"\n",
    "    if type(data) == str:\n",
    "        if re.search('full', data):\n",
    "            out = 'full'\n",
    "        if re.search('part', data):\n",
    "            out = 'part'\n",
    "        if re.search('contract', data):\n",
    "            out = 'contract'\n",
    "        if re.search('temp', data):\n",
    "            out = 'temp'\n",
    "        return out\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "cols = ['company','description', 'job_type_items','location','postdate','requirements',\t'summary','job_title']\n",
    "for txt_col in cols:\n",
    "    data[txt_col] = data[txt_col].str.lower()\n",
    "\n",
    "\n",
    "\n",
    "data['jobtype'] = data.job_type_items.apply(jobtype)\n",
    "\n",
    "data['text'] = data.description+' '+data.summary\n",
    "#TODO add requirements\n",
    "\n",
    "cols = ['job_type_items','raw_desc_soup','url','description','summary','requirements']\n",
    "\n",
    "\n",
    "data.text = data.text.replace(')',' ')\n",
    "\n",
    "\n",
    "\n",
    "data = annual(data)\n",
    "#data.schedule.value_counts() keep and eye on this, may have to deal with more than just hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'company'\\\n",
    "There's presently 96 different companies - that's a ton of dimensions added to the data, and I've still got to add count vectorizer. I'll try label encoding these. In the future, shifting these into clusters might be useful.\\\n",
    "Other than that, this variable doesn't seem to need much - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.company.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'location'\\\n",
    "This feature holds a wealth of information: state, city/remote, and in some cases a zip code.\n",
    "Zip codes seem to be a recent addition to the information I can get from Indeed.com but since its presence is rather sparse I'll just extract city/remote and state values, splitting them into their respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.job_type_items.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'job_title'\\\n",
    "Although the webscrapper searches for 'data scientist' the results contain over 80 unique job titles. This is largely due to a prefix or suffix that provides the role's specialization at that particular company for that particular job posting. These include items like 'Marketing', 'Senior', 'Geospatial...'.\n",
    "I'll start by cleaning and simplifying these values, i.e. appearences of 'Sr' can be converted to 'Senior'. This will significantly reduce the number of unique values, hopefully increasing model accuracy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data.job_title.unique()))\n",
    "data.job_title.unique()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'requirements'\\\n",
    "I will most likely merge this in with the description prior to conducting NLP. The rationale is that these explicit requirements may be echoed in the description or summary, so by adding them to the corpus I'll raise the scores derived from their increaseed frequency. Eventually I wiuld like to build a set of requirements that I can use in with a clustering algorithm, like KNN, to find/define subgroups within the sample population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.requirements[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'summary'\\\n",
    "Much like the description, these are unique. COming from the search results page itself, these are brief descriptions of each job. In most cases this is not merely an abridgement of the summary. Like 'requirement' I'll start by lumping these texts in with description so analysis of the corpus can be boosted by the added descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.summary[:3]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'description'\\\n",
    "Each value is a unique string of the full job description. I'll lower and remove special characters but this feature will be dealt with seperately in its own NLP process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'raw_desc_soup'\\\n",
    "I'm only saving this in case I refine or change my parsing. Below is a truncated slice of one of the raw records.\n",
    "\n",
    "'url'\\\n",
    "Like raw_doc_soup, I'm simply keeping these in case I need to go back and re-parse the raw data. In fact, this URL points directly to the page where that raw doc soup is found.\n",
    "I may also be able to detect job post updates and duplicates by using these unique addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.raw_desc_soup[1][:1000]\n",
    "data.url[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Wrangling & Feature Engineering\n",
    "Below are the implementations of the data wrangling steps I described above for each feature. Using Numpy, Regex, and Pandas I'll simutaneaously clean the strings, extract and convert numeric information, and create new feature columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the data looks like at this point. It looks like the 17 original features have been reduced to 11. Most of the columns have been collapsed and cleaned, too. Just a bit more work till this is ready for EDA, NLP, and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data.rating = data.rating.fillna(0)\n",
    "\n",
    "cols = ['extractdate','postdate','schedule','jobtype','salary']\n",
    "data.drop(cols,inplace=True,axis=1)\n",
    "data = data.rename({'annual_sal': 'salary'}, axis=1)\n",
    "\n",
    "data['salary'] = data['salary'].apply(sal_fixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns\n",
    "cols = ['salary','estimated_salary', 'job_type_items','raw_desc_soup', 'sal_guide_items','salary_and_jtype', 'salfromsection']\n",
    "data.drop(cols,inplace=True,axis=1)\n",
    "data.rename(columns={'final_sal': 'salary'},inplace=True)\n",
    "\n",
    "data = data[data.description != 'NaN']\n",
    "\n",
    "len(data)\n",
    "\n",
    "data.description.value_counts()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(memory_usage='deep')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = data.shape\n",
    "print(f\"The {shape[0]} rows of data now have {shape[1]} variables whose values are now cleaned and typed properly.\\nThe only missing values are in Salary which is my target.\\nNext, I'll conduct analysis, encoding, and scaling of each feature, starting with Salary which .\")\n",
    "data.describe(include = 'all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='salary', data=data.notnull(), palette='Set3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = len(data[data.salary.isnull()])\n",
    "nnull = len(data[data.salary.notnull()])\n",
    "print(f'{round(nnull/len(data)*100,2)}% of the data has salary information, should be an easy target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(f'../app/data/wrangled_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged = data\n",
    "munged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = len(munged[munged.salary.isnull()])\n",
    "nnull = len(munged[munged.salary.notnull()])\n",
    "print(f'Droping {len(data)-len(munged)} duplicates and out of date posts leaves {len(munged)} rows of data for the regressor to be trained and tested on.')\n",
    "print(f'- Of those {len(munged)} job postings {nnull} or {round(nnull/len(munged)*100,2)}% include salary information,\\n- The remaining {null} rows, or {round(null/len(munged)*100,2)}% are missing salary data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy.stats import pearsonr, chi2_contingency\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "plt.rcParams['font.size'] = 17\n",
    "\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target\n",
    "\n",
    "### Numeric Features\n",
    "\n",
    "#### 'dateposted'\n",
    "This isn't going into the pipelie, despite some interesting patterns, I don't think there's enough periodic/seasonality information available here. Still, great for visualizing the job market.\n",
    "### Categorical Features\n",
    "\n",
    "#### 'job_title'\n",
    "'job_title'\\\n",
    "Oddly enough, these number as many as company, I suspect multicolinearity. I'll come back to collapse these a bit and perhaps build a new feature for specializations out of these.\n",
    "To be label encoded and possibly clustered later.\n",
    "\n",
    "### Feature Associations\n",
    "#### Feature Selection and Hyperparameter Tuning\n",
    "### Pipeline GridSearch\n",
    "## Modelling\n",
    "## Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Outliers: Using both the Z-Score and IQR methods\n",
    "    Now that preprocessed the data and building the target variable of Salary let's see conduct a bit more EDA to see if there are any outliers that could potentially skew how the regressor will learn from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pairplot(data=munged[munged.salary.notnull()],\n",
    "                  x_vars=['dateposted'],\n",
    "                  y_vars=['salary'], height=8, hue=\"salary\", palette=\"crest\")\n",
    "\n",
    "ax.fig.set_size_inches(18,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot is helpful in isualizing data, but let's program a few functions to detect and remove outliers based on thresholds.\n",
    "With the Z-Score, we can determine any data outside 3 standard deviations from the mean of our salary data to be an outlier.\n",
    "Wheras with the interquartile (IQR) range, the middle 50% of given salaries, I'll set the conventional cutoff of 1.5+/- the IQR as the cutoff.\n",
    "Values found outside of either range will be collected into a list and that'll allow for some fancy indexing so those particular postings can be examined and removed programatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z-Score\n",
    "Salaries 3 standard deviations away from the mean will be listed as outliers.\n",
    "Here we can see that there are five records with salary values outside the Z-score threshold.\n",
    "\n",
    "To explore the possibility of outliers I'll start with a box and whisker plot. Those whisters reach 1.5 times past the IQR boundary on either side, and are generally provide a good means of identifying outliers - values beyond the boundaries.\n",
    "\n",
    "\n",
    "A plot is helpful in isualizing data, but let's program a few functions to detect and remove outliers based on thresholds.\n",
    "With the Z-Score, we can determine any data outside 3 standard deviations from the mean of our salary data to be an outlier.\n",
    "Wheras with the interquartile (IQR) range, the middle 50% of given salaries, I'll set the conventional cutoff of 1.5+/- the IQR as the cutoff.\n",
    "Values found outside of either range will be collected into a list and that'll allow for some fancy indexing so those particular postings can be examined and removed programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = []\n",
    "def z_detect(munged):\n",
    "    sample = munged[munged['salary'].notnull()].salary\n",
    "    threshold=3\n",
    "    mean = np.mean(sample)\n",
    "    std = np.std(sample)\n",
    "    \n",
    "    for i in sample:\n",
    "        z_score = (i-mean)/std\n",
    "        if np.abs(z_score) > threshold:\n",
    "            outliers.append(i)\n",
    "    if len(outliers) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        return outliers\n",
    "z_detect(munged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### IQR\n",
    "Salaries outside 1.5 times the interquartile range boundaries, either above or below will be listed as outliers.\\\n",
    "Here, I clearly demarcate the quartiles as well as the uppet and lower boundaries. Before removing these outliers let's take a look.\n",
    "\n",
    "There aren't any outliers beyond the lower bound for either test so let's take a look at what's above the IQR threshold.\\\n",
    "While not guarenteed, its a good thing that both tests seem to identify the same five records, the salary values match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trimming the data, while showing the number of records were starting with and how many remain after removing the outliers viewed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "data1 = data[data.salary <= iqr_upper]\n",
    "data2 = data[data.salary.isnull()]\n",
    "data = pd.concat([data1,data2])\n",
    "print(len(data))\n",
    "eda_data = data[data[col]>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_detect(munged):\n",
    "    sample = munged[munged['salary'].notnull()].salary\n",
    "    Q1, Q3 = np.percentile(sample,[25,75])\n",
    "    iqr = Q3-Q1\n",
    "    lower_bound = Q1-(1.5*iqr)\n",
    "    upper_bound = Q3+(1.5*iqr)\n",
    "    for i in sample:\n",
    "        if (i < lower_bound)  | (i > upper_bound):\n",
    "            outliers.append(i)\n",
    "    if len(outliers) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        return outliers\n",
    "set(iqr_detect(munged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1):\n",
    "    list_set = set(list1)\n",
    "    # convert the set to the list\n",
    "    unique_list = (list(list_set))\n",
    "    for x in unique_list:\n",
    "        print(x)\n",
    "        \n",
    "mn = min(outliers)\n",
    "mx = max(outliers)\n",
    "print(f'The {len(set(outliers))} Unique Outliers Are:')\n",
    "unique(outliers)\n",
    "print(f'- With a minumum of ${round(mn)} and a maximum of ${round(mx)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at those tables.\n",
    "\n",
    "outliers = munged[munged['salary'].isin(outliers)]\n",
    "munged.drop(outliers.index, axis=0,inplace=True)\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = len(munged[munged.salary.isnull()])\n",
    "\n",
    "nnull = len(munged[munged.salary.notnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pairplot(data=munged[munged.salary.notnull()],\n",
    "                  x_vars=['dateposted'],\n",
    "                  y_vars=['salary'], height=8, hue=\"salary\", palette=\"crest\")\n",
    "\n",
    "ax.fig.set_size_inches(18,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='salary', data=munged.notnull(), palette='Set3')\n",
    "print(f'- Dropping {len(outliers)} outliers now leaves {nnull}, or {round(nnull/len(munged)*100,2)}%, of rows with with salary information,\\n- The remaining {null} rows, or {round(null/len(munged)*100,2)}%, are missing salary data.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.to_csv(f'../app/data/wrangled_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.drop('location',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pairplot(data=data[data.salary.notnull()],\n",
    "                  x_vars=['dateposted'],\n",
    "                  y_vars=['salary'], height=10, hue=\"salary\", palette=\"crest\")\n",
    "\n",
    "ax.fig.set_size_inches(18,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='salary', data=data.notnull(), palette='Set3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = len(data[data.salary.isnull()])\n",
    "nnull = len(data[data.salary.notnull()])\n",
    "print(f'{round(nnull/len(data)*100,2)}% of the data has salary information, should be an easy target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data,x='salary',bins=100)\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=\"salary\", data=data)\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"dateposted\", y=\"salary\", hue=\"target\", data=data)\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pairplot(data=data,\n",
    "                  x_vars=['dateposted'],\n",
    "                  y_vars=['salary'], height=8, hue=\"target\", palette=\"crest\")\n",
    "\n",
    "ax.fig.set_size_inches(18,4)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac83f05b37eb5c5c49ba67e50f0047ddfbc4b30205fc79ee5f327b9c0ac37f55"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('indeedapp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "46216370387cf43888a1dc9433c5a4546bda9feed2ad4f35c2a851da9960dc54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
