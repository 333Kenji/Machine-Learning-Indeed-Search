{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "import re\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from to_img import to_img\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 17\n",
    "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"ticks\", color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data\n",
    "\n",
    "The data consists of text scraped from every search result for 'data science/scientist' on Indeed.com using the [Requests](https://docs.python-requests.org/en/master/ \"Requests Library\"), Tor, and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/ \"BeautifulSoup\") libraries.\n",
    "\n",
    "### 1.1 Load and Inspect Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# note the change in memory usage.\n",
    "data = pd.read_csv('../app/data/total.csv')\n",
    "print(len(data))\n",
    "data.info(memory_usage='deep')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table is a view of the raw data from the scrape. I'll build the target by reducing the pay column to floating point values; this will require text cleaning which will also be conducted for the rest of the table. Location will be split into new columns for City and State while and because Indeed's search results show dates relative to the date the query was made I'll need to use PostDate and ExtractDate build a DatePosted column. It is important to note that JobUrl and date-related columns will not be converted into features for the logistic regression, instead they will be reattached to the data at the end in order to provide additional information and insights through the EC2 application. The only rows that'l  be using for modelling are JobTitle, Company, Summary, Requirements, and Description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = map(str.lower, data.columns)\n",
    "data = data.rename({'title': 'job_title'}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Dictionary\n",
    "- company<br>\n",
    "&nbsp;\n",
    "&nbsp;name of company\n",
    "- description<br>\n",
    "&nbsp;full text description of job\n",
    "- estimated_salary<br>\n",
    "&nbsp;Indeed's salary estimation\n",
    "- extractDate<br>\n",
    "&nbsp;String(?), date record was webscrapped\n",
    "- job_type_items<br>\n",
    "&nbsp;formatted list of 'full-time', 'part-time', etc\n",
    "- location<br>\n",
    "&nbsp;state and city/remote\n",
    "- postDate<br>\n",
    "&nbsp;date job posting was created\n",
    "- rating<br>\n",
    "&nbsp;Indeed.com ascribed company ratings\n",
    "- raw_desc_soup<br>\n",
    "&nbsp;raw full description, retained for possible future use/analysis\n",
    "- requirements<br>\n",
    "&nbsp;employer-listed educational, aptitude, and experience requirements\n",
    "- sal_guide_items<br>\n",
    "&nbsp;formatted list containing redundant characters and the Indeed salary estimate\n",
    "- salary<br>\n",
    "&nbsp;if given by employer\n",
    "- salary_and_jType<br>\n",
    "&nbsp;formatted list combining salary from salfromsection and job_type_items\n",
    "- salfromsection<br>\n",
    "&nbsp;salary, if given, extracted from job description\n",
    "- summary<br>\n",
    "&nbsp;brief summaries of each job extracted from sach results\n",
    "- title<br>\n",
    "&nbsp;job position title\n",
    "- url<br>\n",
    "&nbsp;job posting/description address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplcates.\n",
    "print(f'Number of dupes: {sum(data.duplicated())}')\n",
    "data[data.duplicated() == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of seeing duplicates coming from the raw scrapping is very unlikely, but this is still a good check to do in case something changes with Indeed.com's front-end code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values are another story. It seems that Indeed.com doesn't require employers to provide values for every feature, this certainly seems to be the case with requirements, not to mention everywhere salary information coud be held.\\\n",
    "Others, like estimated salary and ratings are generated by Indeed.com but don't exist for every posting.\\\n",
    "job_type_items and salary_and_jType will likely be collapsed into a new column for job types, I'll also do the same with all of the salary related features to build the model's target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Diagnose Data\n",
    "In order to proceed with any statistical or exploratory data analysis I will need to wrangle the data quite a bit.\n",
    "Here, I'll list each of the current feature columns and briefly describe what I'll need to do with them.\n",
    "\n",
    "Column Groups and Needs\n",
    "Simple\n",
    "Salary\n",
    "Text\n",
    "etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing\n",
    "print(data.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"The {data.shape[0]} rows of data have {data.shape[1]} features. Since this is a classification problem I'll be selecting one to be the target, leaving me with {data.shape[1]-1} dependent variables to work with.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2a Rating\n",
    "\n",
    "As the only feature coming in with numeric (float) values I should just need to scale it. I'll address missing values by imputing zero. The rationale behind this is that an unrated company should simply be rated as zero.\n",
    "print(sorted(data.rating.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.rating.isnull()].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = len(data)\n",
    "missing_len = len(data[data.rating.isnull()])\n",
    "print(str(round(data_len/missing_len,1))+\"%\")\n",
    "\n",
    "notna_ratings = data[data.rating.notna()]\n",
    "print(notna_ratings.rating.nunique())\n",
    "print(np.mean(notna_ratings.rating))\n",
    "sorted(list(notna_ratings.rating.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data[data.rating.isna()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['rating'].isna()]\n",
    "print(data_len-len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1127-21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = sns.distplot(notna_ratings.rating, fit=norm, bins=notna_ratings.rating.nunique(), kde=True)\n",
    "plt.title('raw')\n",
    "#ax.set_xticks(notna_ratings.rating.unique())\n",
    "plt.axvline(x=np.mean(notna_ratings.rating), label='mean', c = 'r')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = []\n",
    "def z_detect(data):\n",
    "    sample = notna_ratings.rating\n",
    "    threshold=3\n",
    "    mean = np.mean(sample)\n",
    "    std = np.std(sample)\n",
    "    \n",
    "    for i in sample:\n",
    "        z_score = (i-mean)/std\n",
    "        if np.abs(z_score) > threshold:\n",
    "            outliers.append(i)\n",
    "    if len(outliers) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        return outliers\n",
    "z_detect(notna_ratings.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = np.quantile(notna_ratings.rating, 0.25)\n",
    "q2 = np.quantile(notna_ratings.rating, 0.5)\n",
    "q3 = np.quantile(notna_ratings.rating, 0.75)\n",
    "\n",
    "# calc iqr\n",
    "iqr = (q3 - q1)\n",
    "# expand iqr to discern outliers\n",
    "iqr_x = iqr*1.5\n",
    "\n",
    "# setting the lower and upper limits\n",
    "iqr_lower = q1-iqr_x\n",
    "iqr_upper = q3+iqr_x\n",
    "\n",
    "\n",
    "sns.displot(notna_ratings.rating)\n",
    "plt.axvline(x=q1, label=\"Q1\", c = 'g')\n",
    "plt.axvline(x=q2, label=\"Q2\", c = 'b')\n",
    "plt.axvline(x=q3, label=\"Q3\", c = 'r')\n",
    "\n",
    "plt.axvline(x=iqr_lower, label = 'IQR Lower', c = 'black')\n",
    "plt.axvline(x=iqr_upper, label = 'IQR Upper', c = 'black')\n",
    "plt.xticks(rotation=30)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data[data.rating > iqr_upper]))\n",
    "print(len(data[data.rating < iqr_lower]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rating.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(data))\n",
    "data = data[data.rating <= iqr_upper]\n",
    "data = data[data.rating >= iqr_lower]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = np.quantile(data.rating, 0.25)\n",
    "q2 = np.quantile(data.rating, 0.5)\n",
    "q3 = np.quantile(data.rating, 0.75)\n",
    "\n",
    "# calc iqr\n",
    "iqr = (q3 - q1)\n",
    "# expand iqr to discern outliers\n",
    "iqr_x = iqr*1.5\n",
    "\n",
    "# setting the lower and upper limits\n",
    "iqr_lower = q1-iqr_x\n",
    "iqr_upper = q3+iqr_x\n",
    "\n",
    "\n",
    "sns.displot(data.rating)\n",
    "plt.axvline(x=q1, label=\"Q1\", c = 'g')\n",
    "plt.axvline(x=q2, label=\"Q2\", c = 'b')\n",
    "plt.axvline(x=q3, label=\"Q3\", c = 'r')\n",
    "\n",
    "plt.axvline(x=iqr_lower, label = 'IQR Lower', c = 'black')\n",
    "plt.axvline(x=iqr_upper, label = 'IQR Upper', c = 'black')\n",
    "plt.xticks(rotation=30)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scalar = StandardScaler()\n",
    "scaled_ratings = pd.DataFrame(scalar.fit_transform(pd.DataFrame(notna_ratings.rating)), columns=['rating'])\n",
    "\n",
    "scalar = StandardScaler()\n",
    "scaled_ratings = pd.DataFrame(scalar.fit_transform(pd.DataFrame(notna_ratings.rating)), columns=['rating'])\n",
    "ax = sns.distplot(scaled_ratings, fit=norm, bins=notna_ratings.rating.nunique(), kde=True)\n",
    "plt.title('scaled')\n",
    "#ax.set_xticks(notna_ratings.rating.unique())\n",
    "plt.axvline(x=np.mean(scaled_ratings.rating), label='mean', c = 'r')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2a Dates\n",
    "\n",
    "'extractdate' & 'postdate'\\\n",
    "extract_date is a string showing the day the record was webscrapped.\n",
    "post_date is a string description of the relative age of each post from the date it was posted to the day it was webscrapped.\n",
    "Converting post_date to a number will give me the the posts ae, which I'll then subtract from it's extract date in order to build a new feature carrying values for the DATE each job posting has been submitted to Indeed.com.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.postdate.unique()[:5]\n",
    "data.extractdate.unique()[:5]\n",
    "\n",
    "def postD(data):\n",
    "    \"\"\"Critical function that converts postdate values into relative (in days) distance from the extract date.\n",
    "\n",
    "    Args:\n",
    "        data (string): text descrption of how old the post is.\n",
    "\n",
    "    Returns:\n",
    "        npnan or int: numeric representation of post age\n",
    "    \"\"\"\n",
    "    if data != np.nan:\n",
    "        rid = ['active','%+ days ago','+']\n",
    "        for i in rid:\n",
    "            if i in data:\n",
    "                return np.nan\n",
    "        \n",
    "        rid2 = ['just posted','today','postedjust posted','postedtoday','hiring ongoing']\n",
    "        for i in rid2:\n",
    "            if i in data:\n",
    "                return 0   \n",
    "            \n",
    "        rid3 = ['posted 1 day ago']\n",
    "        for i in rid3:\n",
    "            if i in data:\n",
    "                return 1\n",
    "        \n",
    "        rid3 = ['posted1 day ago']\n",
    "        for i in rid3:\n",
    "            if i in data:\n",
    "                return 1\n",
    "\n",
    "\n",
    "    #data['dateposted'] = data['dateposted'].astype('int')\n",
    "    return data\n",
    "\n",
    "def pDate(row):\n",
    "    \"\"\"Subtracts the distance (in days) provided by the postD function.\n",
    "    Args:\n",
    "        row (record in the data): record\n",
    "\n",
    "    Returns:\n",
    "        datetime object: extract date - relative age = actual post date\n",
    "    \"\"\"\n",
    "\n",
    "    days_ago = int(row['dateposted'])\n",
    "    delta = timedelta(days_ago)\n",
    "    try:\n",
    "        return row['extractdate'] - delta\n",
    "    except:\n",
    "        return row\n",
    "\n",
    "def sal_fixer(data):\n",
    "    \"\"\"Ensures that all salaries are on the same annualized scale.\n",
    "\n",
    "    Args:\n",
    "        data (float): values from the salary column\n",
    "\n",
    "    Returns:\n",
    "        float: annualized salary\n",
    "    \"\"\"\n",
    "    if type(data) != np.nan:\n",
    "        data = round(data,0)\n",
    "\n",
    "        if len(str(data)) < 5:\n",
    "            data*=1000\n",
    "    else:\n",
    "        pass\n",
    "    return data\n",
    "\n",
    "data.postdate.value_counts()\n",
    "data['dateposted'] = data.postdate.apply(postD)\n",
    "data['extractdate'] = pd.to_datetime(data['extractdate'])\n",
    "data['dateposted'] = data.apply( lambda row : pDate(row), axis = 1)\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2b Salary\n",
    "\n",
    "'salary'\\\n",
    "Several different approaches will need to be used to extract salary information from this feature because employers provide this information themselves.\n",
    "For one example, the pay periods of hour, day, week etc will need to be scaled to an anuual scale so I can have a standard scale to start wiith.\n",
    "The salary ranges also appear here, so I'll be extracting numeric string characters, converting them into floats representing the lower and upper bounds of each range, and taking their medaian value.\n",
    "\n",
    "'estimated_salary'\\\n",
    "Indeed.com provides its own salary estimates in what appear to be bins.\n",
    "These and all other salary ranges will be collapsed to their median values once I've extracted and converted their numeric string reresentations.\n",
    "\n",
    "\n",
    "'sal_guide_items'\\\n",
    "While I could convert these to lists, the only information I need (salary) can be parsed out.\n",
    "\n",
    "\n",
    "\n",
    "'salary_and_jtype' & 'salfromsection'\\\n",
    "These will be wrangled in much the same way. The only other pertinent information in these features would be the job_type data already found in job_type_items,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(data.salary[-10:])\n",
    "data.salary.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.salary.unique()[:5]\n",
    "data.description[0]\n",
    "data.estimated_salary.unique()[:5]\n",
    "data.sal_guide_items.unique()[:5]\n",
    "data.salary_and_jtype.unique()[:3]\n",
    "data.salfromsection.unique()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def annual(data):\n",
    "    \"\"\"Annualizes salary data\n",
    "\n",
    "    Args:\n",
    "        data (dataframe): the entire dataframe is passed through.\n",
    "\n",
    "    Returns:\n",
    "        string: In the new 'annual_sal' column, creates annualized salaries by multiplying them against the observed (and previously extracted) pay rates.\n",
    "    \"\"\"\n",
    "    data['annual_sal'] = np.nan\n",
    "    data['annual_sal'] = np.where(data['schedule'].str.contains(\"hour\"), data.salary*40*(365/7), data['salary'])\n",
    "    #data['annual_sal'] = np.where(data['schedule'].str.contains(\"day\"), data['salary']*365/7*5, data['salary'])\n",
    "    #data['annual_sal'] = np.where(data['schedule'].str.contains(\"week\"), data['salary']*365/7, data['salary'])\n",
    "    #data['annual_sal'] = np.where(data['schedule'].str.contains(\"month\"), data['salary']*365/12, data['salary'])\n",
    "    #data['annual_sal'] = np.where(data['schedule'].str.contains(\"year\"), data['salary']/100, data['salary'])\n",
    "    return data\n",
    "\n",
    "def split_sal(i):\n",
    "    try:\n",
    "        lst = i.split('-',1)\n",
    "        x = lst[0]\n",
    "        y = lst[1]\n",
    "        \n",
    "        y = re.sub(r'[–]',r'',y)\n",
    "\n",
    "        return (float(x)+float(y))/2\n",
    "    except:\n",
    "        return i\n",
    "\n",
    "    salary = data[data.salary.notnull()]\n",
    "data['salary'] = salary['salary'].apply(sal_chars)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sal_chars(data):\n",
    "    \"\"\"When applied, substitutes '' for any listed special characters, \n",
    "\n",
    "    Args:\n",
    "        data (column values): I call this function on specific columns. Data, in this case, represents that column and its values.\n",
    "\n",
    "    Returns:\n",
    "        string: the original string values, sans any listed special characters.\n",
    "    \"\"\"\n",
    "    cleaned = re.sub(r'[\\n|,|+|$|[|$|\\'|\"]',r'',data)\n",
    "    return cleaned\n",
    "\n",
    "def sal_splitter(data):\n",
    "    \"\"\"Splits off salary information from a string formatted list\n",
    "\n",
    "    Args:\n",
    "        data (column values):  I call this function on specific columns. Data, in this case, represents that column and its values.\n",
    "\n",
    "    Returns:\n",
    "        string: returns the split containing salary information.\n",
    "    \"\"\"\n",
    "    x = data.split(',')\n",
    "    return x[2]\n",
    "\n",
    "\n",
    "    \n",
    "def Pay_period(data):\n",
    "    \"\"\"Used in the creation of a temporary column that will be discarded once I use it to generate a annualized salary column.\n",
    "\n",
    "    Args:\n",
    "        data (string): I call this function on specific columns. Data, in this case, represents that column and its values.\n",
    "\n",
    "    Returns:\n",
    "        string: pay rate descripions\n",
    "    \"\"\"\n",
    "    z = ''\n",
    "    substrings = ['hour','day','week','year']\n",
    "    for i in substrings:\n",
    "        if i in data:\n",
    "            return i\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "estimated_salary = data[data.estimated_salary.notnull()]\n",
    "data['estimated_salary'] = estimated_salary['estimated_salary'].apply(sal_chars)\n",
    "\n",
    "\n",
    "salfromsection = data[data.salfromsection.notnull()]\n",
    "data['salfromsection'] = salfromsection['salfromsection'].apply(sal_chars)\n",
    "\n",
    "salary_and_jtype = data[data.salfromsection.notnull()]\n",
    "data['salary_and_jtype'] = salary_and_jtype['salary_and_jtype'].apply(sal_chars)\n",
    "\n",
    "\n",
    "\n",
    "sal_guide_arr = data[data.sal_guide_items.notnull()]\n",
    "data['sal_guide_items'] = sal_guide_arr['sal_guide_items'].apply(sal_splitter)\n",
    "\n",
    "sal_guide_arr = data[data.sal_guide_items.notnull()]\n",
    "data['sal_guide_items'] = sal_guide_arr['sal_guide_items'].apply(sal_chars)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "estimated_salary = data[data['estimated_salary'].notnull()]\n",
    "data['Schedule1'] = estimated_salary['estimated_salary'].apply(Pay_period)\n",
    "\n",
    "sal_guide_items = data[data['sal_guide_items'].notnull()]\n",
    "data['Schedule2'] = sal_guide_items['sal_guide_items'].apply(Pay_period)\n",
    "\n",
    "salary = data[data['salary'].notnull()]\n",
    "data['Schedule3'] = salary['salary'].apply(Pay_period)\n",
    "\n",
    "salary_and_jtype = data[data['salary_and_jtype'].notnull()]\n",
    "data['Schedule4'] = salary_and_jtype['salary_and_jtype'].apply(Pay_period)\n",
    "\n",
    "salfromsection = data[data['salfromsection'].notnull()]\n",
    "data['Schedule5'] = salfromsection['salfromsection'].apply(Pay_period)\n",
    "\n",
    "val_cols = ['Schedule1','Schedule2','Schedule3','Schedule4','Schedule5']\n",
    "data['schedule'] = data[val_cols].bfill(axis=1).iloc[:, 0]\n",
    "data.drop(val_cols,inplace=True,axis=1)\n",
    "\n",
    "\n",
    "def splitter(data):\n",
    "    lst = []\n",
    "    if type(data) ==  str:\n",
    "        for val in re.findall('([0-9\\.]+)',data):\n",
    "            lst.append(val)\n",
    "        if len(lst) > 1:\n",
    "            return (float(lst[0])+float(lst[1]))/2\n",
    "        #\n",
    "\n",
    "\n",
    "\n",
    "data['estimated_salary'] = data['estimated_salary'].apply(splitter)\n",
    "data['salary'] = data['salary'].apply(splitter)\n",
    "col = ['estimated_salary','salary']\n",
    "data['salary'] = data[col].bfill(axis=1).iloc[:, 0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer(data):\n",
    "    if data > 1000:\n",
    "        return round(data/1000)\n",
    "    elif data > 1:\n",
    "        return round(data)\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "data['final_sal'] = data.annual_sal.apply(reducer)\n",
    "\n",
    "\n",
    "\n",
    "def restorer(data):\n",
    "    if data > 1:\n",
    "        return data*1000\n",
    "    else:\n",
    "        return data\n",
    "data['final_sal'] = data.final_sal.apply(restorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = sum(data.salary.isnull())\n",
    "nnull = sum(data.salary.notnull())\n",
    "print('')\n",
    "print(f'The data contains {data.shape[0]} rows of individual job postings with values for {data.shape[1]} columns based on\\n different sections of each post\\'s web page.')\n",
    "print('')\n",
    "print(f'- Out of {len(data)} job postings {nnull} or {round(nnull/len(data)*100,2)}% include some sort of salary information,\\n- The remaining {null} rowws, or {round(null/len(data)*100,2)}% are missing salary data.\\n- Also, there are no null values in the rest of the table meaning we won\\'t need\\n  to lose any data by dropping rows. While the null values in the Pay column will\\n  simply be dropped after we translate the not nulls into the target.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'job_type_items'\\\n",
    "    I'm going to treat these as likert values, converrting each of these string formatted lists into an ordinal values for a new column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.location.unique()[:5]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def jobtype(data):\n",
    "    \"\"\"Used for creating a new feature containing nominal values for the job type.\n",
    "\n",
    "    Args:\n",
    "        data (string): Values from the 'job_type_items' column.\n",
    "\n",
    "    Returns:\n",
    "        string: Extracted and simplified values for each category.\n",
    "    \"\"\"\n",
    "    if type(data) == str:\n",
    "        if re.search('full', data):\n",
    "            out = 'full'\n",
    "        if re.search('part', data):\n",
    "            out = 'part'\n",
    "        if re.search('contract', data):\n",
    "            out = 'contract'\n",
    "        if re.search('temp', data):\n",
    "            out = 'temp'\n",
    "        return out\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "cols = ['company','description', 'job_type_items','location','postdate','requirements',\t'summary','job_title']\n",
    "for txt_col in cols:\n",
    "    data[txt_col] = data[txt_col].str.lower()\n",
    "\n",
    "\n",
    "\n",
    "data['jobtype'] = data.job_type_items.apply(jobtype)\n",
    "\n",
    "data['text'] = data.description+' '+data.summary\n",
    "#TODO add requirements\n",
    "\n",
    "cols = ['job_type_items','raw_desc_soup','url','description','summary','requirements']\n",
    "\n",
    "\n",
    "data.text = data.text.replace(')',' ')\n",
    "\n",
    "\n",
    "\n",
    "data = annual(data)\n",
    "#data.schedule.value_counts() keep and eye on this, may have to deal with more than just hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'location'\\\n",
    "This feature holds a wealth of information: state, city/remote, and in some cases a zip code.\n",
    "Zip codes seem to be a recent addition to the information I can get from Indeed.com but since its presence is rather sparse I'll just extract city/remote and state values, splitting them into their respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.job_type_items.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'job_title'\\\n",
    "Although the webscrapper searches for 'data scientist' the results contain over 80 unique job titles. This is largely due to a prefix or suffix that provides the role's specialization at that particular company for that particular job posting. These include items like 'Marketing', 'Senior', 'Geospatial...'.\n",
    "I'll start by cleaning and simplifying these values, i.e. appearences of 'Sr' can be converted to 'Senior'. This will significantly reduce the number of unique values, hopefully increasing model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data.job_title.unique()))\n",
    "data.job_title.unique()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'requirements'\\\n",
    "I will most likely merge this in with the description prior to conducting NLP. The rationale is that these explicit requirements may be echoed in the description or summary, so by adding them to the corpus I'll raise the scores derived from their increaseed frequency. Eventually I wiuld like to build a set of requirements that I can use in with a clustering algorithm, like KNN, to find/define subgroups within the sample population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.requirements[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'summary'\\\n",
    "Much like the description, these are unique. COming from the search results page itself, these are brief descriptions of each job. In most cases this is not merely an abridgement of the summary. Like 'requirement' I'll start by lumping these texts in with description so analysis of the corpus can be boosted by the added descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.summary[:3]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'description'\\\n",
    "Each value is a unique string of the full job description. I'll lower and remove special characters but this feature will be dealt with seperately in its own NLP process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'raw_desc_soup'\\\n",
    "I'm only saving this in case I refine or change my parsing. Below is a truncated slice of one of the raw records.\n",
    "\n",
    "'url'\\\n",
    "Like raw_doc_soup, I'm simply keeping these in case I need to go back and re-parse the raw data. In fact, this URL points directly to the page where that raw doc soup is found.\n",
    "I may also be able to detect job post updates and duplicates by using these unique addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.raw_desc_soup[1][:1000]\n",
    "data.url[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Wrangling & Feature Engineering\n",
    "Below are the implementations of the data wrangling steps I described above for each feature. Using Numpy, Regex, and Pandas I'll simutaneaously clean the strings, extract and convert numeric information, and create new feature columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the data looks like at this point. It looks like the 17 original features have been reduced to 11. Most of the columns have been collapsed and cleaned, too. Just a bit more work till this is ready for EDA, NLP, and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data.rating = data.rating.fillna(0)\n",
    "\n",
    "cols = ['extractdate','postdate','schedule','jobtype','salary']\n",
    "data.drop(cols,inplace=True,axis=1)\n",
    "data = data.rename({'annual_sal': 'salary'}, axis=1)\n",
    "\n",
    "data['salary'] = data['salary'].apply(sal_fixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def states_(i):\n",
    "    \"\"\"Extracts state values from location\n",
    "\n",
    "    Args:\n",
    "        i (string): values from location\n",
    "\n",
    "    Returns:\n",
    "        string: extracted state value\n",
    "    \"\"\"\n",
    "    if ',' in i:\n",
    "        fix =  i.partition(',')[2].strip()\n",
    "        if len(fix) >2:\n",
    "            return fix[:2]\n",
    "        else:\n",
    "            return fix\n",
    "    if ',' not in i:\n",
    "        return i\n",
    "    elif 'remote' in i:\n",
    "        return 'remote'\n",
    "\n",
    "\n",
    "def cities_(i):\n",
    "    \"\"\"Extracts city values from location\n",
    "\n",
    "    Args:\n",
    "        i (string): values from location\n",
    "\n",
    "    Returns:\n",
    "        string: extracted city value\n",
    "    \"\"\"\n",
    "    if ',' in i:\n",
    "        return i.partition(',')[0]\n",
    "    if ',' not in i:\n",
    "        return i\n",
    "    elif 'remote' in i:\n",
    "        return 'remote'\n",
    "\n",
    "data['state'] = data.location.apply(states_)\n",
    "data['city'] = data[\"location\"].apply(cities_)\n",
    "#data[data.state.isnull()] = 'remote'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns\n",
    "cols = ['salary','estimated_salary', 'job_type_items','raw_desc_soup', 'sal_guide_items','salary_and_jtype', 'salfromsection']\n",
    "data.drop(cols,inplace=True,axis=1)\n",
    "data.rename(columns={'final_sal': 'salary'},inplace=True)\n",
    "\n",
    "data = data[data.description != 'NaN']\n",
    "\n",
    "len(data)\n",
    "\n",
    "data.description.value_counts()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(memory_usage='deep')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = data.shape\n",
    "print(f\"The {shape[0]} rows of data now have {shape[1]} variables whose values are now cleaned and typed properly.\\nThe only missing values are in Salary which is my target.\\nNext, I'll conduct analysis, encoding, and scaling of each feature, starting with Salary which .\")\n",
    "data.describe(include = 'all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='salary', data=data.notnull(), palette='Set3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = len(data[data.salary.isnull()])\n",
    "nnull = len(data[data.salary.notnull()])\n",
    "print(f'{round(nnull/len(data)*100,2)}% of the data has salary information, should be an easy target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(f'../app/data/wrangled_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged = data\n",
    "munged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = len(munged[munged.salary.isnull()])\n",
    "nnull = len(munged[munged.salary.notnull()])\n",
    "print(f'Droping {len(data)-len(munged)} duplicates and out of date posts leaves {len(munged)} rows of data for the regressor to be trained and tested on.')\n",
    "print(f'- Of those {len(munged)} job postings {nnull} or {round(nnull/len(munged)*100,2)}% include salary information,\\n- The remaining {null} rows, or {round(null/len(munged)*100,2)}% are missing salary data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy.stats import pearsonr, chi2_contingency\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "plt.rcParams['font.size'] = 17\n",
    "\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target\n",
    "\n",
    "### Numeric Features\n",
    "### Categorical Features\n",
    "### Feature Associations\n",
    "#### Feature Selection and Hyperparameter Tuning\n",
    "### Pipeline GridSearch\n",
    "## Modelling\n",
    "## Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Outliers: Using both the Z-Score and IQR methods\n",
    "    Now that preprocessed the data and building the target variable of Salary let's see conduct a bit more EDA to see if there are any outliers that could potentially skew how the regressor will learn from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pairplot(data=munged[munged.salary.notnull()],\n",
    "                  x_vars=['dateposted'],\n",
    "                  y_vars=['salary'], height=8, hue=\"salary\", palette=\"crest\")\n",
    "\n",
    "ax.fig.set_size_inches(18,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot is helpful in isualizing data, but let's program a few functions to detect and remove outliers based on thresholds.\n",
    "With the Z-Score, we can determine any data outside 3 standard deviations from the mean of our salary data to be an outlier.\n",
    "Wheras with the interquartile (IQR) range, the middle 50% of given salaries, I'll set the conventional cutoff of 1.5+/- the IQR as the cutoff.\n",
    "Values found outside of either range will be collected into a list and that'll allow for some fancy indexing so those particular postings can be examined and removed programatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z-Score\n",
    "Salaries 3 standard deviations away from the mean will be listed as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = []\n",
    "def z_detect(munged):\n",
    "    sample = munged[munged['salary'].notnull()].salary\n",
    "    threshold=3\n",
    "    mean = np.mean(sample)\n",
    "    std = np.std(sample)\n",
    "    \n",
    "    for i in sample:\n",
    "        z_score = (i-mean)/std\n",
    "        if np.abs(z_score) > threshold:\n",
    "            outliers.append(i)\n",
    "    if len(outliers) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        return outliers\n",
    "z_detect(munged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IQR\n",
    "Salaries outside 1.5 times the interquartile range boundaries, either above or below will be listed as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_detect(munged):\n",
    "    sample = munged[munged['salary'].notnull()].salary\n",
    "    Q1, Q3 = np.percentile(sample,[25,75])\n",
    "    iqr = Q3-Q1\n",
    "    lower_bound = Q1-(1.5*iqr)\n",
    "    upper_bound = Q3+(1.5*iqr)\n",
    "    for i in sample:\n",
    "        if (i < lower_bound)  | (i > upper_bound):\n",
    "            outliers.append(i)\n",
    "    if len(outliers) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        return outliers\n",
    "set(iqr_detect(munged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1):\n",
    "    list_set = set(list1)\n",
    "    # convert the set to the list\n",
    "    unique_list = (list(list_set))\n",
    "    for x in unique_list:\n",
    "        print(x)\n",
    "        \n",
    "mn = min(outliers)\n",
    "mx = max(outliers)\n",
    "print(f'The {len(set(outliers))} Unique Outliers Are:')\n",
    "unique(outliers)\n",
    "print(f'- With a minumum of ${round(mn)} and a maximum of ${round(mx)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at those tables.\n",
    "\n",
    "outliers = munged[munged['salary'].isin(outliers)]\n",
    "munged.drop(outliers.index, axis=0,inplace=True)\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = len(munged[munged.salary.isnull()])\n",
    "\n",
    "nnull = len(munged[munged.salary.notnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pairplot(data=munged[munged.salary.notnull()],\n",
    "                  x_vars=['dateposted'],\n",
    "                  y_vars=['salary'], height=8, hue=\"salary\", palette=\"crest\")\n",
    "\n",
    "ax.fig.set_size_inches(18,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='salary', data=munged.notnull(), palette='Set3')\n",
    "print(f'- Dropping {len(outliers)} outliers now leaves {nnull}, or {round(nnull/len(munged)*100,2)}%, of rows with with salary information,\\n- The remaining {null} rows, or {round(null/len(munged)*100,2)}%, are missing salary data.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.to_csv(f'../app/data/wrangled_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.drop('location',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac83f05b37eb5c5c49ba67e50f0047ddfbc4b30205fc79ee5f327b9c0ac37f55"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('indeedapp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "46216370387cf43888a1dc9433c5a4546bda9feed2ad4f35c2a851da9960dc54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
