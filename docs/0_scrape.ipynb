{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import random\n",
    "import time\n",
    "import re\n",
    "# current version of seaborn generates a bunch of warnings that we'll ignore\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this to test for captcha block or IP ban\n",
    "def get_URL(position,location):\n",
    "    #from torrequest import TorRequest\n",
    "    \"\"\"[Build a template url for a dummy call to verify the site isn't returning a captcha]\n",
    "    Args:\n",
    "        position ([string]): [job for query]\n",
    "        location ([string]): [location for query]\n",
    "    Returns:\n",
    "        [string]: [formatted url]\n",
    "    \"\"\"\n",
    "    template = 'https://www.indeed.com/jobs?q={}&l={}&fromage=1&sort=date'\n",
    "                \n",
    "    position = position.replace(' ', '%20')\n",
    "    location = location.replace(' ', '+')\n",
    "    url = template.format(position,location)\n",
    "    return url\n",
    "\n",
    "\n",
    "# from torrequest import TorRequest\n",
    "# tr=TorRequest(password='your_super_secure_password')\n",
    "position = 'data scientist'\n",
    "location = 'california'\n",
    "# tr.reset_identity()\n",
    "response = requests.get(get_URL(position,location))\n",
    "# This will either return an HTML block for a captcha or of a search result\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_desc_features(job_url):\n",
    "    \"\"\"Parses each job description, searching for and extracting values for features\n",
    "\n",
    "    Args:\n",
    "        job_url (string): http address of each job posting\n",
    "\n",
    "    Returns:\n",
    "        tuple: job feature values\n",
    "    \"\"\"\n",
    "    response_job_desc = requests.get(job_url)\n",
    "    soup = BeautifulSoup(response_job_desc.text, 'html.parser')\n",
    "\n",
    "    \n",
    "    try:\n",
    "        salary_and_job_type = soup.find('div', id='salaryInfoAndJobType').text.strip()\n",
    "    except:\n",
    "        salary_and_job_type = None\n",
    "    if salary_and_job_type == None:\n",
    "        try:\n",
    "            salary_and_job_type = soup.find('div',id=\"icl-u-xs-block jobsearch-ReqAndQualSection-item--title\").text.replace(\"\\n\", \"\").strip()\n",
    "        except:\n",
    "            salary_and_job_type = None\n",
    "    #TODO get benefits from its designated section\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        salary_guidance = []\n",
    "        items = soup.find('ul',class_='css-1lyr5hv eu4oa1w0')\n",
    "        for i in items:\n",
    "            salary_guidance.append(i.text)\n",
    "    except:\n",
    "        salary_guidance = None\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        job_type_items = []\n",
    "        job_type_from_section = soup.find('div',class_='jobsearch-JobDescriptionSection-sectionItem').next_sibling.children\n",
    "        for i in job_type_from_section:\n",
    "            if i.text == 'Job Type':\n",
    "                continue\n",
    "            else:\n",
    "                job_type_items.append(i.text)\n",
    "    except:\n",
    "        job_type_items = None\n",
    "        \n",
    "    \n",
    "    try:\n",
    "        requirements = soup.find(class_=\"icl-u-xs-block jobsearch-ReqAndQualSection-item--title\").text.replace(\"\\n\", \"\").strip()      \n",
    "\n",
    "    except:\n",
    "        requirements = None\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        description = soup.find(id=\"jobDescriptionText\").text.replace('\\n', '')\n",
    "    except:\n",
    "        description = None\n",
    "        \n",
    "        \n",
    "    # A nifty little workaround for evading detection.\n",
    "    time.sleep(.5+random()*3)\n",
    "    #TODO assess h2 tags commonalities to determine if these section descriptions are from Indeed or are at least of only a few variations.\n",
    "        #you could then distinguish the description into sections and conduct NLP etc each.\n",
    "    raw_desc_soup = soup\n",
    "    return salary_and_job_type, salary_guidance, job_type_items, requirements, description, raw_desc_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO condense these with lists, particularly fields that have .text.strip()\n",
    "def get_features(post):\n",
    "    \"\"\"parses search results and extracts basic job feature values,\n",
    "        then combines this with output of 'get_desc_features' function.\n",
    "\n",
    "    Args:\n",
    "        post (string): response for each post in search results page\n",
    "\n",
    "    Returns:\n",
    "        dict: single-feature deep dictionary of features (dictionary keys) and their values (dictionary values)\n",
    "    \"\"\"\n",
    "    datapoint_dict = {}\n",
    "\n",
    "    title = post.find('h2',\n",
    "              attrs={'class': lambda e: e.startswith('jobTitle') if e else False}).text.replace('new', '')\n",
    "\n",
    "    company = post.find('span', 'companyName').text.strip()\n",
    "    try:\n",
    "        rating = post.find('span', 'ratingNumber').text\n",
    "    except:\n",
    "        rating = None\n",
    "\n",
    "    location = post.find('div', 'companyLocation').text.strip()\n",
    "    postDate = post.find('span', 'date').text\n",
    "    extractDate = datetime.today().strftime('%Y-%m-%d')\n",
    "    summary = post.find('div', 'job-snippet').text.strip().replace('\\n', ' ')\n",
    "    url = 'https://www.indeed.com'+ post.find('a', href = re.compile(r'[/]([a-z]|[A-Z])\\w+')).attrs['href']\n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "    salary, estimated_salary, job_type_items, requirements, description, raw_desc_soup = get_desc_features(url)\n",
    "    datapoint_dict = {\n",
    "                        'title':title,\n",
    "                        'company':company,\n",
    "                        'rating':rating,\n",
    "                        'location':location,\n",
    "                        'estimated_salary':estimated_salary,\n",
    "                        'postDate':postDate,\n",
    "                        'extractDate':extractDate,\n",
    "                        'summary':summary,\n",
    "                        'url':url,\n",
    "                        'salary':salary,\n",
    "                        'job_type_items':job_type_items,\n",
    "                        'requirements':requirements,\n",
    "                        'description':description,\n",
    "                        'raw_desc_soup':raw_desc_soup}\n",
    "    if len(datapoint_dict) > 0:\n",
    "        return datapoint_dict\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(position, location):\n",
    "    \"\"\"[Conducts the web scraping process]\n",
    "    Args:\n",
    "        position ([string]): [job position for indeed.com query]\n",
    "        position ([string]): [job location for indeed.com query]\n",
    "        \n",
    "        Returns:\n",
    "        [csv]: [scraped data]\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    # extract the job data\n",
    "    run = True\n",
    "    while run == True:\n",
    "        response = requests.get(get_URL(position, location))\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        searchResults = soup.find('div', id='mosaic-provider-jobcards')\n",
    "        refinedsearchResults = searchResults.find_all('div', attrs={'class': lambda e: e.startswith('cardOutline') if e else False})\n",
    "        \n",
    "        raw_posts = []\n",
    "        for post in refinedsearchResults:\n",
    "            raw_posts.append(post)\n",
    "        \n",
    "        n = 0\n",
    "        for post in raw_posts:\n",
    "            datapoint = get_features(post)\n",
    "            data = data.append(datapoint, ignore_index=True)\n",
    "        # Again, a nifty little workaround for evading detection.\n",
    "            n+=1\n",
    "            if n%15==0:\n",
    "                print('New Page')\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "            \n",
    "        try:\n",
    "            url = 'https://www.indeed.com' + soup.find('a', {'aria-label': 'Next'}).get('href')\n",
    "        except AttributeError:\n",
    "            run = False\n",
    "\n",
    "        \n",
    "\n",
    "    name = position.replace(' ','_')\n",
    "    loc = location.replace(' ','_')\n",
    "    day = date.today()\n",
    "    # save the job data\n",
    "    data.to_csv(f'../app/data/scraped_{name}_{loc}.csv', index=False)\n",
    "    \n",
    "    return f'Scraped {len(data)} new records.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idaho\n",
      "illinois\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "indiana\n",
      "kansas\n",
      "kentucky\n",
      "louisiana\n",
      "maryland\n",
      "New Page\n",
      "New Page\n",
      "maine\n",
      "michigan\n",
      "minnesota\n",
      "New Page\n",
      "missouri\n",
      "mississippi\n",
      "montana\n",
      "north carolina\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "north dakota\n",
      "nebraska\n",
      "new hampshire\n",
      "new jersey\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "new mexico\n",
      "nevada\n",
      "ohio\n",
      "oklahoma\n",
      "pennsylvania\n",
      "rhode island\n",
      "south carolina\n",
      "south dakota\n",
      "tennessee\n",
      "utah\n",
      "virginia\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "vermont\n",
      "wisconsin\n",
      "west virginia\n",
      "wyoming\n"
     ]
    }
   ],
   "source": [
    "#state_names = [ \"alabama\", \"arkansas\",  \"arizona\",  \"colorado\", \"connecticut\", \"delaware\",  \"georgia\", \"iowa\", \"idaho\", \"illinois\", \"indiana\", \"kansas\", \"kentucky\", \"louisiana\", \"maryland\", \"maine\", \"michigan\", \"minnesota\", \"missouri\", \"mississippi\", \"montana\", \"north carolina\", \"north dakota\", \"nebraska\", \"new hampshire\", \"new jersey\", \"new mexico\", \"nevada\",  \"ohio\", \"oklahoma\", \"pennsylvania\", \"rhode island\", \"south carolina\", \"south dakota\", \"tennessee\", \"utah\", \"virginia\",  \"vermont\",  \"wisconsin\", \"west virginia\", \"wyoming\"]\n",
    "state_names = [\"idaho\", \"illinois\", \"indiana\", \"kansas\", \"kentucky\", \"louisiana\", \"maryland\", \"maine\", \"michigan\", \"minnesota\", \"missouri\", \"mississippi\", \"montana\", \"north carolina\", \"north dakota\", \"nebraska\", \"new hampshire\", \"new jersey\", \"new mexico\", \"nevada\",  \"ohio\", \"oklahoma\", \"pennsylvania\", \"rhode island\", \"south carolina\", \"south dakota\", \"tennessee\", \"utah\", \"virginia\",  \"vermont\",  \"wisconsin\", \"west virginia\", \"wyoming\"]\n",
    "\n",
    "for state in state_names:\n",
    "    position = 'data scientist'\n",
    "    location = state\n",
    "    print(state)\n",
    "    data = main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Scraped 135 new records.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 'data scientist'\n",
    "location = 'california'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Scraped 120 new records.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 'data scientist'\n",
    "location = 'remote'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.read_csv('../app/data/scraped_data_scientist_remote.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Page\n",
      "New Page\n",
      "New Page\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Scraped 45 new records.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 'data scientist'\n",
    "location = 'new york'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Page\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Scraped 15 new records.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 'data scientist'\n",
    "location = 'texas'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Page\n",
      "New Page\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Scraped 30 new records.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 'data scientist'\n",
    "location = 'washington'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scraped 12 new records.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 'data scientist'\n",
    "location = 'florida'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n",
      "New Page\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Scraped 135 new records.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 'data scientist'\n",
    "location = 'massachusetts'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scraped 4 new records.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 'data scientist'\n",
    "location = 'oregon'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  below is used for various adjustments to my webscraping process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating Data With New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped  2 new records for alabama\n",
      "Scraped  1 new records for arkansas\n",
      "Scraped  7 new records for arizona\n",
      "Scraped  135 new records for california\n",
      "Scraped  13 new records for colorado\n",
      "Scraped  9 new records for connecticut\n",
      "Scraped  5 new records for delaware\n",
      "Scraped  12 new records for florida\n",
      "Scraped  120 new records for georgia\n",
      "Scraped  120 new records for remote\n",
      "Scraped  2 new records for iowa\n",
      "Scraped  1 new records for idaho\n",
      "Scraped  45 new records for illinois\n",
      "Scraped  4 new records for indiana\n",
      "Scraped  2 new records for kansas\n",
      "Scraped  6 new records for kentucky\n",
      "Scraped  1 new records for louisiana\n",
      "Scraped  135 new records for massachusetts\n",
      "Scraped  30 new records for maryland\n",
      "Scraped  1 new records for maine\n",
      "Scraped  4 new records for michigan\n",
      "Scraped  15 new records for minnesota\n",
      "Scraped  4 new records for missouri\n",
      "Scraped  1 new records for mississippi\n",
      "Scraped  1 new records for montana\n",
      "Scraped  45 new records for north_carolina\n",
      "Scraped  2 new records for north_dakota\n",
      "Scraped  1 new records for nebraska\n",
      "Scraped  1 new records for new_hampshire\n",
      "Scraped  60 new records for new_jersey\n",
      "Scraped  2 new records for new_mexico\n",
      "Scraped  1 new records for nevada\n",
      "Scraped  45 new records for new_york\n",
      "Scraped  8 new records for ohio\n",
      "Scraped  3 new records for oklahoma\n",
      "Scraped  4 new records for oregon\n",
      "Scraped  14 new records for pennsylvania\n",
      "Scraped  4 new records for rhode_island\n",
      "Scraped  3 new records for south_carolina\n",
      "Scraped  2 new records for south_dakota\n",
      "Scraped  7 new records for tennessee\n",
      "Scraped  15 new records for texas\n",
      "Scraped  4 new records for utah\n",
      "Scraped  150 new records for virginia\n",
      "Scraped  2 new records for vermont\n",
      "Scraped  30 new records for washington\n",
      "Scraped  8 new records for wisconsin\n",
      "Scraped  1 new records for west_virginia\n",
      "Scraped  1 new records for wyoming\n",
      "Scraped Records: 1089\n",
      "Total Records: 2515\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>description</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>extractDate</th>\n",
       "      <th>job_type_items</th>\n",
       "      <th>location</th>\n",
       "      <th>postDate</th>\n",
       "      <th>rating</th>\n",
       "      <th>raw_desc_soup</th>\n",
       "      <th>requirements</th>\n",
       "      <th>salary</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Torch Technologies, Inc.</td>\n",
       "      <td>Job Description:  Torch Technologies is seeki...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-06-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Huntsville, AL 35802</td>\n",
       "      <td>PostedToday</td>\n",
       "      <td>4.4</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Experience with physics-based model and simula...</td>\n",
       "      <td>Data Assimilation Scientist</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=427952c4d220d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CyberCoders</td>\n",
       "      <td>Senior ML Engineer - Stealth, Fully Funded (5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-06-28</td>\n",
       "      <td>['Full-time']</td>\n",
       "      <td>Huntsville, AL 35801</td>\n",
       "      <td>Posted1 day ago</td>\n",
       "      <td>3.6</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$100,000 - $175,000 a year -  Full-time</td>\n",
       "      <td>Online (real-time) and offline data collection...</td>\n",
       "      <td>Senior ML Engineer - Stealth, Fully Funded (50...</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NielsenIQ</td>\n",
       "      <td>Company Description  REFERENCE ID# REF16983Y ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-06-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bentonville, AR</td>\n",
       "      <td>PostedToday</td>\n",
       "      <td>3.6</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Act as department/company subject matter exper...</td>\n",
       "      <td>Sr. Statistical Operations Analyst</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=3388832cf2a27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ryder System</td>\n",
       "      <td>Current Employees:If you are a current employe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-06-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote in Phoenix, AZ 85067+1 location</td>\n",
       "      <td>PostedJust posted</td>\n",
       "      <td>3.3</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Challenge the status quo and help guide entrep...</td>\n",
       "      <td>Sr. Data Analyst (REMOTE)</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=a9256d941cfea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Realty Income</td>\n",
       "      <td>Are you interested in working with newest data...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-06-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Phoenix, AZ 85016 (Camelback East area)</td>\n",
       "      <td>PostedJust posted</td>\n",
       "      <td>4.3</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Coordinate with the Predictive Analytics team ...</td>\n",
       "      <td>Senior Data Warehouse Engineer/Architect</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=e90faffcc124d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>Sonic Foundry</td>\n",
       "      <td>The Lead Data Scientist – Video ai is respon...</td>\n",
       "      <td>['', 'Not provided by employer', \"$116K - $147...</td>\n",
       "      <td>2022-06-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madison, WI 53703 (Capitol area)</td>\n",
       "      <td>PostedJust posted</td>\n",
       "      <td>4.2</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>5+ years experience working with machine learn...</td>\n",
       "      <td>Lead Data Scientist - Video AI (FT)</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=d7dfd29204288...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>Change Healthcare</td>\n",
       "      <td>Company: Change Healthcare (NASDAQ: CHNG) is a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-06-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>PostedToday</td>\n",
       "      <td>3.0</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>You will enjoy working with a highly talented ...</td>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=92041a1fbba49...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>Kadiak, LLC</td>\n",
       "      <td>Kadiak,LLC, a Koniag Government Services compa...</td>\n",
       "      <td>['', 'Not provided by employer', \"$84.9K - $10...</td>\n",
       "      <td>2022-06-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Clarksburg, WV</td>\n",
       "      <td>PostedToday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Under the direction of the government, the suc...</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=80347759e1199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>Change Healthcare</td>\n",
       "      <td>Company: Change Healthcare (NASDAQ: CHNG) is a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-06-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>PostedToday</td>\n",
       "      <td>3.0</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>You will enjoy working with a highly talented ...</td>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=422bdc77b407f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>Change Healthcare</td>\n",
       "      <td>Company: Change Healthcare (NASDAQ: CHNG) is a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-06-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>PostedToday</td>\n",
       "      <td>3.0</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>You will enjoy working with a highly talented ...</td>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=43f692fed6925...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2515 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       company  \\\n",
       "0     Torch Technologies, Inc.   \n",
       "1                  CyberCoders   \n",
       "0                    NielsenIQ   \n",
       "0                 Ryder System   \n",
       "1                Realty Income   \n",
       "...                        ...   \n",
       "2510             Sonic Foundry   \n",
       "2511         Change Healthcare   \n",
       "2512               Kadiak, LLC   \n",
       "2513         Change Healthcare   \n",
       "2514         Change Healthcare   \n",
       "\n",
       "                                            description  \\\n",
       "0      Job Description:  Torch Technologies is seeki...   \n",
       "1      Senior ML Engineer - Stealth, Fully Funded (5...   \n",
       "0      Company Description  REFERENCE ID# REF16983Y ...   \n",
       "0     Current Employees:If you are a current employe...   \n",
       "1     Are you interested in working with newest data...   \n",
       "...                                                 ...   \n",
       "2510    The Lead Data Scientist – Video ai is respon...   \n",
       "2511  Company: Change Healthcare (NASDAQ: CHNG) is a...   \n",
       "2512  Kadiak,LLC, a Koniag Government Services compa...   \n",
       "2513  Company: Change Healthcare (NASDAQ: CHNG) is a...   \n",
       "2514  Company: Change Healthcare (NASDAQ: CHNG) is a...   \n",
       "\n",
       "                                       estimated_salary extractDate  \\\n",
       "0                                                   NaN  2022-06-28   \n",
       "1                                                   NaN  2022-06-28   \n",
       "0                                                   NaN  2022-06-28   \n",
       "0                                                   NaN  2022-06-28   \n",
       "1                                                   NaN  2022-06-28   \n",
       "...                                                 ...         ...   \n",
       "2510  ['', 'Not provided by employer', \"$116K - $147...  2022-06-24   \n",
       "2511                                                NaN  2022-06-24   \n",
       "2512  ['', 'Not provided by employer', \"$84.9K - $10...  2022-06-24   \n",
       "2513                                                NaN  2022-06-24   \n",
       "2514                                                NaN  2022-06-24   \n",
       "\n",
       "     job_type_items                                 location  \\\n",
       "0               NaN                     Huntsville, AL 35802   \n",
       "1     ['Full-time']                     Huntsville, AL 35801   \n",
       "0               NaN                          Bentonville, AR   \n",
       "0               NaN   Remote in Phoenix, AZ 85067+1 location   \n",
       "1               NaN  Phoenix, AZ 85016 (Camelback East area)   \n",
       "...             ...                                      ...   \n",
       "2510            NaN         Madison, WI 53703 (Capitol area)   \n",
       "2511            NaN                                Wisconsin   \n",
       "2512            NaN                           Clarksburg, WV   \n",
       "2513            NaN                            West Virginia   \n",
       "2514            NaN                                  Wyoming   \n",
       "\n",
       "               postDate  rating  \\\n",
       "0           PostedToday     4.4   \n",
       "1       Posted1 day ago     3.6   \n",
       "0           PostedToday     3.6   \n",
       "0     PostedJust posted     3.3   \n",
       "1     PostedJust posted     4.3   \n",
       "...                 ...     ...   \n",
       "2510  PostedJust posted     4.2   \n",
       "2511        PostedToday     3.0   \n",
       "2512        PostedToday     NaN   \n",
       "2513        PostedToday     3.0   \n",
       "2514        PostedToday     3.0   \n",
       "\n",
       "                                          raw_desc_soup requirements  \\\n",
       "0     <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "1     <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "0     <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "0     <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "1     <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "...                                                 ...          ...   \n",
       "2510  <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "2511  <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "2512  <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "2513  <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "2514  <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "\n",
       "                                       salary  \\\n",
       "0                                   Full-time   \n",
       "1     $100,000 - $175,000 a year -  Full-time   \n",
       "0                                   Full-time   \n",
       "0                                         NaN   \n",
       "1                                   Full-time   \n",
       "...                                       ...   \n",
       "2510                                Full-time   \n",
       "2511                                Full-time   \n",
       "2512                                      NaN   \n",
       "2513                                Full-time   \n",
       "2514                                Full-time   \n",
       "\n",
       "                                                summary  \\\n",
       "0     Experience with physics-based model and simula...   \n",
       "1     Online (real-time) and offline data collection...   \n",
       "0     Act as department/company subject matter exper...   \n",
       "0     Challenge the status quo and help guide entrep...   \n",
       "1     Coordinate with the Predictive Analytics team ...   \n",
       "...                                                 ...   \n",
       "2510  5+ years experience working with machine learn...   \n",
       "2511  You will enjoy working with a highly talented ...   \n",
       "2512  Under the direction of the government, the suc...   \n",
       "2513  You will enjoy working with a highly talented ...   \n",
       "2514  You will enjoy working with a highly talented ...   \n",
       "\n",
       "                                                  title  \\\n",
       "0                           Data Assimilation Scientist   \n",
       "1     Senior ML Engineer - Stealth, Fully Funded (50...   \n",
       "0                    Sr. Statistical Operations Analyst   \n",
       "0                             Sr. Data Analyst (REMOTE)   \n",
       "1              Senior Data Warehouse Engineer/Architect   \n",
       "...                                                 ...   \n",
       "2510                Lead Data Scientist - Video AI (FT)   \n",
       "2511                                Lead Data Scientist   \n",
       "2512                                       Data Science   \n",
       "2513                                Lead Data Scientist   \n",
       "2514                                Lead Data Scientist   \n",
       "\n",
       "                                                    url  \n",
       "0     https://www.indeed.com/rc/clk?jk=427952c4d220d...  \n",
       "1     https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...  \n",
       "0     https://www.indeed.com/rc/clk?jk=3388832cf2a27...  \n",
       "0     https://www.indeed.com/rc/clk?jk=a9256d941cfea...  \n",
       "1     https://www.indeed.com/rc/clk?jk=e90faffcc124d...  \n",
       "...                                                 ...  \n",
       "2510  https://www.indeed.com/rc/clk?jk=d7dfd29204288...  \n",
       "2511  https://www.indeed.com/rc/clk?jk=92041a1fbba49...  \n",
       "2512  https://www.indeed.com/rc/clk?jk=80347759e1199...  \n",
       "2513  https://www.indeed.com/rc/clk?jk=422bdc77b407f...  \n",
       "2514  https://www.indeed.com/rc/clk?jk=43f692fed6925...  \n",
       "\n",
       "[2515 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_names = ['alabama', 'arkansas', 'arizona', 'california', 'colorado', 'connecticut', 'delaware', 'florida', 'georgia', 'remote', 'iowa', 'idaho', 'illinois', 'indiana', 'kansas', 'kentucky', 'louisiana', 'massachusetts', 'maryland', 'maine', 'michigan', 'minnesota', 'missouri', 'mississippi', 'montana', 'north carolina', 'north dakota', 'nebraska', 'new hampshire', 'new jersey', 'new mexico', 'nevada', 'new york', 'ohio', 'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas', 'utah', 'virginia',  'vermont', 'washington', 'wisconsin', 'west virginia', 'wyoming']\n",
    "\n",
    "lst = []\n",
    "records = 0\n",
    "for i in state_names:\n",
    "    i = i.replace(' ','_')\n",
    "    try:\n",
    "        location = pd.read_csv(f'../app/data/scraped_data_scientist_{i}.csv')\n",
    "        records += len(location)\n",
    "        lst.append(location)\n",
    "        print(f'Scraped  {len(location)} new records for {i}')\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f'Scraped Records: {(records)}')\n",
    "\n",
    "#todays_scrape = pd.concat(lst)\n",
    "## unblock if you miss too many days\n",
    "#total = todays_scrape.to_csv('../app/data/total.csv', index= False)\n",
    "\n",
    "total = pd.read_csv(f'../app/data/total.csv')\n",
    "lst.append(total)\n",
    "\n",
    "total = pd.concat(lst)\n",
    "\n",
    "total.drop_duplicates(inplace=True)\n",
    "\n",
    "total.to_csv('../app/data/total.csv', index= False)\n",
    "print(f'Total Records: {(len(total))}')\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO explain why this is being shown. remove from data and save, but also show what portion of the data it represents. Relatively miniscule.\n",
    "total[total.description.isna()].location.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below are bits of code I employ if something goes wrong with the webscraping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#fix old imports\n",
    "\n",
    "data['extractDate']= pd.to_datetime(data['extractDate'])\n",
    "\n",
    "def pDate(row):\n",
    "    from datetime import datetime, date, timedelta\n",
    "\n",
    "    #days_ago = row['dateposted']\n",
    "    delta = timedelta(0)\n",
    "    try:\n",
    "        return row['extractDate'] - delta\n",
    "    except:\n",
    "        return row\n",
    "\n",
    "data['extractDate'] = data.apply( lambda row : pDate(row), axis = 1)\n",
    "data['extractDate'] = data['extractDate'].astype(str)\n",
    "#data.to_csv('../app/data/scraped_data_scientist_remote_2022-04-14.csv', index= False)\n",
    "data.extractDate.unique()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# codescraps in case they change the html and break my parsers\n",
    "\n",
    "searchResults = soup.find('div', id='mosaic-provider-jobcards')\n",
    "refinedsearchResults = searchResults.find_all('div', attrs={'class': lambda e: e.startswith('cardOutline') if e else False})\n",
    "len(refinedsearchResults)\n",
    "z = searchResults.children\n",
    "lst = []\n",
    "for i in z:\n",
    "    lst.append(i)\n",
    "\n",
    "x = lst[0]\n",
    "\n",
    "#checking again\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#searchResults = soup.find('div', id='mosaic-provider-jobcards')\n",
    "refinedsearchResults = soup.find_all('div', attrs={'class': lambda e: e.startswith('cardOutline') if e else False})\n",
    "        \n",
    "\n",
    "raw_posts = []\n",
    "for post in refinedsearchResults:\n",
    "        raw_posts.append(post)\n",
    "        n = 0\n",
    "\n",
    "z = raw_posts[0]\n",
    "url = z.find('a', href=True)\n",
    "url\n",
    "\n",
    "\n",
    "z.find('a', href = re.compile(r'[/]([a-z]|[A-Z])\\w+')).attrs['href']\n",
    "z\n",
    "\n",
    "postDate = z.find('span', 'date').text\n",
    "extractDate = datetime.today().strftime('%Y-%m-%d')\n",
    "summary = z.find('div', 'job-snippet').text.strip().replace('\\n', ' ')\n",
    "\n",
    "summary\n",
    "\n",
    "company_name = z.find('a', attrs={'class':'turnstileLink companyOverviewLink'}).text.strip()\n",
    "company_name\n",
    "\n",
    "job_title = z.find('a', attrs={'class':'jcs-JobTitle'}).text.strip()\n",
    "job_title\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac83f05b37eb5c5c49ba67e50f0047ddfbc4b30205fc79ee5f327b9c0ac37f55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "46216370387cf43888a1dc9433c5a4546bda9feed2ad4f35c2a851da9960dc54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
