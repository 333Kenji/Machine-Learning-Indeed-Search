{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, date\n",
    "\n",
    "from random import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# current version of seaborn generates a bunch of warnings that we'll ignore\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this to test for captcha block or IP ban\n",
    "def get_URL(position,location):\n",
    "    #from torrequest import TorRequest\n",
    "    \"\"\"[Build a template url for a dummy call to verify the site isn't returning a captcha]\n",
    "    Args:\n",
    "        position ([string]): [job for query]\n",
    "        location ([string]): [location for query]\n",
    "    Returns:\n",
    "        [string]: [formatted url]\n",
    "    \"\"\"\n",
    "    template = 'https://www.indeed.com/jobs?q={}&l={}&fromage=2&sort=date'\n",
    "                \n",
    "    position = position.replace(' ', '%20')\n",
    "    location = location.replace(' ', '+')\n",
    "    url = template.format(position,location)\n",
    "    return url\n",
    "\n",
    "# The below is just to check that the response is '200', successful connection\n",
    "position = 'data scientist'\n",
    "location = 'california'\n",
    "response = requests.get(get_URL(position,location))\n",
    "# This will either return an HTML block for a captcha or of a search result\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_desc_features(job_url):\n",
    "    \"\"\"Parses each job description, searching for and extracting values for features\n",
    "\n",
    "    Args:\n",
    "        job_url (string): http address of each job posting\n",
    "\n",
    "    Returns:\n",
    "        tuple: job feature values\n",
    "    \"\"\"\n",
    "    response_job_desc = requests.get(job_url)\n",
    "    soup = BeautifulSoup(response_job_desc.text, 'html.parser')\n",
    "\n",
    "    \n",
    "    try:\n",
    "        salary_and_job_type = soup.find('div', id='salaryInfoAndJobType').text.strip()\n",
    "    except:\n",
    "        salary_and_job_type = None\n",
    "    if salary_and_job_type == None:\n",
    "        try:\n",
    "            salary_and_job_type = soup.find('div',id=\"icl-u-xs-block jobsearch-ReqAndQualSection-item--title\").text.replace(\"\\n\", \"\").strip()\n",
    "        except:\n",
    "            salary_and_job_type = None\n",
    "    #TODO get benefits from its designated section\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        salary_guidance = []\n",
    "        items = soup.find('ul',class_='css-1lyr5hv eu4oa1w0')\n",
    "        for i in items:\n",
    "            salary_guidance.append(i.text)\n",
    "    except:\n",
    "        salary_guidance = None\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        job_type_items = []\n",
    "        job_type_from_section = soup.find('div',class_='jobsearch-JobDescriptionSection-sectionItem').next_sibling.children\n",
    "        for i in job_type_from_section:\n",
    "            if i.text == 'Job Type':\n",
    "                continue\n",
    "            else:\n",
    "                job_type_items.append(i.text)\n",
    "    except:\n",
    "        job_type_items = None\n",
    "        \n",
    "    \n",
    "    try:\n",
    "        requirements = soup.find(class_=\"icl-u-xs-block jobsearch-ReqAndQualSection-item--title\").text.replace(\"\\n\", \"\").strip()      \n",
    "\n",
    "    except:\n",
    "        requirements = None\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        description = soup.find(id=\"jobDescriptionText\").text.replace('\\n', '')\n",
    "    except:\n",
    "        description = None\n",
    "        \n",
    "        \n",
    "    # A nifty little workaround for evading detection.\n",
    "    time.sleep(.5+random()*3)\n",
    "    #TODO assess h2 tags commonalities to determine if these section descriptions are from Indeed or are at least of only a few variations.\n",
    "        #you could then distinguish the description into sections and conduct NLP etc each.\n",
    "    raw_desc_soup = soup\n",
    "    return salary_and_job_type, salary_guidance, job_type_items, requirements, description, raw_desc_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO condense these with lists, particularly fields that have .text.strip()\n",
    "def get_features(post):\n",
    "    \"\"\"parses search results and extracts basic job feature values,\n",
    "        then combines this with output of 'get_desc_features' function.\n",
    "\n",
    "    Args:\n",
    "        post (string): response for each post in search results page\n",
    "\n",
    "    Returns:\n",
    "        dict: single-feature deep dictionary of features (dictionary keys) and their values (dictionary values)\n",
    "    \"\"\"\n",
    "    #make set list of old urls\n",
    "    urls = pd.read_csv('../app/data/total.csv', usecols = ['url'])\n",
    "    urls.drop_duplicates(inplace=True)\n",
    "\n",
    "    url = 'https://www.indeed.com'+ post.find('a', href = re.compile(r'[/]([a-z]|[A-Z])\\w+')).attrs['href']\n",
    "\n",
    "\n",
    "    #check if new url is in list\n",
    "    if url not in urls.url.values:\n",
    "        title = post.find('h2',\n",
    "                attrs={'class': lambda e: e.startswith('jobTitle') if e else False}).text.replace('new', '')\n",
    "\n",
    "        company = post.find('span', 'companyName').text.strip()\n",
    "        try:\n",
    "            rating = post.find('span', 'ratingNumber').text\n",
    "        except:\n",
    "            rating = None\n",
    "\n",
    "        location = post.find('div', 'companyLocation').text.strip()\n",
    "        postDate = post.find('span', 'date').text\n",
    "        extractDate = datetime.today().strftime('%Y-%m-%d')\n",
    "        summary = post.find('div', 'job-snippet').text.strip().replace('\\n', ' ')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        salary, estimated_salary, job_type_items, requirements, description, raw_desc_soup = get_desc_features(url)\n",
    "\n",
    "        datapoint_dict = {}\n",
    "        datapoint_dict = {\n",
    "                            'title':title,\n",
    "                            'company':company,\n",
    "                            'rating':rating,\n",
    "                            'location':location,\n",
    "                            'estimated_salary':estimated_salary,\n",
    "                            'postDate':postDate,\n",
    "                            'extractDate':extractDate,\n",
    "                            'summary':summary,\n",
    "                            'url':url,\n",
    "                            'salary':salary,\n",
    "                            'job_type_items':job_type_items,\n",
    "                            'requirements':requirements,\n",
    "                            'description':description,\n",
    "                            'raw_desc_soup':raw_desc_soup}\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return datapoint_dict\n",
    "    else:\n",
    "        print(' skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(position, location):\n",
    "    \"\"\"[Conducts the web scraping process]\n",
    "    Args:\n",
    "        position ([string]): [job position for indeed.com query]\n",
    "        position ([string]): [job location for indeed.com query]\n",
    "        \n",
    "        Returns:\n",
    "        [csv]: [scraped data]\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    # extract the job data\n",
    "    while True:\n",
    "        response = requests.get(get_URL(position, location))\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        searchResults = soup.find('div', id='mosaic-provider-jobcards')\n",
    "        refinedsearchResults = searchResults.find_all('div', attrs={'class': lambda e: e.startswith('cardOutline') if e else False})\n",
    "        \n",
    "        raw_posts = []\n",
    "        for post in refinedsearchResults:\n",
    "            raw_posts.append(post)\n",
    "        \n",
    "        for post in raw_posts:\n",
    "            datapoint = get_features(post)\n",
    "            data = data.append(datapoint, ignore_index=True)\n",
    "        \n",
    "        try:\n",
    "            url = 'https://www.indeed.com' + soup.find('a', {'aria-label': 'Next'}).get('href')\n",
    "            print(' new page')\n",
    "        except AttributeError:\n",
    "            break\n",
    "\n",
    "        \n",
    "\n",
    "    name = position.replace(' ','_')\n",
    "    loc = location.replace(' ','_')\n",
    "    # save the job data\n",
    "    data.to_csv(f'../app/data/scraped_{name}_{loc}.csv', index=False)\n",
    "    \n",
    "    return f'Scraped {len(data)} new records.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alabama\n",
      "23\n",
      "arkansas\n",
      "22\n",
      "arizona\n",
      "23\n",
      "colorado\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      "24\n",
      "connecticut\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      "24\n",
      "delaware\n",
      "22\n",
      "georgia\n",
      " new page\n",
      " new page\n",
      " new page\n",
      "23\n",
      "iowa\n",
      "22\n",
      "idaho\n",
      "22\n",
      "illinois\n",
      " new page\n",
      " new page\n",
      "23\n",
      "indiana\n",
      "22\n",
      "kansas\n",
      "22\n",
      "kentucky\n",
      "22\n",
      "louisiana\n",
      "22\n",
      "maryland\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      "24\n",
      "maine\n",
      "22\n",
      "michigan\n",
      "22\n",
      "minnesota\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      "23\n",
      "missouri\n",
      "22\n",
      "mississippi\n",
      "22\n",
      "montana\n",
      "22\n",
      "north carolina\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      "23\n",
      "north dakota\n",
      "22\n",
      "nebraska\n",
      "22\n",
      "new hampshire\n",
      "22\n",
      "new jersey\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      "24\n",
      "new mexico\n",
      "22\n",
      "nevada\n",
      "22\n",
      "ohio\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      "24\n",
      "oklahoma\n",
      "22\n",
      "pennsylvania\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      "24\n",
      "rhode island\n",
      "22\n",
      "south carolina\n",
      "22\n",
      "south dakota\n",
      "22\n",
      "tennessee\n",
      "23\n",
      "utah\n",
      "22\n",
      "virginia\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n",
      "24\n",
      "vermont\n",
      "22\n",
      "wisconsin\n",
      "22\n",
      "west virginia\n",
      "22\n",
      "wyoming\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "state_names = [ \"alabama\", \"arkansas\",  \"arizona\",  \"colorado\", \"connecticut\", \"delaware\",  \"georgia\", \"iowa\", \"idaho\", \"illinois\", \"indiana\", \"kansas\", \"kentucky\", \"louisiana\", \"maryland\", \"maine\", \"michigan\", \"minnesota\", \"missouri\", \"mississippi\", \"montana\", \"north carolina\", \"north dakota\", \"nebraska\", \"new hampshire\", \"new jersey\", \"new mexico\", \"nevada\",  \"ohio\", \"oklahoma\", \"pennsylvania\", \"rhode island\", \"south carolina\", \"south dakota\", \"tennessee\", \"utah\", \"virginia\",  \"vermont\",  \"wisconsin\", \"west virginia\", \"wyoming\"]\n",
    "\n",
    "\n",
    "for state in state_names:\n",
    "    position = 'data scientist'\n",
    "    location = state\n",
    "    print(state)\n",
    "    data = main(position,location )\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " new page\n",
      " new page\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Scraped 45 new records.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 'data scientist'\n",
    "location = 'california'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " new page\n",
      " new page\n",
      " new page\n",
      " new page\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='www.indeed.com', port=443): Max retries exceeded with url: /rc/clk?jk=7d3b67195e4f01f2&fccid=dc023faa36b88bf2&vjs=3 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd678d28e20>: Failed to establish a new connection: [Errno 110] Connection timed out'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             conn = connection.create_connection(\n\u001b[0m\u001b[1;32m    170\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 110] Connection timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             raise NewConnectionError(\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x7fd678d28e20>: Failed to establish a new connection: [Errno 110] Connection timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    756\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.indeed.com', port=443): Max retries exceeded with url: /rc/clk?jk=7d3b67195e4f01f2&fccid=dc023faa36b88bf2&vjs=3 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd678d28e20>: Failed to establish a new connection: [Errno 110] Connection timed out'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-91c2ca49eb4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data scientist'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'remote'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlocation\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-343ebfbc68a5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(position, location)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_posts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mdatapoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-693b5414a14f>\u001b[0m in \u001b[0;36mget_features\u001b[0;34m(post)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0msalary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimated_salary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_type_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequirements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_desc_soup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_desc_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mdatapoint_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-404b337186cd>\u001b[0m in \u001b[0;36mget_desc_features\u001b[0;34m(job_url)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjob\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mresponse_job_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_job_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='www.indeed.com', port=443): Max retries exceeded with url: /rc/clk?jk=7d3b67195e4f01f2&fccid=dc023faa36b88bf2&vjs=3 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd678d28e20>: Failed to establish a new connection: [Errno 110] Connection timed out'))"
     ]
    }
   ],
   "source": [
    "position = 'data scientist'\n",
    "location = 'remote'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 'data scientist'\n",
    "location = 'new york'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 'data scientist'\n",
    "location = 'texas'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 'data scientist'\n",
    "location = 'washington'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 'data scientist'\n",
    "location = 'florida'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 'data scientist'\n",
    "location = 'massachusetts'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 'data scientist'\n",
    "location = 'oregon'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  below is used for various adjustments to my webscraping process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating Data With New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_names = ['alabama', 'arkansas', 'arizona', 'california', 'colorado', 'connecticut', 'delaware', 'florida', 'georgia', 'remote', 'iowa', 'idaho', 'illinois', 'indiana', 'kansas', 'kentucky', 'louisiana', 'massachusetts', 'maryland', 'maine', 'michigan', 'minnesota', 'missouri', 'mississippi', 'montana', 'north carolina', 'north dakota', 'nebraska', 'new hampshire', 'new jersey', 'new mexico', 'nevada', 'new york', 'ohio', 'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas', 'utah', 'virginia',  'vermont', 'washington', 'wisconsin', 'west virginia', 'wyoming']\n",
    "\n",
    "lst = []\n",
    "records = 0\n",
    "for i in state_names:\n",
    "    i = i.replace(' ','_')\n",
    "    try:\n",
    "        location = pd.read_csv(f'../app/data/scraped_data_scientist_{i}.csv')\n",
    "        records += len(location)\n",
    "        lst.append(location)\n",
    "        print(f'Scraped  {len(location)} new records for {i}')\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f'Scraped Records: {(records)}')\n",
    "\n",
    "#todays_scrape = pd.concat(lst)\n",
    "## unblock if you miss too many days\n",
    "#total = todays_scrape.to_csv('../app/data/total.csv', index= False)\n",
    "\n",
    "total = pd.read_csv(f'../app/data/total.csv')\n",
    "lst.append(total)\n",
    "\n",
    "total = pd.concat(lst)\n",
    "\n",
    "total.drop_duplicates(inplace=True)\n",
    "\n",
    "total.to_csv('../app/data/total.csv', index= False)\n",
    "print(f'Total Records: {(len(total))}')\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO explain why this is being shown. remove from data and save, but also show what portion of the data it represents. Relatively miniscule.\n",
    "total[total.description.isna()].location.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below are bits of code I employ if something goes wrong with the webscraping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#fix old imports\n",
    "\n",
    "data['extractDate']= pd.to_datetime(data['extractDate'])\n",
    "\n",
    "def pDate(row):\n",
    "    from datetime import datetime, date, timedelta\n",
    "\n",
    "    #days_ago = row['dateposted']\n",
    "    delta = timedelta(0)\n",
    "    try:\n",
    "        return row['extractDate'] - delta\n",
    "    except:\n",
    "        return row\n",
    "\n",
    "data['extractDate'] = data.apply( lambda row : pDate(row), axis = 1)\n",
    "data['extractDate'] = data['extractDate'].astype(str)\n",
    "#data.to_csv('../app/data/scraped_data_scientist_remote_2022-04-14.csv', index= False)\n",
    "data.extractDate.unique()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# codescraps in case they change the html and break my parsers\n",
    "\n",
    "searchResults = soup.find('div', id='mosaic-provider-jobcards')\n",
    "refinedsearchResults = searchResults.find_all('div', attrs={'class': lambda e: e.startswith('cardOutline') if e else False})\n",
    "len(refinedsearchResults)\n",
    "z = searchResults.children\n",
    "lst = []\n",
    "for i in z:\n",
    "    lst.append(i)\n",
    "\n",
    "x = lst[0]\n",
    "\n",
    "#checking again\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#searchResults = soup.find('div', id='mosaic-provider-jobcards')\n",
    "refinedsearchResults = soup.find_all('div', attrs={'class': lambda e: e.startswith('cardOutline') if e else False})\n",
    "        \n",
    "\n",
    "raw_posts = []\n",
    "for post in refinedsearchResults:\n",
    "        raw_posts.append(post)\n",
    "        n = 0\n",
    "\n",
    "z = raw_posts[0]\n",
    "url = z.find('a', href=True)\n",
    "url\n",
    "\n",
    "\n",
    "z.find('a', href = re.compile(r'[/]([a-z]|[A-Z])\\w+')).attrs['href']\n",
    "z\n",
    "\n",
    "postDate = z.find('span', 'date').text\n",
    "extractDate = datetime.today().strftime('%Y-%m-%d')\n",
    "summary = z.find('div', 'job-snippet').text.strip().replace('\\n', ' ')\n",
    "\n",
    "summary\n",
    "\n",
    "company_name = z.find('a', attrs={'class':'turnstileLink companyOverviewLink'}).text.strip()\n",
    "company_name\n",
    "\n",
    "job_title = z.find('a', attrs={'class':'jcs-JobTitle'}).text.strip()\n",
    "job_title\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac83f05b37eb5c5c49ba67e50f0047ddfbc4b30205fc79ee5f327b9c0ac37f55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "46216370387cf43888a1dc9433c5a4546bda9feed2ad4f35c2a851da9960dc54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
