{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import random\n",
    "import time\n",
    "import re\n",
    "# current version of seaborn generates a bunch of warnings that we'll ignore\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this to test for captcha block or IP ban\n",
    "def get_URL(position,location):\n",
    "    #from torrequest import TorRequest\n",
    "    \"\"\"[Build a template url for a dummy call to verify the site isn't returning a captcha]\n",
    "    Args:\n",
    "        position ([string]): [job for query]\n",
    "        location ([string]): [location for query]\n",
    "    Returns:\n",
    "        [string]: [formatted url]\n",
    "    \"\"\"\n",
    "    template = 'https://www.indeed.com/jobs?q={}&l={}&fromage=1&sort=date'\n",
    "                \n",
    "    position = position.replace(' ', '%20')\n",
    "    location = location.replace(' ', '+')\n",
    "    url = template.format(position,location)\n",
    "    return url\n",
    "\n",
    "\n",
    "# from torrequest import TorRequest\n",
    "# tr=TorRequest(password='your_super_secure_password')\n",
    "position = 'data scientist'\n",
    "location = 'california'\n",
    "# tr.reset_identity()\n",
    "response = requests.get(get_URL(position,location))\n",
    "# This will either return an HTML block for a captcha or of a search result\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_desc_features(job_url):\n",
    "    \"\"\"Parses each job description, searching for and extracting values for features\n",
    "\n",
    "    Args:\n",
    "        job_url (string): http address of each job posting\n",
    "\n",
    "    Returns:\n",
    "        tuple: job feature values\n",
    "    \"\"\"\n",
    "    response_job_desc = requests.get(job_url)\n",
    "    soup = BeautifulSoup(response_job_desc.text, 'html.parser')\n",
    "\n",
    "    \n",
    "    try:\n",
    "        salary_and_job_type = soup.find('div', id='salaryInfoAndJobType').text.strip()\n",
    "    except:\n",
    "        salary_and_job_type = None\n",
    "    if salary_and_job_type == None:\n",
    "        try:\n",
    "            salary_and_job_type = soup.find('div',id=\"icl-u-xs-block jobsearch-ReqAndQualSection-item--title\").text.replace(\"\\n\", \"\").strip()\n",
    "        except:\n",
    "            salary_and_job_type = None\n",
    "    #TODO get benefits from its designated section\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        salary_guidance = []\n",
    "        items = soup.find('ul',class_='css-1lyr5hv eu4oa1w0')\n",
    "        for i in items:\n",
    "            salary_guidance.append(i.text)\n",
    "    except:\n",
    "        salary_guidance = None\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        job_type_items = []\n",
    "        job_type_from_section = soup.find('div',class_='jobsearch-JobDescriptionSection-sectionItem').next_sibling.children\n",
    "        for i in job_type_from_section:\n",
    "            if i.text == 'Job Type':\n",
    "                continue\n",
    "            else:\n",
    "                job_type_items.append(i.text)\n",
    "    except:\n",
    "        job_type_items = None\n",
    "        \n",
    "    \n",
    "    try:\n",
    "        requirements = soup.find(class_=\"icl-u-xs-block jobsearch-ReqAndQualSection-item--title\").text.replace(\"\\n\", \"\").strip()      \n",
    "\n",
    "    except:\n",
    "        requirements = None\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        description = soup.find(id=\"jobDescriptionText\").text.replace('\\n', '')\n",
    "    except:\n",
    "        description = None\n",
    "        \n",
    "        \n",
    "    # A nifty little workaround for evading detection.\n",
    "    time.sleep(.5+random()*3)\n",
    "    #TODO assess h2 tags commonalities to determine if these section descriptions are from Indeed or are at least of only a few variations.\n",
    "        #you could then distinguish the description into sections and conduct NLP etc each.\n",
    "    raw_desc_soup = soup\n",
    "    return salary_and_job_type, salary_guidance, job_type_items, requirements, description, raw_desc_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO condense these with lists, particularly fields that have .text.strip()\n",
    "def get_features(post):\n",
    "    \"\"\"parses search results and extracts basic job feature values,\n",
    "        then combines this with output of 'get_desc_features' function.\n",
    "\n",
    "    Args:\n",
    "        post (string): response for each post in search results page\n",
    "\n",
    "    Returns:\n",
    "        dict: single-feature deep dictionary of features (dictionary keys) and their values (dictionary values)\n",
    "    \"\"\"\n",
    "    datapoint_dict = {}\n",
    "\n",
    "    title = post.find('h2',\n",
    "              attrs={'class': lambda e: e.startswith('jobTitle') if e else False}).text.replace('new', '')\n",
    "\n",
    "    company = post.find('span', 'companyName').text.strip()\n",
    "    try:\n",
    "        rating = post.find('span', 'ratingNumber').text\n",
    "    except:\n",
    "        rating = None\n",
    "\n",
    "    location = post.find('div', 'companyLocation').text.strip()\n",
    "    postDate = post.find('span', 'date').text\n",
    "    extractDate = datetime.today().strftime('%Y-%m-%d')\n",
    "    summary = post.find('div', 'job-snippet').text.strip().replace('\\n', ' ')\n",
    "    url = 'https://www.indeed.com'+ post.find('a', href = re.compile(r'[/]([a-z]|[A-Z])\\w+')).attrs['href']\n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "    salary, estimated_salary, job_type_items, requirements, description, raw_desc_soup = get_desc_features(url)\n",
    "    datapoint_dict = {\n",
    "                        'title':title,\n",
    "                        'company':company,\n",
    "                        'rating':rating,\n",
    "                        'location':location,\n",
    "                        'estimated_salary':estimated_salary,\n",
    "                        'postDate':postDate,\n",
    "                        'extractDate':extractDate,\n",
    "                        'summary':summary,\n",
    "                        'url':url,\n",
    "                        'salary':salary,\n",
    "                        'job_type_items':job_type_items,\n",
    "                        'requirements':requirements,\n",
    "                        'description':description,\n",
    "                        'raw_desc_soup':raw_desc_soup}\n",
    "    if len(datapoint_dict) > 0:\n",
    "        return datapoint_dict\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(position, location):\n",
    "    \"\"\"[Conducts the web scraping process]\n",
    "    Args:\n",
    "        position ([string]): [job position for indeed.com query]\n",
    "        position ([string]): [job location for indeed.com query]\n",
    "        \n",
    "        Returns:\n",
    "        [csv]: [scraped data]\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    # extract the job data\n",
    "    run = True\n",
    "    try:\n",
    "        response = requests.get(get_URL(position, location))\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        searchResults = soup.find('div', id='mosaic-provider-jobcards')\n",
    "        refinedsearchResults = searchResults.find_all('div', attrs={'class': lambda e: e.startswith('cardOutline') if e else False})\n",
    "        \n",
    "\n",
    "        raw_posts = []\n",
    "        for post in refinedsearchResults:\n",
    "            raw_posts.append(post)\n",
    "        \n",
    "        n = 0\n",
    "        for post in raw_posts:\n",
    "            datapoint = get_features(post)\n",
    "            data = data.append(datapoint, ignore_index=True)\n",
    "        # Again, a nifty little workaround for evading detection.\n",
    "            n+=1\n",
    "            if n%15==0:\n",
    "                print('New Page')\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "            \n",
    "        try:\n",
    "            url = 'https://www.indeed.com' + soup.find('a', {'aria-label': 'Next'}).get('href')\n",
    "        except:\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "\n",
    "    name = position.replace(' ','_')\n",
    "    loc = location.replace(' ','_')\n",
    "    day = date.today()\n",
    "    # save the job data\n",
    "    data.to_csv(f'../app/data/scraped_{name}_{loc}.csv', index=False)\n",
    "    \n",
    "    return f'Scraped {len(data)} new records.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "louisiana\n",
      "maryland\n",
      "maine\n",
      "michigan\n",
      "minnesota\n",
      "missouri\n",
      "mississippi\n",
      "montana\n",
      "north carolina\n",
      "north dakota\n",
      "nebraska\n",
      "new hampshire\n",
      "new jersey\n",
      "new mexico\n",
      "nevada\n",
      "ohio\n",
      "oklahoma\n",
      "pennsylvania\n",
      "rhode island\n",
      "south carolina\n",
      "south dakota\n",
      "tennessee\n",
      "utah\n",
      "virginia\n",
      "vermont\n",
      "wisconsin\n",
      "west virginia\n",
      "wyoming\n"
     ]
    }
   ],
   "source": [
    "#state_names = [ \"alabama\", \"arkansas\",  \"arizona\",  \"colorado\", \"connecticut\", \"delaware\",  \"georgia\", \"iowa\", \"idaho\", \"illinois\", \"indiana\", \"kansas\", \"kentucky\", \"louisiana\", \"maryland\", \"maine\", \"michigan\", \"minnesota\", \"missouri\", \"mississippi\", \"montana\", \"north carolina\", \"north dakota\", \"nebraska\", \"new hampshire\", \"new jersey\", \"new mexico\", \"nevada\",  \"ohio\", \"oklahoma\", \"pennsylvania\", \"rhode island\", \"south carolina\", \"south dakota\", \"tennessee\", \"utah\", \"virginia\",  \"vermont\",  \"wisconsin\", \"west virginia\", \"wyoming\"]\n",
    "\n",
    "state_names = [  \"louisiana\", \"maryland\", \"maine\", \"michigan\", \"minnesota\", \"missouri\", \"mississippi\", \"montana\", \"north carolina\", \"north dakota\", \"nebraska\", \"new hampshire\", \"new jersey\", \"new mexico\", \"nevada\",  \"ohio\", \"oklahoma\", \"pennsylvania\", \"rhode island\", \"south carolina\", \"south dakota\", \"tennessee\", \"utah\", \"virginia\",  \"vermont\",  \"wisconsin\", \"west virginia\", \"wyoming\"]\n",
    "\n",
    "for state in state_names:\n",
    "    position = 'data scientist'\n",
    "    location = state\n",
    "    print(state)\n",
    "    data = main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scraped 3 new records.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 'data scientist'\n",
    "location = 'california'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Page\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Scraped 15 new records.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 'data scientist'\n",
    "location = 'remote'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Page\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Scraped 15 new records.'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 'data scientist'\n",
    "location = 'new york'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Page\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Scraped 15 new records.'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 'data scientist'\n",
    "location = 'texas'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scraped 9 new records.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 'data scientist'\n",
    "location = 'washington'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scraped 10 new records.'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 'data scientist'\n",
    "location = 'florida'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Page\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Scraped 15 new records.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 'data scientist'\n",
    "location = 'massachusetts'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scraped 1 new records.'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 'data scientist'\n",
    "location = 'oregon'\n",
    "main(position,location )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  below is used for various adjustments to my webscraping process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating Data With New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped  2 new records for alabama\n",
      "Scraped  3 new records for arkansas\n",
      "Scraped  2 new records for arizona\n",
      "Scraped  3 new records for california\n",
      "Scraped  5 new records for colorado\n",
      "Scraped  4 new records for connecticut\n",
      "Scraped  3 new records for delaware\n",
      "Scraped  10 new records for florida\n",
      "Scraped  6 new records for georgia\n",
      "Scraped  15 new records for remote\n",
      "Scraped  1 new records for iowa\n",
      "Scraped  1 new records for idaho\n",
      "Scraped  6 new records for illinois\n",
      "Scraped  3 new records for indiana\n",
      "Scraped  2 new records for kansas\n",
      "Scraped  4 new records for kentucky\n",
      "Scraped  1 new records for louisiana\n",
      "Scraped  15 new records for massachusetts\n",
      "Scraped  5 new records for maryland\n",
      "Scraped  1 new records for michigan\n",
      "Scraped  3 new records for minnesota\n",
      "Scraped  1 new records for missouri\n",
      "Scraped  11 new records for north_carolina\n",
      "Scraped  1 new records for nebraska\n",
      "Scraped  11 new records for new_jersey\n",
      "Scraped  1 new records for new_mexico\n",
      "Scraped  15 new records for new_york\n",
      "Scraped  8 new records for ohio\n",
      "Scraped  1 new records for oklahoma\n",
      "Scraped  1 new records for oregon\n",
      "Scraped  6 new records for pennsylvania\n",
      "Scraped  2 new records for rhode_island\n",
      "Scraped  3 new records for south_carolina\n",
      "Scraped  3 new records for tennessee\n",
      "Scraped  15 new records for texas\n",
      "Scraped  4 new records for utah\n",
      "Scraped  9 new records for virginia\n",
      "Scraped  9 new records for washington\n",
      "Scraped  5 new records for wisconsin\n",
      "Scraped Records: 201\n",
      "Total Records: 918\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>description</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>extractDate</th>\n",
       "      <th>job_type_items</th>\n",
       "      <th>location</th>\n",
       "      <th>postDate</th>\n",
       "      <th>rating</th>\n",
       "      <th>raw_desc_soup</th>\n",
       "      <th>requirements</th>\n",
       "      <th>salary</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spire Inc.</td>\n",
       "      <td>Summary Spire is hiring a Data Scientist in Bi...</td>\n",
       "      <td>['', 'Not provided by employer', \"$103K - $130...</td>\n",
       "      <td>2022-06-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hybrid remote in Birmingham, AL 35203</td>\n",
       "      <td>PostedPosted 1 day ago</td>\n",
       "      <td>3.3</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This role will have a direct impact on transfo...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=137727a016a51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COLSA</td>\n",
       "      <td>General Summary Designs, develops and analyzes...</td>\n",
       "      <td>['', 'Not provided by employer', \"$78.9K - $99...</td>\n",
       "      <td>2022-06-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Huntsville, AL</td>\n",
       "      <td>PostedPosted 1 day ago</td>\n",
       "      <td>3.9</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Advises hardware design engineers on machine c...</td>\n",
       "      <td>Artificial Intelligence Developer</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=b9e5801c8446f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>Position Summary...What you'll do...About Proj...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-06-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote in Bentonville, AR 72712</td>\n",
       "      <td>PostedPosted 1 day ago</td>\n",
       "      <td>3.4</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bachelor's degree in Statistics, Economics, An...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=55113e2916c5a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>Position Summary...What you'll do...Who We Are...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-06-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bentonville, AR 72712</td>\n",
       "      <td>PostedPosted 1 day ago</td>\n",
       "      <td>3.4</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Perform data-wrangling activities to ensure qu...</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=d4bb530920a8f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Change Healthcare</td>\n",
       "      <td>Company: Change Healthcare (NASDAQ: CHNG) is a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-06-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>PostedPosted 1 day ago</td>\n",
       "      <td>3.0</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>You will enjoy working with a highly talented ...</td>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=904838a5db031...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>Sonic Foundry</td>\n",
       "      <td>The Lead Data Scientist – Video ai is respon...</td>\n",
       "      <td>['', 'Not provided by employer', \"$116K - $147...</td>\n",
       "      <td>2022-06-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madison, WI 53703 (Capitol area)</td>\n",
       "      <td>PostedJust posted</td>\n",
       "      <td>4.2</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>5+ years experience working with machine learn...</td>\n",
       "      <td>Lead Data Scientist - Video AI (FT)</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=d7dfd29204288...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>Change Healthcare</td>\n",
       "      <td>Company: Change Healthcare (NASDAQ: CHNG) is a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-06-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>PostedToday</td>\n",
       "      <td>3.0</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>You will enjoy working with a highly talented ...</td>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=92041a1fbba49...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>Kadiak, LLC</td>\n",
       "      <td>Kadiak,LLC, a Koniag Government Services compa...</td>\n",
       "      <td>['', 'Not provided by employer', \"$84.9K - $10...</td>\n",
       "      <td>2022-06-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Clarksburg, WV</td>\n",
       "      <td>PostedToday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Under the direction of the government, the suc...</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=80347759e1199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>Change Healthcare</td>\n",
       "      <td>Company: Change Healthcare (NASDAQ: CHNG) is a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-06-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>PostedToday</td>\n",
       "      <td>3.0</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>You will enjoy working with a highly talented ...</td>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=422bdc77b407f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>Change Healthcare</td>\n",
       "      <td>Company: Change Healthcare (NASDAQ: CHNG) is a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-06-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>PostedToday</td>\n",
       "      <td>3.0</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;html dir=\"ltr\" lang=\"en\"&gt;\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>You will enjoy working with a highly talented ...</td>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=43f692fed6925...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>918 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               company                                        description  \\\n",
       "0           Spire Inc.  Summary Spire is hiring a Data Scientist in Bi...   \n",
       "1                COLSA  General Summary Designs, develops and analyzes...   \n",
       "0              Walmart  Position Summary...What you'll do...About Proj...   \n",
       "1              Walmart  Position Summary...What you'll do...Who We Are...   \n",
       "2    Change Healthcare  Company: Change Healthcare (NASDAQ: CHNG) is a...   \n",
       "..                 ...                                                ...   \n",
       "913      Sonic Foundry    The Lead Data Scientist – Video ai is respon...   \n",
       "914  Change Healthcare  Company: Change Healthcare (NASDAQ: CHNG) is a...   \n",
       "915        Kadiak, LLC  Kadiak,LLC, a Koniag Government Services compa...   \n",
       "916  Change Healthcare  Company: Change Healthcare (NASDAQ: CHNG) is a...   \n",
       "917  Change Healthcare  Company: Change Healthcare (NASDAQ: CHNG) is a...   \n",
       "\n",
       "                                      estimated_salary extractDate  \\\n",
       "0    ['', 'Not provided by employer', \"$103K - $130...  2022-06-26   \n",
       "1    ['', 'Not provided by employer', \"$78.9K - $99...  2022-06-26   \n",
       "0                                                  NaN  2022-06-25   \n",
       "1                                                  NaN  2022-06-25   \n",
       "2                                                  NaN  2022-06-25   \n",
       "..                                                 ...         ...   \n",
       "913  ['', 'Not provided by employer', \"$116K - $147...  2022-06-24   \n",
       "914                                                NaN  2022-06-24   \n",
       "915  ['', 'Not provided by employer', \"$84.9K - $10...  2022-06-24   \n",
       "916                                                NaN  2022-06-24   \n",
       "917                                                NaN  2022-06-24   \n",
       "\n",
       "    job_type_items                               location  \\\n",
       "0              NaN  Hybrid remote in Birmingham, AL 35203   \n",
       "1              NaN                         Huntsville, AL   \n",
       "0              NaN        Remote in Bentonville, AR 72712   \n",
       "1              NaN                  Bentonville, AR 72712   \n",
       "2              NaN                               Arkansas   \n",
       "..             ...                                    ...   \n",
       "913            NaN       Madison, WI 53703 (Capitol area)   \n",
       "914            NaN                              Wisconsin   \n",
       "915            NaN                         Clarksburg, WV   \n",
       "916            NaN                          West Virginia   \n",
       "917            NaN                                Wyoming   \n",
       "\n",
       "                   postDate  rating  \\\n",
       "0    PostedPosted 1 day ago     3.3   \n",
       "1    PostedPosted 1 day ago     3.9   \n",
       "0    PostedPosted 1 day ago     3.4   \n",
       "1    PostedPosted 1 day ago     3.4   \n",
       "2    PostedPosted 1 day ago     3.0   \n",
       "..                      ...     ...   \n",
       "913       PostedJust posted     4.2   \n",
       "914             PostedToday     3.0   \n",
       "915             PostedToday     NaN   \n",
       "916             PostedToday     3.0   \n",
       "917             PostedToday     3.0   \n",
       "\n",
       "                                         raw_desc_soup requirements  \\\n",
       "0    <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "1    <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "0    <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "1    <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "2    <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "..                                                 ...          ...   \n",
       "913  <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "914  <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "915  <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "916  <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "917  <!DOCTYPE html>\\n\\n<html dir=\"ltr\" lang=\"en\">\\...          NaN   \n",
       "\n",
       "        salary                                            summary  \\\n",
       "0          NaN  This role will have a direct impact on transfo...   \n",
       "1    Full-time  Advises hardware design engineers on machine c...   \n",
       "0          NaN  Bachelor's degree in Statistics, Economics, An...   \n",
       "1          NaN  Perform data-wrangling activities to ensure qu...   \n",
       "2    Full-time  You will enjoy working with a highly talented ...   \n",
       "..         ...                                                ...   \n",
       "913  Full-time  5+ years experience working with machine learn...   \n",
       "914  Full-time  You will enjoy working with a highly talented ...   \n",
       "915        NaN  Under the direction of the government, the suc...   \n",
       "916  Full-time  You will enjoy working with a highly talented ...   \n",
       "917  Full-time  You will enjoy working with a highly talented ...   \n",
       "\n",
       "                                   title  \\\n",
       "0                         Data Scientist   \n",
       "1      Artificial Intelligence Developer   \n",
       "0                         Data Scientist   \n",
       "1                  Senior Data Scientist   \n",
       "2                    Lead Data Scientist   \n",
       "..                                   ...   \n",
       "913  Lead Data Scientist - Video AI (FT)   \n",
       "914                  Lead Data Scientist   \n",
       "915                         Data Science   \n",
       "916                  Lead Data Scientist   \n",
       "917                  Lead Data Scientist   \n",
       "\n",
       "                                                   url  \n",
       "0    https://www.indeed.com/rc/clk?jk=137727a016a51...  \n",
       "1    https://www.indeed.com/rc/clk?jk=b9e5801c8446f...  \n",
       "0    https://www.indeed.com/rc/clk?jk=55113e2916c5a...  \n",
       "1    https://www.indeed.com/rc/clk?jk=d4bb530920a8f...  \n",
       "2    https://www.indeed.com/rc/clk?jk=904838a5db031...  \n",
       "..                                                 ...  \n",
       "913  https://www.indeed.com/rc/clk?jk=d7dfd29204288...  \n",
       "914  https://www.indeed.com/rc/clk?jk=92041a1fbba49...  \n",
       "915  https://www.indeed.com/rc/clk?jk=80347759e1199...  \n",
       "916  https://www.indeed.com/rc/clk?jk=422bdc77b407f...  \n",
       "917  https://www.indeed.com/rc/clk?jk=43f692fed6925...  \n",
       "\n",
       "[918 rows x 14 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_names = ['alabama', 'arkansas', 'arizona', 'california', 'colorado', 'connecticut', 'delaware', 'florida', 'georgia', 'remote', 'iowa', 'idaho', 'illinois', 'indiana', 'kansas', 'kentucky', 'louisiana', 'massachusetts', 'maryland', 'maine', 'michigan', 'minnesota', 'missouri', 'mississippi', 'montana', 'north carolina', 'north dakota', 'nebraska', 'new hampshire', 'new jersey', 'new mexico', 'nevada', 'new york', 'ohio', 'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas', 'utah', 'virginia',  'vermont', 'washington', 'wisconsin', 'west virginia', 'wyoming']\n",
    "\n",
    "lst = []\n",
    "records = 0\n",
    "for i in state_names:\n",
    "    i = i.replace(' ','_')\n",
    "    try:\n",
    "        location = pd.read_csv(f'../app/data/scraped_data_scientist_{i}.csv')\n",
    "        records += len(location)\n",
    "        lst.append(location)\n",
    "        print(f'Scraped  {len(location)} new records for {i}')\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f'Scraped Records: {(records)}')\n",
    "\n",
    "todays_scrape = pd.concat(lst)\n",
    "## unblock if you miss too many days\n",
    "#total = todays_scrape.to_csv('../app/data/total.csv', index= False)\n",
    "\n",
    "total = pd.read_csv(f'../app/data/total.csv')\n",
    "lst.append(total)\n",
    "\n",
    "total = pd.concat(lst)\n",
    "\n",
    "total.drop_duplicates(inplace=True)\n",
    "\n",
    "total.to_csv('../app/data/total.csv', index= False)\n",
    "print(f'Total Records: {(len(total))}')\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Austin, TX    2\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO explain why this is being shown. remove from data and save, but also show what portion of the data it represents. Relatively miniscule.\n",
    "total[total.description.isna()].location.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below are bits of code I employ if something goes wrong with the webscraping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#fix old imports\n",
    "\n",
    "data['extractDate']= pd.to_datetime(data['extractDate'])\n",
    "\n",
    "def pDate(row):\n",
    "    from datetime import datetime, date, timedelta\n",
    "\n",
    "    #days_ago = row['dateposted']\n",
    "    delta = timedelta(0)\n",
    "    try:\n",
    "        return row['extractDate'] - delta\n",
    "    except:\n",
    "        return row\n",
    "\n",
    "data['extractDate'] = data.apply( lambda row : pDate(row), axis = 1)\n",
    "data['extractDate'] = data['extractDate'].astype(str)\n",
    "#data.to_csv('../app/data/scraped_data_scientist_remote_2022-04-14.csv', index= False)\n",
    "data.extractDate.unique()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# codescraps in case they change the html and break my parsers\n",
    "\n",
    "searchResults = soup.find('div', id='mosaic-provider-jobcards')\n",
    "refinedsearchResults = searchResults.find_all('div', attrs={'class': lambda e: e.startswith('cardOutline') if e else False})\n",
    "len(refinedsearchResults)\n",
    "z = searchResults.children\n",
    "lst = []\n",
    "for i in z:\n",
    "    lst.append(i)\n",
    "\n",
    "x = lst[0]\n",
    "\n",
    "#checking again\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#searchResults = soup.find('div', id='mosaic-provider-jobcards')\n",
    "refinedsearchResults = soup.find_all('div', attrs={'class': lambda e: e.startswith('cardOutline') if e else False})\n",
    "        \n",
    "\n",
    "raw_posts = []\n",
    "for post in refinedsearchResults:\n",
    "        raw_posts.append(post)\n",
    "        n = 0\n",
    "\n",
    "z = raw_posts[0]\n",
    "url = z.find('a', href=True)\n",
    "url\n",
    "\n",
    "\n",
    "z.find('a', href = re.compile(r'[/]([a-z]|[A-Z])\\w+')).attrs['href']\n",
    "z\n",
    "\n",
    "postDate = z.find('span', 'date').text\n",
    "extractDate = datetime.today().strftime('%Y-%m-%d')\n",
    "summary = z.find('div', 'job-snippet').text.strip().replace('\\n', ' ')\n",
    "\n",
    "summary\n",
    "\n",
    "company_name = z.find('a', attrs={'class':'turnstileLink companyOverviewLink'}).text.strip()\n",
    "company_name\n",
    "\n",
    "job_title = z.find('a', attrs={'class':'jcs-JobTitle'}).text.strip()\n",
    "job_title\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac83f05b37eb5c5c49ba67e50f0047ddfbc4b30205fc79ee5f327b9c0ac37f55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "46216370387cf43888a1dc9433c5a4546bda9feed2ad4f35c2a851da9960dc54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
